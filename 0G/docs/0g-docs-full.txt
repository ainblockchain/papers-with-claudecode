========== [ https://docs.0g.ai/ai-context ] ==========

On this page
0G AI Context for Coding Assistants
This page provides comprehensive context about 0G infrastructure to help AI coding assistants help developers build on 0G. All information is extracted from the official documentation at
https://docs.0g.ai
.
Network Configurations
‚Äã
Testnet (Galileo)
‚Äã
Explorer
:
https://explorer.0g.ai/testnet/home
Parameter
Value
Network Name
0G-Galileo-Testnet
Chain ID
16602
Token Symbol
0G
RPC Endpoint
https://evmrpc-testnet.0g.ai
Block Explorer
https://chainscan-galileo.0g.ai
Storage Explorer
https://storagescan-galileo.0g.ai
Validator Dashboard
https://testnet.0g.explorers.guru
Faucet
https://faucet.0g.ai
(0.1 0G/day)
Storage Start Block
1
DA Start Block
940000
Documentation
:
https://docs.0g.ai/developer-hub/testnet/testnet-overview
Third-Party RPCs (Recommended for Production)
:
QuickNode:
https://www.quicknode.com/chains/0g
ThirdWeb:
https://thirdweb.com/0g-galileo-testnet-16601
Ankr:
https://www.ankr.com/rpc/0g/
dRPC NodeCloud:
https://drpc.org/chainlist/0g-galileo-testnet-rpc
Mainnet (Aristotle)
‚Äã
Explorer
:
https://explorer.0g.ai/mainnet/home
Parameter
Value
Network Name
0G Mainnet
Chain ID
16661
Token Symbol
0G
RPC Endpoint
https://evmrpc.0g.ai
Storage Indexer
https://indexer-storage-turbo.0g.ai
Block Explorer
https://chainscan.0g.ai
Storage Start Block
2387557
Documentation
:
https://docs.0g.ai/developer-hub/mainnet/mainnet-overview
Third-Party RPCs (Recommended for Production)
:
QuickNode:
https://www.quicknode.com/chains/0g
ThirdWeb:
https://thirdweb.com/0g-aristotle
Ankr:
https://www.ankr.com/rpc/0g/
Smart Contract Addresses
‚Äã
Testnet Contracts
‚Äã
Contract
Address
Purpose
Flow
0x22E03a6A89B950F1c82ec5e74F8eCa321a105296
Storage data flow management
Mine
0x00A9E9604b0538e06b268Fb297Df333337f9593b
Storage mining rewards
Reward
0xA97B57b4BdFEA2D0a25e535bd849ad4e6C440A69
Reward distribution
DAEntrance
0xE75A073dA5bb7b0eC622170Fd268f35E675a957B
DA blob submission
DASigners
0x0000000000000000000000000000000000001000
DA signer management (precompile)
WrappedOGBase
0x0000000000000000000000000000000000001001
Wrapped native token (precompile)
Compute Ledger
0xE70830508dAc0A97e6c087c75f402f9Be669E406
Compute network payment ledger
Compute Inference
0xa79F4c8311FF93C06b8CfB403690cc987c93F91E
Compute inference service
Compute FineTuning
0xaC66eBd174435c04F1449BBa08157a707B6fa7b1
Compute fine-tuning service
Mainnet Contracts
‚Äã
Contract
Address
Purpose
Flow
0x62D4144dB0F0a6fBBaeb6296c785C71B3D57C526
Storage data flow management
Mine
0xCd01c5Cd953971CE4C2c9bFb95610236a7F414fe
Storage mining rewards
Reward
0x457aC76B58ffcDc118AABD6DbC63ff9072880870
Reward distribution
DASigners
0x0000000000000000000000000000000000001000
DA signer management (precompile)
WrappedOGBase
0x0000000000000000000000000000000000001001
Wrapped native token (precompile)
Compute Ledger
0x2dE54c845Cd948B72D2e32e39586fe89607074E3
Compute network payment ledger
Compute Inference
0x47340d900bdFec2BD393c626E12ea0656F938d84
Compute inference service
Compute FineTuning
0x4e3474095518883744ddf135b7E0A23301c7F9c0
Compute fine-tuning service
0G Services Overview
‚Äã
0G Chain
‚Äã
Documentation
:
https://docs.0g.ai/concepts/chain
Fastest modular AI chain with 11,000 TPS per Shard, sub-second finality, and full EVM compatibility.
Key Features
:
Full EVM compatibility
- Use existing Ethereum tools (Hardhat, Foundry, Remix)
11,000 TPS per Shard
with sub-second finality
Same as Ethereum development
- just different RPC endpoint
Optimized CometBFT consensus
Native precompiled contracts for DA and wrapped tokens
Deploy Smart Contracts
:
Using Hardhat:
// hardhat.config.js
module
.
exports
=
{
networks
:
{
testnet
:
{
url
:
"https://evmrpc-testnet.0g.ai"
,
chainId
:
16602
,
accounts
:
[
"YOUR_PRIVATE_KEY"
]
}
,
mainnet
:
{
url
:
"https://evmrpc.0g.ai"
,
chainId
:
16661
,
accounts
:
[
"YOUR_PRIVATE_KEY"
]
}
}
,
solidity
:
"0.8.20"
}
;
Using Foundry:
# Testnet
forge create --rpc-url https://evmrpc-testnet.0g.ai \
--private-key YOUR_PRIVATE_KEY \
src/MyContract.sol:MyContract
# Mainnet
forge create --rpc-url https://evmrpc.0g.ai \
--private-key YOUR_PRIVATE_KEY \
src/MyContract.sol:MyContract
Using Remix:
Open Remix IDE
Compile your contract
Go to Deploy & Run Transactions
Select "Injected Provider - MetaMask"
Ensure MetaMask is connected to 0G network
Deploy!
Precompiled Contracts
:
DASigners (0x0000000000000000000000000000000000001000):
// Query DA signers and epochs
function getEpochNumber(uint256 blockNumber) external view returns (uint256);
function getQuorum(uint256 epochNumber, uint256 quorumId) external view returns (Signer[] memory);
function isSigner(uint256 epochNumber, address account) external view returns (bool);
WrappedOGBase (0x0000000000000000000000000000000000001001):
// Wrapped native token (like WETH)
function deposit() external payable;
function withdraw(uint256 amount) external;
function balanceOf(address account) external view returns (uint256);
Verification & Indexing
:
Goldsky
: GraphQL indexing and real-time data streaming
Docs:
https://docs.goldsky.com/chains/0g
Guide:
https://docs.0g.ai/developer-hub/building-on-0g/indexing/goldsky
Documentation Links
:
Deploy Contracts:
https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/deploy-contracts
Precompiles:
https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/precompiles/overview
Staking:
https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/staking-interfaces
Validator Contracts:
https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/validator-contract-functions
0G Storage
‚Äã
Documentation
:
https://docs.0g.ai/concepts/storage
Decentralized storage offering 95% lower costs than AWS with instant retrieval.
Key Features
:
95% cheaper than centralized alternatives
200 MBPS retrieval speed
Proven TB-scale operations
Two storage layers: Log (immutable) + KV (mutable)
Proof of Random Access (PoRA) consensus
SDK Installation
:
TypeScript/JavaScript:
npm install @0glabs/0g-ts-sdk ethers
Python:
pip install 0g-storage-client
Go:
go get github.com/0gfoundation/0g-storage-client
Quick Start Examples
:
TypeScript - Upload File:
import
{
ZgFile
,
Indexer
}
from
"@0glabs/0g-ts-sdk"
;
import
{
ethers
}
from
"ethers"
;
const
provider
=
new
ethers
.
JsonRpcProvider
(
"https://evmrpc-testnet.0g.ai"
)
;
const
signer
=
new
ethers
.
Wallet
(
"YOUR_PRIVATE_KEY"
,
provider
)
;
const
indexer
=
new
Indexer
(
"https://indexer-storage-testnet-turbo.0g.ai"
)
;
const
file
=
await
ZgFile
.
fromFilePath
(
"/path/to/file"
)
;
const
[
tree
,
treeErr
]
=
await
file
.
merkleTree
(
)
;
console
.
log
(
"Root Hash:"
,
tree
?.
rootHash
(
)
)
;
const
[
tx
,
uploadErr
]
=
await
indexer
.
upload
(
file
,
"https://evmrpc-testnet.0g.ai"
,
signer
)
;
await
file
.
close
(
)
;
Python - Upload File:
from
storage_client
import
ZgStorageClient
client
=
ZgStorageClient
(
rpc_endpoint
=
"https://evmrpc-testnet.0g.ai"
,
contract_address
=
"0x22E03a6A89B950F1c82ec5e74F8eCa321a105296"
,
private_key
=
"YOUR_PRIVATE_KEY"
)
root_hash
=
client
.
upload_file
(
"/path/to/file"
)
CLI Tool
:
# Install
git clone https://github.com/0gfoundation/0g-storage-client.git
cd 0g-storage-client
go build
# Upload file
0g-storage-client upload \
--url https://evmrpc-testnet.0g.ai \
--key YOUR_PRIVATE_KEY \
--indexer https://indexer-storage-testnet-turbo.0g.ai \
--file /path/to/file
# Download file
0g-storage-client download \
--indexer https://indexer-storage-testnet-turbo.0g.ai \
--root <ROOT_HASH> \
--file output.dat
# Upload directory
0g-storage-client upload-dir \
--url https://evmrpc-testnet.0g.ai \
--key YOUR_PRIVATE_KEY \
--indexer https://indexer-storage-testnet-turbo.0g.ai \
--file /path/to/directory
# Download directory
0g-storage-client download-dir \
--indexer https://indexer-storage-testnet-turbo.0g.ai \
--root <DIRECTORY_ROOT_HASH> \
--file /path/to/output
Documentation Links
:
SDK Guide:
https://docs.0g.ai/developer-hub/building-on-0g/storage/sdk
CLI Guide:
https://docs.0g.ai/developer-hub/building-on-0g/storage/storage-cli
GitHub Repositories
:
Storage Node:
https://github.com/0gfoundation/0g-storage-node
Storage KV:
https://github.com/0gfoundation/0g-storage-kv
Storage Client/CLI:
https://github.com/0gfoundation/0g-storage-client
TypeScript SDK:
https://github.com/0gfoundation/0g-ts-sdk
Python SDK:
https://github.com/0gfoundation/0g-storage-client
Go SDK:
https://github.com/0gfoundation/0g-storage-client
0G Compute
‚Äã
Documentation
:
https://docs.0g.ai/concepts/compute
Decentralized GPU marketplace offering 90% cheaper AI workloads with OpenAI SDK compatibility.
Key Features
:
90% cost reduction vs traditional cloud (e.g.,
0.003
v
s
0.003 vs
0.003
v
s
0.03 per 1K tokens)
Pay-per-use pricing (no subscriptions or monthly minimums)
OpenAI SDK compatible - drop-in replacement
Smart contract escrow for trustless payments
TEE (Trusted Execution Environment) for secure processing
50-100ms inference latency
Supports: Chatbot (LLM), Text-to-Image, Speech-to-Text
DePIN Partners
:
io.net
: 300,000+ GPUs across 139 countries
Aethir
: 43,000+ enterprise-grade GPUs, 3,000+ H100s/H200s
Quick Start (5 minutes)
:
Install CLI:
pnpm add @0glabs/0g-serving-broker -g
Option 1 - Web UI (Easiest):
# Launch Web UI
0g-compute-cli ui start-web
# Open http://localhost:3090
# Connect wallet, deposit tokens, start using AI services
Option 2 - CLI:
# Setup network
0g-compute-cli setup-network
# Login with wallet
0g-compute-cli login
# Fund account
0g-compute-cli deposit --amount 10
# List available providers
0g-compute-cli inference list-providers
# Transfer funds to provider
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 5
# Acknowledge provider
0g-compute-cli inference acknowledge-provider --provider <PROVIDER_ADDRESS>
# Get API key for direct access
0g-compute-cli inference get-secret --provider <PROVIDER_ADDRESS>
Option 3 - Direct API (OpenAI Compatible):
curl <service_url>/v1/proxy/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer app-sk-<YOUR_SECRET>" \
-d '{
"model": "<model_name>",
"messages": [
{"role": "system", "content": "You are a helpful assistant."},
{"role": "user", "content": "Hello!"}
]
}'
OpenAI SDK Integration
:
from
openai
import
OpenAI
client
=
OpenAI
(
api_key
=
"app-sk-<YOUR_SECRET>"
,
base_url
=
"<service_url>/v1/proxy"
)
response
=
client
.
chat
.
completions
.
create
(
model
=
"<model_name>"
,
messages
=
[
{
"role"
:
"user"
,
"content"
:
"Hello!"
}
]
)
Fine-tuning Models
:
# Prepare dataset (JSONL format)
# Each line: {"prompt": "...", "completion": "..."}
# Upload dataset
0g-compute-cli fine-tuning upload-data --file dataset.jsonl
# Create fine-tuning task
0g-compute-cli fine-tuning create-task \
--model Qwen2.5-0.5B-Instruct \
--dataset <DATASET_ID> \
--provider <PROVIDER_ADDRESS>
# Monitor progress
0g-compute-cli fine-tuning get-task --task-id <TASK_ID>
Documentation Links
:
Overview:
https://docs.0g.ai/developer-hub/building-on-0g/compute-network/overview
Inference:
https://docs.0g.ai/developer-hub/building-on-0g/compute-network/inference
Fine-tuning:
https://docs.0g.ai/developer-hub/building-on-0g/compute-network/fine-tuning
Account Management:
https://docs.0g.ai/developer-hub/building-on-0g/compute-network/account-management
Provider Setup:
https://docs.0g.ai/developer-hub/building-on-0g/compute-network/inference-provider
0G DA (Data Availability)
‚Äã
Documentation
:
https://docs.0g.ai/concepts/da
Scalable data availability layer for rollups with 50 Gbps throughput.
Key Features
:
50 Gbps demonstrated throughput
VRF-based node selection
Inherits Ethereum security
For Rollup Developers
:
OP Stack Integration:
https://docs.0g.ai/developer-hub/building-on-0g/rollups-and-appchains/op-stack-on-0g-da
Repo:
https://github.com/0gfoundation/0g-da-op-plasma
Arbitrum Nitro:
https://docs.0g.ai/developer-hub/building-on-0g/rollups-and-appchains/arbitrum-nitro-on-0g-da
Repo:
https://github.com/0gfoundation/nitro
Integration Guide:
https://docs.0g.ai/developer-hub/building-on-0g/da-integration
INFT (Intelligent NFTs)
‚Äã
Documentation
:
https://docs.0g.ai/concepts/inft
NFT standard (ERC-7857) for tokenizing AI agents with complete intelligence.
Key Features
:
Extends ERC-721 standard
Encrypted metadata storage via 0G Storage
Secure re-encryption for ownership transfer
Oracle verification
True AI ownership transfer
Use Cases
:
AI Trading Bots
Personal Assistants
Game Characters
Content Creation AI
Research Tools
Documentation Links
:
INFT Overview:
https://docs.0g.ai/developer-hub/building-on-0g/inft/inft-overview
ERC-7857 Standard:
https://docs.0g.ai/developer-hub/building-on-0g/inft/erc7857
Integration Guide:
https://docs.0g.ai/developer-hub/building-on-0g/inft/integration
Running Nodes
‚Äã
Hardware Requirements
‚Äã
Node Type
Memory
CPU
Disk
Bandwidth
Purpose
Validator (Mainnet)
64 GB
8 cores
1 TB NVMe
100 Mbps
Transaction validation
Validator (Testnet)
64 GB
8 cores
4 TB NVMe
100 Mbps
Transaction validation
Storage Node
32 GB
8 cores
500GB-1TB SSD
100 Mbps
Data storage
Storage KV
32 GB
8 cores
Flexible
-
Key-value storage
DA Node
16 GB
8 cores
1 TB NVMe
100 Mbps
Blob verification
Archival Node
64 GB
8 cores
Large NVMe
100 Mbps
Historical data
Validator Node Setup
‚Äã
Mainnet (Aristotle)
:
# Install dependencies
sudo apt update && sudo apt install -y make gcc jq curl git lz4 build-essential
# Install Go
wget https://go.dev/dl/go1.22.0.linux-amd64.tar.gz
sudo tar -C /usr/local -xzf go1.22.0.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin
# Clone and build
git clone -b v1.0.4 https://github.com/0gfoundation/0gchain-Aristotle
cd 0gchain-Aristotle
make install
# Initialize
0gchaind init <YOUR_MONIKER> --chain-id zgtendermint_16661-1
# Configure
wget -O ~/.0gchain/config/genesis.json https://github.com/0gfoundation/0gchain-Aristotle/releases/download/v1.0.4/genesis.json
Testnet (Galileo)
:
# Clone and build
git clone -b v3.0.4 https://github.com/0gfoundation/0gchain-NG
cd 0gchain-NG
make install
# Initialize
0gchaind init <YOUR_MONIKER> --chain-id zgtendermint_16602-1
# Configure
wget -O ~/.0gchain/config/genesis.json https://github.com/0gfoundation/0gchain-NG/releases/download/v3.0.4/genesis.json
Documentation
:
https://docs.0g.ai/run-a-node/validator-node
Storage Node Setup
‚Äã
Build from Source
:
# Install dependencies
sudo apt-get update
sudo apt-get install clang cmake build-essential pkg-config libssl-dev protobuf-compiler
# Install Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
# Clone and build
git clone -b <latest_tag> https://github.com/0gfoundation/0g-storage-node.git
cd 0g-storage-node
cargo build --release
Configuration (Testnet)
:
# config.toml
log_contract_address = "0x22E03a6A89B950F1c82ec5e74F8eCa321a105296"
mine_contract_address = "0x00A9E9604b0538e06b268Fb297Df333337f9593b"
reward_contract_address = "0xA97B57b4BdFEA2D0a25e535bd849ad4e6C440A69"
blockchain_rpc_endpoint = "https://evmrpc-testnet.0g.ai"
log_sync_start_block_number = 1
miner_key = "YOUR_PRIVATE_KEY"
Configuration (Mainnet)
:
# config.toml
log_contract_address = "0x62D4144dB0F0a6fBBaeb6296c785C71B3D57C526"
mine_contract_address = "0xCd01c5Cd953971CE4C2c9bFb95610236a7F414fe"
reward_contract_address = "0x457aC76B58ffcDc118AABD6DbC63ff9072880870"
blockchain_rpc_endpoint = "https://evmrpc.0g.ai"
log_sync_start_block_number = 2387557
miner_key = "YOUR_PRIVATE_KEY"
Sharding Configuration
:
# Format: shard_id/shard_number where shard_number is 2^n
# Only applies on first startup if no stored shard config in db
shard_position = "0/2"
Sharding allows controlling how much data your node stores:
"0/2"
or
"1/2"
= 50% of total data (each on specific ranges)
"0/4"
to
"3/4"
= 25% of total data each
Initial setup is deterministic (shard_id maps to specific range)
Auto-splitting is NOT deterministic (random assignment when capacity exceeded)
Run Storage Node
:
cd run
../target/release/zgs_node --config config.toml --miner-key <your_private_key>
Documentation
:
https://docs.0g.ai/run-a-node/storage-node
Developer Tools
‚Äã
Indexing with Goldsky
‚Äã
Website
:
https://docs.goldsky.com/chains/0g
Products
:
Subgraphs
: GraphQL indexing for smart contracts
Mirror
: Real-time data streaming to databases
Documentation
:
https://docs.0g.ai/developer-hub/building-on-0g/indexing/goldsky
Rollup-as-a-Service
‚Äã
Caldera on 0G DA
:
https://docs.0g.ai/developer-hub/building-on-0g/rollup-as-a-service/caldera-on-0g-da
Smart Contract Development
‚Äã
Deploy with Hardhat
:
// hardhat.config.js
module
.
exports
=
{
networks
:
{
testnet
:
{
url
:
"https://evmrpc-testnet.0g.ai"
,
chainId
:
16602
,
accounts
:
[
"YOUR_PRIVATE_KEY"
]
}
,
mainnet
:
{
url
:
"https://evmrpc.0g.ai"
,
chainId
:
16661
,
accounts
:
[
"YOUR_PRIVATE_KEY"
]
}
}
}
;
Deploy with Foundry
:
# Testnet
forge create --rpc-url https://evmrpc-testnet.0g.ai \
--private-key YOUR_PRIVATE_KEY \
src/MyContract.sol:MyContract
# Mainnet
forge create --rpc-url https://evmrpc.0g.ai \
--private-key YOUR_PRIVATE_KEY \
src/MyContract.sol:MyContract
Documentation
:
https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/deploy-contracts
Key Concepts
‚Äã
AI Alignment
‚Äã
Documentation
:
https://docs.0g.ai/concepts/ai-alignment
Monitor AI systems for proper behavior, safety, and alignment with human values.
Functions
:
Track model drift
Verify outputs
Monitor performance
Flag anomalies
DePIN (Decentralized Physical Infrastructure)
‚Äã
Documentation
:
https://docs.0g.ai/concepts/depin
Physical GPU infrastructure provided by decentralized partners.
Partners
:
io.net
: 300,000+ verified GPUs, 139 countries, 90% cost savings
Aethir
: 43,000+ enterprise GPUs, 3,000+ H100s/H200s, 99.99% uptime
Starter Kits & Examples
‚Äã
Compute Starter Kit
‚Äã
Quick Start (Recommended for Hackathons)
:
# Install global CLI
pnpm add @0glabs/0g-serving-broker -g
# Option 1: Web UI (fastest way to start)
0g-compute-cli ui start-web
# Open http://localhost:3090, connect wallet, start using AI
# Option 2: CLI for automation
0g-compute-cli setup-network  # Choose testnet/mainnet
0g-compute-cli login           # Connect your wallet
0g-compute-cli deposit --amount 10  # Fund account
0g-compute-cli inference list-providers  # See available services
OpenAI SDK Drop-in Replacement
:
from
openai
import
OpenAI
# Just change base_url and api_key!
client
=
OpenAI
(
api_key
=
"app-sk-<YOUR_SECRET>"
,
base_url
=
"<PROVIDER_URL>/v1/proxy"
)
# Same API as OpenAI
response
=
client
.
chat
.
completions
.
create
(
model
=
"<model_name>"
,
messages
=
[
{
"role"
:
"user"
,
"content"
:
"Hello!"
}
]
)
Storage Starter Kit
‚Äã
TypeScript Example
:
npm install @0glabs/0g-ts-sdk ethers
import
{
ZgFile
,
Indexer
}
from
"@0glabs/0g-ts-sdk"
;
import
{
ethers
}
from
"ethers"
;
const
provider
=
new
ethers
.
JsonRpcProvider
(
"https://evmrpc-testnet.0g.ai"
)
;
const
signer
=
new
ethers
.
Wallet
(
"YOUR_PRIVATE_KEY"
,
provider
)
;
const
indexer
=
new
Indexer
(
"https://indexer-storage-testnet-turbo.0g.ai"
)
;
// Upload file
const
file
=
await
ZgFile
.
fromFilePath
(
"/path/to/file"
)
;
const
[
tree
,
treeErr
]
=
await
file
.
merkleTree
(
)
;
console
.
log
(
"Root Hash:"
,
tree
?.
rootHash
(
)
)
;
const
[
tx
,
uploadErr
]
=
await
indexer
.
upload
(
file
,
"https://evmrpc-testnet.0g.ai"
,
signer
)
;
await
file
.
close
(
)
;
// Download file
const
err
=
await
indexer
.
download
(
rootHash
,
"/path/to/output"
,
true
)
;
Python Example
:
pip install 0g-storage-client
from
storage_client
import
ZgStorageClient
client
=
ZgStorageClient
(
rpc_endpoint
=
"https://evmrpc-testnet.0g.ai"
,
contract_address
=
"0x22E03a6A89B950F1c82ec5e74F8eCa321a105296"
,
private_key
=
"YOUR_PRIVATE_KEY"
)
# Upload
root_hash
=
client
.
upload_file
(
"/path/to/file"
)
# Download
client
.
download_file
(
root_hash
,
"/path/to/output"
)
Chain/Smart Contract Starter Kit
‚Äã
Hardhat Project
:
npm install --save-dev hardhat @nomicfoundation/hardhat-toolbox
npx hardhat init
// hardhat.config.js
require
(
"@nomicfoundation/hardhat-toolbox"
)
;
module
.
exports
=
{
solidity
:
"0.8.20"
,
networks
:
{
testnet
:
{
url
:
"https://evmrpc-testnet.0g.ai"
,
chainId
:
16602
,
accounts
:
[
process
.
env
.
PRIVATE_KEY
]
}
}
}
;
npx hardhat compile
npx hardhat run scripts/deploy.js --network testnet
Foundry Project
:
forge init my-project
cd my-project
# Deploy
forge create --rpc-url https://evmrpc-testnet.0g.ai \
--private-key $PRIVATE_KEY \
src/Counter.sol:Counter
# Verify interaction
cast call <CONTRACT_ADDRESS> "number()" --rpc-url https://evmrpc-testnet.0g.ai
SDK Examples
‚Äã
TypeScript SDK:
https://github.com/0gfoundation/0g-ts-sdk/tree/main/examples
Go SDK:
https://github.com/0gfoundation/0g-storage-client/tree/main/examples
Community Projects
‚Äã
Awesome 0G Repository
:
https://github.com/0gfoundation/awesome-0g
Curated list of community projects, tools, and resources built on 0G.
Quick Reference
‚Äã
Add Network to MetaMask
‚Äã
Testnet
:
await
window
.
ethereum
.
request
(
{
method
:
'wallet_addEthereumChain'
,
params
:
[
{
chainId
:
'0x40EA'
,
chainName
:
'0G-Galileo-Testnet'
,
nativeCurrency
:
{
name
:
'0G'
,
symbol
:
'0G'
,
decimals
:
18
}
,
rpcUrls
:
[
'https://evmrpc-testnet.0g.ai'
]
,
blockExplorerUrls
:
[
'https://chainscan-galileo.0g.ai'
]
}
]
}
)
;
Mainnet
:
await
window
.
ethereum
.
request
(
{
method
:
'wallet_addEthereumChain'
,
params
:
[
{
chainId
:
'0x4125'
,
chainName
:
'0G Mainnet'
,
nativeCurrency
:
{
name
:
'0G'
,
symbol
:
'0G'
,
decimals
:
18
}
,
rpcUrls
:
[
'https://evmrpc.0g.ai'
]
,
blockExplorerUrls
:
[
'https://chainscan.0g.ai'
]
}
]
}
)
;
Common Commands
‚Äã
Storage Upload (CLI)
:
0g-storage-client upload \
--url https://evmrpc-testnet.0g.ai \
--key YOUR_PRIVATE_KEY \
--indexer https://indexer-storage-testnet-turbo.0g.ai \
--file /path/to/file
Storage Download (CLI)
:
0g-storage-client download \
--indexer https://indexer-storage-testnet-turbo.0g.ai \
--root ROOT_HASH \
--file output.dat
Check Node Status
:
# Validator
0gchaind status 2>&1 | jq .
# Storage Node
curl http://localhost:5678/status
# DA Node
curl http://localhost:34000/health
Community & Support
‚Äã
Official Links
‚Äã
Documentation
:
https://docs.0g.ai
Website
:
https://0g.ai
GitHub
:
https://github.com/0gfoundation
Discord
:
https://discord.gg/0gLabs
Twitter/X
:
https://x.com/0g_Labs
Getting Help
‚Äã
Documentation:
https://docs.0g.ai/developer-hub/getting-started
Discord Developer Channel:
https://discord.gg/0gLabs
GitHub Issues: Create issues in respective repositories
Vision & Mission
‚Äã
Mission
: Make AI a Public Good
Vision
: Democratized, transparent, fair, and secure AI infrastructure
Approach
:
Open infrastructure
Community ownership
Economic alignment
Technical excellence
Documentation
:
https://docs.0g.ai/introduction/vision-mission
Additional Resources
‚Äã
Node Sale Information
‚Äã
Documentation
:
https://docs.0g.ai/node-sale/node-sale-index
Security
‚Äã
Documentation
:
https://docs.0g.ai/resources/security
Contributing
‚Äã
Documentation
:
https://docs.0g.ai/resources/how-to-contribute
Glossary
‚Äã
Documentation
:
https://docs.0g.ai/resources/glossary
This context page is automatically maintained to provide AI coding assistants with comprehensive, up-to-date information about 0G infrastructure. All information is sourced from official documentation at
https://docs.0g.ai
.


========== [ https://docs.0g.ai/concepts/ai-alignment ] ==========

Concepts
AI Alignment Nodes
On this page
AI Alignment Nodes: Ensuring Safe Decentralized AI
What are AI Alignment Nodes?
‚Äã
AI Alignment Nodes are specialized network participants that monitor and ensure the proper behavior of AI systems and network protocols within the 0G ecosystem. They serve as the guardians of network integrity, verifying that all components operate according to their intended specifications.
Why AI Alignment Matters
As AI becomes more powerful, ensuring it remains aligned with human values and operates safely becomes critical üõ°Ô∏è
The Problem with Centralized AI
‚Äã
Traditional AI systems face several alignment challenges:
Lack of Transparency
: Black box operations with no external oversight
Single Point of Control
: Centralized entities make all decisions about AI behavior
Limited Accountability
: No mechanism for community oversight or intervention
Potential for Misalignment
: AI systems may drift from their intended purpose without detection
How AI Alignment Nodes Work
‚Äã
AI Model Monitoring
‚Äã
As 0G's on-chain AI capabilities expand, Alignment Nodes will:
Track Model Drift
: Detect when AI models deviate from expected behavior
Verify Outputs
: Ensure AI-generated results meet quality and safety standards
Monitor Performance
: Track AI system efficiency and accuracy over time
Flag Anomalies
: Alert the network to unusual or potentially harmful AI behavior
Network Security
‚Äã
Alignment Nodes contribute to overall network security by:
Identifying Protocol Violations
: Detecting when nodes fail to follow network rules
Reporting Malicious Behavior
: Flagging potential attacks or bad actors
Maintaining Ethical Standards
: Ensuring AI operations align with community values
Supporting Governance
: Providing data for network governance decisions
The Future of Decentralized AI Safety
‚Äã
AI Alignment Nodes represent a critical innovation in ensuring that decentralized AI systems remain safe, transparent, and aligned with human values. As AI capabilities expand, these nodes will become increasingly important for:
Scalable Oversight
‚Äã
Automated Monitoring
: AI-powered oversight that scales with network growth
Distributed Governance
: Community-driven decisions about AI behavior
Continuous Learning
: Alignment systems that improve over time
Global Participation
: Worldwide network of AI safety monitors
Innovation Enablement
‚Äã
By providing robust safety guarantees, Alignment Nodes enable:
Faster AI Development
: Developers can build with confidence in safety systems
Greater Public Trust
: Transparent oversight builds user confidence
Regulatory Compliance
: Meeting emerging AI safety regulations
Ecosystem Growth
: Safe AI attracts more users and developers
Getting Started
‚Äã
Interested in contributing to AI safety through Alignment Nodes?
AI Alignment Node
- Learn more about the AI Alignment Node.
Building safe AI for everyone, together.


========== [ https://docs.0g.ai/concepts/chain ] ==========

Concepts
0G Chain
On this page
0G Chain: The Fastest Modular AI Chain
The Problem with AI on Blockchain
‚Äã
Try running an AI model on Ethereum today:
Cost
: $1M+ in gas fees for a simple model
Speed
: 15 transactions per second (AI needs thousands)
Data
: Can't handle AI's massive data requirements
What is 0G Chain?
‚Äã
0G Chain is a blockchain built specifically for AI applications. Think of it as Ethereum, but optimized for AI workloads with significantly higher throughput.
EVM Compatibility
Your existing Ethereum code works without changes ü§ù
How 0G Chain Works
‚Äã
Modular Architecture
‚Äã
0G Chain features an advanced modular design that distinctly separates consensus from execution. This separation into independent, yet interconnected, layers is a cornerstone of 0G Chain's architecture, delivering enhanced flexibility, scalability, and a faster pace of innovation.
Architecture Overview
:
Consensus Layer
: Dedicated to achieving network agreement. It manages validator coordination, block production, and ensures the overall security and finality of the chain.
Execution Layer
: Focused on state management. It handles smart contract execution, processes transactions, and maintains compatibility with the EVM (Ethereum Virtual Machine).
Key Technical Advantages
:
Independent Upgradability
: The execution layer can rapidly incorporate new EVM features (such as EIP-4844, account abstraction, or novel opcodes) without requiring changes to the underlying consensus mechanism.
Focused Optimization
: Conversely, the consensus layer can be upgraded with critical performance or security enhancements without impacting the EVM or ongoing execution processes.
Accelerated Development
: This decoupling allows for parallel development and faster iteration cycles for both layers, leading to quicker adoption of new technologies and improvements in both performance and features.
This design makes 0G Chain flexible and fast. When new blockchain features come out, we can add them quickly without breaking anything. This keeps 0G optimized for AI while staying up-to-date with the latest technology.
Optimized Consensus
‚Äã
0G Chain employs a highly optimized version of CometBFT (formerly Tendermint) as its consensus mechanism, with meticulously tuned parameters that achieve maximum performance while maintaining security. The system features carefully calibrated block production intervals and timeout configurations that work together to deliver high throughput, ensure network stability, and enable faster consensus rounds‚Äîall without compromising the fundamental safety guarantees.
These optimizations enable 0G Chain to achieve maximum performance:
11,000 TPS per Shard
: Current throughput significantly exceeds traditional blockchain networks
Sub-second Finality
: Near-instant transaction confirmation for AI applications
Consistent Performance
: Maintains high throughput even under heavy network load
Scaling Roadmap
‚Äã
DAG-Based Consensus
: Transitioning to Directed Acyclic Graph (DAG) based consensus for exponentially higher efficiency
Parallel transaction processing capabilities
Elimination of sequential block limitations
Shared Security Model
: Implementing shared staking mechanisms to enhance network security
Validators can secure multiple services simultaneously
Increased capital efficiency for stakers
Technical Deep Dive
‚Äã
How does 0G achieve high throughput?
Currently achieves 11,000 TPS per Shard through:
Optimized CometBFT
: Highly efficient consensus based on Tendermind
Efficient block production
: Tuned for AI-scale data processing
Fast finality
: Sub-second transaction confirmation
Future scaling
will add:
Multiple parallel consensus networks
Dynamic capacity expansion
Automatic load balancing
How does the validator system work?
Staking & Consensus
:
Validators stake 0G tokens to participate
CometBFT ensures Byzantine fault tolerance
Rewards
:
Block production rewards
Transaction fee collection
Staking yields proportional to stake size
Node Selection
:
VRF (Verifiable Random Function) for fair validator selection
Prevents collusion and ensures decentralization
What makes 0G different from other fast chains?
Unlike general-purpose "fast" blockchains:
AI-First Design
: Data structures optimized for AI workloads
Modular Architecture
: Upgrade components independently
EVM + More
: Start with Ethereum compatibility, expand to other VMs
Purpose-Built
: Not retrofitted - designed from scratch for AI
0G Chain's modular architecture enables seamless integration with storage, compute, and DA layers
Validator Participation
‚Äã
Validators earn rewards through:
Block rewards
: For producing valid blocks
Transaction fees
: From network usage
Staking rewards
: Based on stake size and uptime
Validator reward and penalty structure in the 0G network
Frequently Asked Questions
‚Äã
Is 0G Chain truly decentralized?
Yes! 0G Chain operates with a permissionless, globally distributed validator set using proof-of-stake consensus. No single entity controls the network.
Do I need to rewrite my Ethereum dApp?
No! Full EVM compatibility means your Solidity code deploys without changes. The only differences you'll notice are speed and cost improvements.
Why is it faster than Ethereum?
0G Chain is purpose-built for AI workloads, while Ethereum is general-purpose. We achieve speed through:
Optimized consensus mechanism (CometBFT)
AI-specific data structures
Focused use case optimization
Next Steps
‚Äã
Ready to build? Start here:
Quick Start Guide
- Deploy in 5 minutes
Migration from Ethereum
- Move existing dApps
Technical Whitepaper
- Deep architecture details
0G Chain: Where AI meets blockchain at scale.


========== [ https://docs.0g.ai/concepts/compute ] ==========

Concepts
0G Compute Network
On this page
0G Compute Network: Decentralized AI Computing
In today's world, AI models are transforming industries, driving innovation, and powering new applications. However, running advanced AI models for your application faces several obstacles:
High Costs
: Enterprise AI services require significant monthly commitments
Complex Setup
: Cloud GPU configuration requires technical expertise
Vendor Lock-in
: Limited flexibility when switching providers
The result?
AI computing remains inaccessible for many developers and startups.
What is 0G Compute?
‚Äã
0G Compute is a decentralized framework that provides AI computing capabilities to our community. It forms a crucial part of deAIOS. 0G Compute is a decentralized marketplace where GPU owners sell computing power to developers who need it - think Uber for AI computing.
Key difference
: Instead of renting from AWS/Google with high costs and lock-in, you access a global GPU network that's 90% cheaper with pay-per-use pricing.
How It Works
‚Äã
For AI Users
‚Äã
Deposit Funds
: Pre-fund your account with pay-as-you-go credits
Request Service
: Send your AI request (inference, training, etc.)
Get Results
: Receive output from the best available GPU
Automatic Payment
: Pay only for actual compute used
For GPU Owners
‚Äã
Register Hardware
: List your GPU specs and availability
Set Your Price
: Competitive marketplace pricing
Process Requests
: Automatic job allocation
Instant Earnings
: Get paid immediately upon completion
Technical Implementation
‚Äã
Smart Contract Escrow
: Funds secured until service delivery
Signed Transactions
: Cryptographic verification of all interactions
ZK-Proof Settlement
: 100x lower transaction costs through compressed proofs
What makes it trustless?
Like eBay with automatic escrow - the smart contract ensures:
Payment only after service delivery
Both parties must fulfill obligations
No intermediary can interfere
This means no one can censor your AI usage, freeze your account, or change terms suddenly.
Why Choose 0G Compute?
‚Äã
üí∞ Key Advantages
‚Äã
Feature
Traditional Cloud
0G Compute
Pricing Model
Fixed monthly costs
Pay-per-use
Provider Options
Limited vendors
Global GPU network
üåê Access From Anywhere
‚Äã
Blockchains
: Direct integration with Ethereum, Solana, any chain
Traditional Apps
: Simple REST API using 0G SDK
üîê Your Data, Your Control
‚Äã
No data retention by providers
Verifiable computation proofs - Supports TEEML, OPML & ZKML
Common Questions
‚Äã
What about reliability?
Built-in redundancy:
Automatic failover to next provider
Thousands of providers globally
Can I run proprietary models?
Yes. Upload any model, set requirements and pricing, start serving requests. Perfect for specialized use cases.
How does pricing work?
Pure pay-per-use:
No subscriptions
Competitive market-driven pricing
Transparent costs visible upfront
Get Started
‚Äã
üìö Technical Deep Dive
‚Äã
Architecture and implementation details
‚Üí
Technical Documentation
üöÄ For Developers
‚Äã
Start using AI compute in 5 minutes
‚Üí
Quick Start Guide
üíé For GPU Owners
‚Äã
Turn idle hardware into income
‚Üí
Become a Provider
0G Compute: Making AI accessible to everyone.


========== [ https://docs.0g.ai/concepts/da ] ==========

Concepts
0G DA
On this page
0G DA: Infinitely Scalable and Programmable Data Availability
The Rise of Data Availability Layers
‚Äã
Data availability (DA) refers to proving that data is readily accessible, verifiable, and retrievable. For example, Layer 2 rollups such as Arbitrum or Base reduce the burden on Ethereum by handling transactions off-chain and then publishing the data back to Ethereum, thereby freeing up L1 throughput and reducing gas costs. The transaction data, however, still needs to be made available so that anyone can validate or challenge the transactions through fraud proofs during the challenge period.
As such, DA is crucial to blockchains as it allows for full validation of the blockchain's history and current state by any participant, thus maintaining the decentralized and trustless nature of the network. Without this, validators would not be able to independently verify the legitimacy of transactions and blocks, leading to potential issues like fraud or censorship.
This led to the emergence of Data Availability Layers (DALs), which provide a significantly more efficient manner of storing and verifying data than publishing directly to Ethereum. DALs are critical for several reasons:
Scalability
: DALs allow networks to process more transactions and larger datasets without overwhelming the system, reducing the burden on network nodes and significantly enhancing network scalability.
Increased Efficiency
: DALs optimize how and where data is stored and made available, increasing data throughput and reducing latency while also minimizing associated costs.
Interoperability & Innovation
: DALs that can interact with multiple ecosystems enable fast and highly secure interoperability for data and assets.
However, it's worth noting that not all DALs are built equally.
The Challenge Today
‚Äã
Existing DALs tend to require that data be simultaneously sent to all of their network nodes, preventing horizontal scalability and limiting network speed to its slowest node. They also do not have built-in storage systems, requiring connectivity to external systems that impact throughput, latency, and cost.
Additionally, 0G inherits Ethereum's security, while other systems rely upon their own security mechanisms that fall short. This is significant because Ethereum's network is secured by over 34 million ETH staked, representing approximately $80 billion in cryptoeconomic security at the time of writing. In contrast, competitors rely on security mechanisms that, at best, represent only a fraction of Ethereum's total security. This gives 0G a distinct advantage, as it leverages the vast economic incentives and decentralization of Ethereum's staking system, providing a level of protection that competitors cannot match.
Even more issues exist, including EigenDA's lack of randomization over its data committees. As data committees are core to a DA system's integrity, a lack of randomization means that collusion is theoretically possible for malicious nodes to predict when they might be on a committee together.
0G's core differentiation is massive throughput and scalability.
This is possible through 0G's unique design includes a built-in storage system and horizontally scalable consensus design, alongside other clever design mechanisms that we'll cover below.
The result is that 0G serves as the foundational layer for decentralized AI applications, bringing on-chain AI and more to life.
Why 0G
‚Äã
There are 4 differentiators of 0G worth highlighting:
1. Infinitely Scalable DA
‚Äã
0G's infinitely scalable DAL can quickly query or confirm data as valid, whether data is held by 0G Storage, or external Web2 or Web3 databases. Infinite scalability comes from the ability to continuously add new consensus networks, supporting workloads that far surpass the capacity of existing systems.
2. Modular and Layered Architecture
‚Äã
0G's design decouples storage, data availability, and consensus, allowing each component to be optimized for its specific function. Data availability is ensured through redundancy, with data distributed across decentralized Storage Nodes. Cryptographic proofs (like Merkle trees and zk-proofs) verify data integrity at regular intervals, automatically replacing nodes that fail to produce valid proofs. And combined with 0G's ability to keep adding new consensus networks that scale with demand, 0G can scale efficiently and is ideal for complex AI workflows and large-scale data processing.
3. Decentralized AI Operating System & High Throughput
‚Äã
0G is the first decentralized AI operating system (deAIOS) designed to give users control over their data, while providing the infrastructure necessary to handle the massive throughput demands of AI applications. Beyond its modular architecture and infinite consensus layers, 0G achieves high throughput through parallel data processing, enabled by erasure coding, horizontally scalable consensus networks, and more. With a demonstrated throughput of 50 Gbps on the Galileo Testnet, 0G seamlessly supports AI workloads and other high-performance needs, including training large language models and managing AI agent networks.
These differentiators make 0G uniquely positioned to tackle the challenges of scaling AI on a decentralized platform, which is critical for the future of Web3 and decentralized intelligence.
How Does This Work?
‚Äã
As covered in
0G Storage
, data within the 0G ecosystem is first erasure-coded and split into "data chunks," which are then distributed across various Storage Nodes in the 0G Storage network.
To ensure data availability, 0G uses
Data Availability Nodes
that are randomly chosen using a Verifiable Random Function (VRF). A VRF generates random values in a way that is unpredictable yet verifiable by others, which is important as it prevents potentially malicious nodes from collusion.
These DA nodes work together in small groups, called quorums, to check and verify the stored data. The system assumes that most nodes in each group will act honestly, known as an "honest majority" assumption.
The consensus mechanism used by 0G is fast and efficient due to its sampling-based approach. Rather than verifying all data, DA nodes sample portions of it, drastically reducing the data they need to handle. Once enough nodes agree that the sampled data is available and correct, they submit availability proofs to the 0G Consensus network. This lightweight, sample-driven approach enables faster verification while maintaining strong security.
Validators in the 0G Consensus network verify and finalize DA proofs
Validators in the 0G Consensus network, who are separate from the DA nodes, verify and finalize these proofs. Although DA nodes ensure data availability, they do not directly participate in the final consensus process, which is the responsibility of 0G validators. Validators use a shared staking mechanism where they stake 0G tokens on a primary network (likely Ethereum). Any slashable event across connected networks leads to slashing on the main network, securing the system's scalability while maintaining robust security.
This is a key mechanism that allows for the system to scale infinitely while maintaining data availability. In return, validators engaged in shared staking receive 0G tokens on any network managed, which can then be burnt in return for 0G tokens on the mainnet.
The lightweight, sample-driven consensus approach
Use Cases
‚Äã
0G DA offers an infinitely scalable and high-performance DA solution for a wide range of applications across Web3, AI, and more.
L1s / L2s
‚Äã
Layer 1 and Layer 2 chains can utilize 0G DA to handle data availability and storage requirements for decentralized AI models, large datasets, and on-chain applications. Existing partners include networks like
Polygon, Optimism, Arbitrum, Fuel, Manta Network
,
and countless more
, which leverage 0G's scalable infrastructure to store data more efficiently and support fast retrieval.
Decentralized Shared Sequencers
‚Äã
Decentralized Shared Sequencers help order L2 transactions before final settlement on Ethereum. By integrating 0G DA, shared sequencers can streamline data across multiple networks in a decentralized manner, unlike existing sequencers which are often centralized. This also means fast and secure data transfers between L2s.
Bridges
‚Äã
Cross-chain bridges benefit from 0G DA's scalable storage and data availability features. Networks can store and retrieve state data using 0G DA, making state migration between networks faster and more secure. For example, a network can confirm a user's assets and transfer them securely to another chain using 0G's highly efficient data verification.
Rollups-as-a-Service (RaaS)
‚Äã
0G DA can serve as a reliable DA solution for RaaS providers like
Caldera and AltLayer
, enabling seamless configuration and deployment of rollups. With 0G DA's highly scalable infrastructure, RaaS providers can ensure the secure availability of data across multiple rollups without compromising performance.
DeFi
‚Äã
0G's DA infrastructure is ideally suited for DeFi applications that require fast settlement and high-frequency trading. For example, by storing order book data on 0G, DeFi projects can achieve faster transaction throughput and enhanced scalability across L2s and L3s.
On-Chain Gaming
‚Äã
On-chain gaming platforms rely on cryptographic proofs and metadata related to player assets, actions, and scores. 0G DA's ability to handle large volumes of data securely and efficiently makes it an optimal solution for gaming applications that require reliable data storage and fast retrieval.
Data Markets
‚Äã
Web3 data markets can benefit from 0G DA by storing datasets on-chain. The decentralized storage and retrieval capabilities of 0G enable real-time updates and querying of data, providing a reliable solution for data market platforms.
AI & Machine Learning
‚Äã
0G DA is particularly focused on supporting decentralized AI, allowing full AI models and vast datasets to be stored and accessed on-chain. This infrastructure is essential for advanced AI applications that demand high data throughput and availability, such as training large language models (LLMs) and managing entire AI agent Networks.
Getting Started
‚Äã
Ready to integrate 0G DA into your project?
Run a DA Node
:
Node operator guide
Integration Guide
:
Developer documentation
Technical Deep Dive
:
DA architecture details
0G DA: Bringing infinite scalability to decentralized data availability.


========== [ https://docs.0g.ai/concepts/depin ] ==========

Concepts
DePIN Providers
On this page
DePIN Providers: Decentralized Infrastructure Networks
What is DePIN?
‚Äã
DePIN (Decentralized Physical Infrastructure Networks) represents a revolutionary approach to building and scaling infrastructure networks. By combining physical hardware with blockchain incentives, DePIN networks create more open, decentralized, and cost-effective infrastructure across various sectors.
Why DePIN Matters
Traditional infrastructure is expensive and centralized. DePIN democratizes access to computing power by utilizing underutilized hardware worldwide üöÄ
How 0G Leverages DePIN Infrastructure
‚Äã
0G Compute utilizes DePIN infrastructure to provide scalable, cost-effective computing resources for AI and blockchain applications. Rather than building massive centralized data centers, 0G partners with decentralized networks that aggregate distributed computing resources.
Key Benefits of DePIN Integration
‚Äã
Cost Efficiency
: DePIN networks utilize existing underutilized hardware, reducing costs by up to 80% compared to traditional cloud providers.
Global Reach
: Distributed nodes worldwide provide low-latency access to computing resources regardless of geographic location.
Scalability
: New resources can be added to the network without physical infrastructure expansion, enabling rapid scaling.
Resilience
: No single point of failure with resources distributed across thousands of independent nodes.
0G's DePIN Partners
‚Äã
io.net
‚Äã
io.net operates the world's largest decentralized GPU network, specifically designed for machine learning and AI workloads. The network aggregates over 300,000 verified GPUs from independent data centers, crypto miners, and consumer hardware across 139 countries.
Key Capabilities:
6,000+ cluster-ready GPUs including NVIDIA H100s
90% cost savings compared to traditional cloud providers
90-second cluster deployment
Built on Ray.io framework (same as OpenAI's GPT training)
Aethir
‚Äã
Aethir provides enterprise-grade GPU-as-a-Service through its decentralized cloud infrastructure. Focused on AI, gaming, and virtualized computing, Aethir maintains over 43,000 enterprise-grade GPUs across 25 global locations.
Key Capabilities:
3,000+ NVIDIA H100s and H200s for advanced AI workloads
99.99% uptime with enterprise-grade reliability
Ultra-low latency for real-time applications
Proof of Rendering verification system
Technical Deep Dive
‚Äã
What makes DePIN different from traditional cloud?
Resource Utilization
: DePIN networks tap into idle hardware worldwide instead of building new data centers
Economic Model
: Token incentives create sustainable participation without massive capital expenditure
Geographic Distribution
: Resources are naturally distributed, reducing latency for global users
Permissionless Access
: Anyone can contribute resources or access compute power without gatekeepers
How do DePIN networks ensure quality?
Verification Systems
: Proof-of-Work, Proof-of-Capacity, and custom verification mechanisms
Quality Scoring
: Real-time monitoring and automated scoring of network participants
Economic Incentives
: Token rewards for good performance, slashing for poor performance
Redundancy
: Multiple nodes can handle the same task to ensure reliability
The Future of Decentralized Computing
‚Äã
The integration of 0G with DePIN infrastructure creates a comprehensive Web3 computing stack that addresses the growing demand for AI and blockchain applications. This partnership model enables:
Affordable AI
: Making advanced AI computing accessible to startups and developers worldwide
Scalable Infrastructure
: Meeting growing compute demands without massive capital investment
Global Accessibility
: Bringing high-performance computing to underserved regions
Sustainable Growth
: Utilizing existing hardware more efficiently
Getting Started
‚Äã
Ready to build with decentralized infrastructure? Explore these resources:
0G Compute Network
- Learn about 0G's compute layer
Developer SDK
- Start building with 0G Compute
Developer Hub
- Get started building on 0G
Powering the future of AI with decentralized infrastructure.


========== [ https://docs.0g.ai/concepts/inft ] ==========

Concepts
INFT
On this page
INFTs: Intelligent NFTs for AI
Traditional NFTs can't handle AI agents. When you "own" an AI agent NFT today, you only own a pointer to some metadata - not the actual intelligence. The AI doesn't transfer with the NFT.
What are INFTs?
‚Äã
INFTs (Intelligent Non-Fungible Tokens)
solve this problem. They're a new type of NFT specifically designed to tokenize AI agents with their complete intelligence intact.
New to AI tokenization?
Traditional approach:
NFT points to AI metadata stored somewhere
When you buy the NFT, you don't get the actual AI
The intelligence stays with the original creator
You can't actually use the AI agent
INFT approach:
NFT contains encrypted AI intelligence
When transferred, the AI moves with it
New owner gets full access to the AI agent
Complete ownership of AI capabilities
Why INFTs Matter
‚Äã
True AI Ownership
‚Äã
Unlike regular NFTs that just point to metadata, INFTs contain the actual AI agent. When you own an INFT, you own the complete intelligence, not just a certificate.
Privacy-First Design
‚Äã
AI agents often contain sensitive data or proprietary algorithms. INFTs keep this data encrypted throughout the entire lifecycle - only the owner can access it.
Secure Transfers
‚Äã
When an INFT changes hands, both the ownership AND the encrypted AI intelligence transfer together. The new owner gets a fully functional AI agent.
Decentralized Storage
‚Äã
INFTs leverage 0G Storage to keep AI agents permanently available without relying on centralized servers that could go offline.
Real-World Use Cases
‚Äã
Use Case
How INFTs Help
Example
AI Trading Bots
Own and transfer profitable trading strategies
DeFi trading bot with proven track record
Personal Assistants
Trained AI agents that know your preferences
AI that learned your workflow and habits
Game Characters
Intelligent NPCs with unique personalities
AI companion that evolved through gameplay
Content Creation
AI models trained for specific styles
AI artist trained on your creative style
Research Tools
Specialized AI for domain-specific tasks
Medical AI trained on specific datasets
How It Works
‚Äã
Create
: Build and train your AI agent
Encrypt
: Secure the AI's intelligence with encryption
Mint
: Create an INFT containing the encrypted AI
Own
: Have complete ownership and control over the AI agent
Technical Foundation
‚Äã
INFTs are built on
ERC-7857
, a new NFT standard that extends ERC-721 with:
Encrypted metadata storage
for protecting AI intelligence
Secure re-encryption
for safe ownership transfers
Oracle verification
to ensure transfer integrity
Authorized usage
for AI-as-a-Service models
Powered by 0G
‚Äã
INFTs leverage the complete 0G ecosystem:
Component
Role
Benefit
0G Storage
Encrypted AI storage
Permanent, decentralized availability
0G Chain
Smart contract execution
Fast, low-cost INFT operations
0G Compute
Secure AI inference
Private execution environment
0G DA
Transfer verification
Guaranteed data availability
Getting Started
‚Äã
For AI Developers
‚Äã
Transform your AI agents into tradeable assets while maintaining privacy and control.
Build INFTs
- Complete development guide
Next Steps
Ready to dive deeper? Check out the
complete INFT documentation
for technical details, implementation guides, and real-world examples.


========== [ https://docs.0g.ai/concepts/storage ] ==========

Concepts
0G Storage
On this page
0G Storage: Built for Massive Data
Current storage options force impossible tradeoffs:
Cloud providers
: Fast but expensive with vendor lock-in
Decentralized options
: Either slow (IPFS), limited (Filecoin), or prohibitively expensive (Arweave)
What is 0G Storage?
‚Äã
0G Storage breaks these tradeoffs - a decentralized storage network that's as fast as AWS S3 but built for Web3. Purpose-designed for AI workloads and massive datasets.
New to decentralized storage?
Traditional storage (like AWS):
One company controls your data
They can delete it, censor it, or change prices
Single point of failure
Decentralized storage (like 0G):
Data spread across thousands of nodes
No single entity can delete or censor
Always available, even if nodes go offline
Why Choose 0G Storage?
‚Äã
üöÄ The Complete Package
‚Äã
What You Get
Why It Matters
95% lower costs than AWS
Sustainable for large datasets
Instant retrieval
No waiting for critical data
Structured + unstructured data
One solution for all storage needs
Universal compatibility
Works with any blockchain or Web2 app
Proven scale
Already handling TB-scale workloads
How It Works
‚Äã
0G Storage is a distributed data storage system designed with on-chain elements to incentivize storage nodes to store data on behalf of users. Anyone can run a storage node and receive rewards for maintaining one.
Technical Architecture
‚Äã
0G Storage uses a two-lane system:
üì§ Data Publishing Lane
Handles metadata and availability proofs
Verified through 0G Consensus network
Enables fast data discovery
üíæ Data Storage Lane
Manages actual data storage
Uses erasure coding: splits data into chunks with redundancy
Even if 30% of nodes fail, your data remains accessible
Automatic replication maintains availability
Storage Layers for Different Needs
‚Äã
üìÅ Log Layer (Immutable Storage)
‚Äã
Perfect for
: AI training data, archives, backups
Append-only (write once, read many)
Optimized for large files
Lower cost for permanent storage
Use cases
:
ML datasets
Video/image archives
Blockchain history
General Large file storage
üîë Key-Value Layer (Mutable Storage)
‚Äã
Perfect for
: Databases, dynamic content, state storage
Update existing data
Fast key-based retrieval
Real-time applications
Use cases
:
On-chain databases
User profiles
Game state
Collaborative documents
How Storage Providers Earn
‚Äã
0G Storage is maintained by a network of miners incentivized to store and manage data through a unique consensus mechanism known as
Proof of Random Access (PoRA)
.
How It Works
‚Äã
Random Challenges
: System randomly asks miners to prove they have specific data
Cryptographic Proof
: Miners must generate a valid hash (like Bitcoin mining)
Quick Response
: Must respond fast to prove data is readily accessible
Fair Rewards
: Successful proofs earn storage fees
What's PoRA in simple terms?
Imagine a teacher randomly checking if students did their homework:
Teacher picks a random student (miner)
Asks for a specific page (data chunk)
Student must show it quickly
If correct, student gets rewarded
This ensures miners actually store the data they claim to store.
Fair Competition = Fair Reward
‚Äã
To promote fairness, the mining range is capped at 8 TB of data per mining operation.
Why 8TB limit?
Small miners can compete with large operations
Prevents centralization
Lower barrier to entry
For large operators
: Run multiple 8TB instances.
For individuals
: Focus on single 8TB range, still profitable
How 0G Compares
‚Äã
Solution
Best For
Limitation
0G Storage
AI/Web3 apps needing speed + scale
Newer ecosystem
AWS S3
Traditional apps
Centralized, expensive
Filecoin
Cold storage archival
Slow retrieval, unstructured only
Arweave
Permanent storage
Extremely expensive
IPFS
Small files, hobby projects
Very slow, no guarantees
0G's Unique Position
‚Äã
Only solution
supporting both structured and unstructured data
Instant access
unlike other decentralized options
Built for AI
from the ground up
Frequently Asked Questions
‚Äã
Is my data really safe if nodes go offline?
Yes! The erasure coding system ensures your data survives node failures. The network automatically maintains redundancy levels, so your data remains accessible even during significant outages.
How fast can I retrieve large files?
Parallel retrieval from multiple nodes
Bandwidth limited only by your connection
200 MBPS retrieval speed even at network congestion
CDN-like performance through geographic distribution
What happens to pricing as the network grows?
The network fee is fixed. All pricing is transparent and on-chain, preventing hidden fees or sudden changes.
Can I migrate from existing storage?
Yes, easily:
Keep existing infrastructure
Use 0G as overflow or backup
Gradually migrate based on access patterns
Get Started
‚Äã
üßë‚Äçüíª For Developers
‚Äã
Integrate 0G Storage in minutes
‚Üí
SDK Documentation
‚õèÔ∏è For Storage Providers
‚Äã
Earn by providing storage capacity
‚Üí
Run a Storage Node
0G Storage: Purpose-built for AI and Web3's massive data needs.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/avs/babylon-avs-on-0g-da ] ==========

Babylon AVS on 0G DA
Under construction...


========== [ https://docs.0g.ai/developer-hub/building-on-0g/avs/eigenlayer-avs-on-0g-da ] ==========

EigenLayer AVS on 0G DA
Under construction...


========== [ https://docs.0g.ai/developer-hub/building-on-0g/compute-network/account-management ] ==========

Developer Hub
Building on 0G
0G Compute
For Developers
Account
On this page
Account
The 0G Compute Network uses a unified account system for managing funds across services. This guide covers how to manage your accounts using different interfaces: Web UI, CLI, and SDK.
Overview
‚Äã
The account system provides a secure and flexible way to manage funds across different AI service providers.
Account Structure
‚Äã
Main Account
: Your primary wallet where funds are deposited. All deposits go here first, and you can withdraw funds from here back to your wallet.
Sub-Accounts
: Provider-specific accounts created automatically when you transfer funds to a provider. Each provider has a separate sub-account where funds are locked for their specific services.
Fund Flow
‚Äã
Deposit
: Transfer funds from your wallet to your Main Account
Transfer
: Move funds from Main Account to Provider Sub-Accounts
Usage
: Provider deducts from Sub-Account for services rendered
Refund Request
: Initiate refund from Sub-Account (enters 24-hour lock period)
Complete Refund
: After lock period expires, call retrieve-fund again to complete transfer back to Main Account
Withdraw
: Transfer funds from Main Account back to your wallet
Security Features
‚Äã
24-hour lock period
for refunds to protect providers from abuse
Single-use authentication
for each request to prevent replay attacks
On-chain verification
for all transactions ensuring transparency
Provider acknowledgment
required before first use of services
Prerequisites
‚Äã
Node.js >= 22.0.0
A wallet with 0G tokens (for testnet or mainnet)
EVM compatible wallet (for Web UI)
Choose Your Interface
‚Äã
Feature
Web UI
CLI
SDK
Setup time
~1 min
~2 min
~5 min
Visual dashboard
‚úÖ
‚ùå
‚ùå
Automation
‚ùå
‚úÖ
‚úÖ
App integration
‚ùå
‚ùå
‚úÖ
Web UI
CLI
SDK
Best for:
Quick account management with visual dashboard
Installation
‚Äã
pnpm add @0glabs/0g-serving-broker -g
Launch Web UI
‚Äã
0g-compute-cli ui start-web
Access the Web UI at
http://localhost:3090/wallet
where you can:
View your account balance in real-time
Deposit funds directly from your connected wallet
Transfer funds to provider sub-accounts
Monitor spending and usage
Request refunds with a visual interface
Best for:
Automation, scripting, and server environments
Installation
‚Äã
pnpm add @0glabs/0g-serving-broker -g
Setup Environment
‚Äã
Choose Network
‚Äã
0g-compute-cli setup-network
Login with Wallet
‚Äã
Enter your wallet private key when prompted. This will be used for account management and service payments.
0g-compute-cli login
CLI Commands
‚Äã
Deposit Funds
‚Äã
Add funds to your main account:
0g-compute-cli deposit --amount 10
Check Balance
‚Äã
View your account overview:
0g-compute-cli get-account
Example output:
Overview
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Balance                                          ‚îÇ Value (0G)                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Total                                            ‚îÇ 8.822778129999999663                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Locked (transferred to sub-accounts)             ‚îÇ 8.257334240000000491                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Available for transfer to sub-accounts           ‚îÇ 0.265443889999999960                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Inference sub-accounts
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Provider               ‚îÇ Balance (0G)                 ‚îÇ Requested Return to Main Account (0G)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0x924A2c71...          ‚îÇ 3.257334240000000047         ‚îÇ 0.000000000000000000                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0x960E74Fc...          ‚îÇ 3.000000000000000000         ‚îÇ 3.000000000000000000                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0x4f371f6e...          ‚îÇ 3.299999999999999822         ‚îÇ 0.000000000000000000                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Transfer to Provider
‚Äã
Before using a provider's service, transfer funds to their sub-account:
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 5
Request Refund
‚Äã
Withdraw unused funds from sub-accounts back to main account:
0g-compute-cli retrieve-fund
Note
: Refunds have a 24-hour lock period for security. After the lock period expires, you need to call this function again to complete the refund and transfer the funds back to your main account. You can check the remaining lock time using the
get-sub-account
command:
0g-compute-cli get-sub-account --provider <PROVIDER_ADDRESS>
Example output showing refund details:
Details of Each Amount Applied for Return to Main Account
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Amount (0G)                                      ‚îÇ Remaining Locked Time                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0.099785050000000000                             ‚îÇ 23h 43min 15s                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Withdraw to Wallet
‚Äã
Withdraw funds from main account to your wallet:
0g-compute-cli refund --amount 5
Best for:
Application integration and programmatic access
Installation
‚Äã
pnpm add @0glabs/0g-serving-broker
Initialize Broker
‚Äã
import
{
ethers
}
from
"ethers"
;
import
{
createZGComputeNetworkBroker
}
from
"@0glabs/0g-serving-broker"
;
const
provider
=
new
ethers
.
JsonRpcProvider
(
"https://evmrpc-testnet.0g.ai"
)
;
const
wallet
=
new
ethers
.
Wallet
(
process
.
env
.
PRIVATE_KEY
!
,
provider
)
;
const
broker
=
await
createZGComputeNetworkBroker
(
wallet
)
;
Check Account Balance
‚Äã
const
account
=
await
broker
.
ledger
.
getLedger
(
)
;
console
.
log
(
`
Total Balance:
${
ethers
.
formatEther
(
account
.
totalBalance
)
}
0G
`
)
;
console
.
log
(
`
Available:
${
ethers
.
formatEther
(
account
.
availableBalance
)
}
0G
`
)
;
Deposit Funds
‚Äã
await
broker
.
ledger
.
depositFund
(
10
)
;
// Deposit 10 0G
Transfer to Provider
‚Äã
const
providerAddress
=
"<PROVIDER_ADDRESS>"
;
const
amount
=
ethers
.
parseEther
(
"5"
)
;
// 5 0G
await
broker
.
ledger
.
transferFund
(
providerAddress
,
"inference"
,
amount
)
;
Check Sub-Account Details
‚Äã
const
[
subAccount
,
refunds
]
=
await
broker
.
inference
.
getAccountWithDetail
(
providerAddress
)
;
console
.
log
(
`
Sub-account balance:
${
ethers
.
formatEther
(
subAccount
.
balance
)
}
0G
`
)
;
const
{
account
:
subAccount
,
refunds
}
=
await
broker
.
fineTuning
.
getAccountWithDetail
(
providerAddress
)
;
console
.
log
(
`
Sub-account balance:
${
ethers
.
formatEther
(
subAccount
.
balance
)
}
0G
`
)
;
Request Refund
‚Äã
await
broker
.
ledger
.
retrieveFund
(
"inference"
)
;
await
broker
.
ledger
.
retrieveFund
(
"fine-tuning"
)
;
Withdraw to Wallet
‚Äã
await
broker
.
ledger
.
refund
(
5
)
;
// Withdraw 5 0G
Best Practices
‚Äã
For Inference Services
‚Äã
Deposit enough funds for expected usage
Transfer funds to providers you plan to use frequently
Keep some balance in sub-accounts for better response times
Monitor usage regularly
For Fine-tuning Services
‚Äã
Calculate dataset size before transferring funds
Transfer enough to cover the entire training job
Request refunds for unused funds after job completion
Troubleshooting
‚Äã
Insufficient Balance Error
Check which account needs funds:
Main account: Use
deposit
Sub-account: Use
transfer-fund
# Check all balances
0g-compute-cli get-account
# Deposit to main account if needed
0g-compute-cli deposit --amount 10
# Transfer to provider if needed
0g-compute-cli transfer-fund --provider <ADDRESS> --amount 5
Refund Not Available
Refunds have a 24-hour lock period. After the lock period expires, you need to call the retrieve-fund function again to complete the refund. Check the status:
0g-compute-cli get-sub-account --provider <PROVIDER_ADDRESS>
Look for "Remaining Locked Time" in the output.
Transaction Failed
Common causes:
Network issues - Check your RPC endpoint
Gas price too low - Increase gas price
Insufficient gas - Ensure wallet has enough for gas fees
# Specify custom gas price
0g-compute-cli deposit --amount 10 --gas-price 20000000000
Related Documentation
‚Äã
Inference Services
- Using AI inference with your funded accounts
Fine-tuning Services
- Training custom models with your funded accounts


========== [ https://docs.0g.ai/developer-hub/building-on-0g/compute-network/fine-tuning ] ==========

Developer Hub
Building on 0G
0G Compute
For Developers
Fine-tuning
On this page
Fine-tuning
Customize AI models with your own data using 0G's distributed GPU network.
Quick Start
‚Äã
Prerequisites
‚Äã
Node version >= 22.0.0
Install CLI
‚Äã
pnpm install @0glabs/0g-serving-broker -g
Set Environment
‚Äã
Choose Network
‚Äã
# Setup network
0g-compute-cli setup-network
Login with Wallet
‚Äã
Enter your wallet private key when prompted.
# Login with your wallet private key
0g-compute-cli login
Create Account & Add Funds
‚Äã
The Fine-tuning CLI requires an account to pay for service fees via the 0G Compute Network.
For detailed account management instructions, see
Account Management
.
# Deposit funds to your account
0g-compute-cli deposit --amount 3
# Transfer funds to a provider for fine-tuning
# IMPORTANT: You must specify --service fine-tuning, otherwise funds go to the inference sub-account
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 2 --service fine-tuning
tip
If you see
MinimumDepositRequired
when creating a task, it means you haven't transferred funds to the provider's
fine-tuning
sub-account. Make sure to include
--service fine-tuning
in the
transfer-fund
command.
List Providers
‚Äã
0g-compute-cli fine-tuning list-providers
The output will be like:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Provider 1                                       ‚îÇ 0x940b4a101CaBa9be04b16A7363cafa29C1660B0d       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Available                                        ‚îÇ ‚úì                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Provider x:
The address of the provider.
Available:
Indicates if the provider is available. If
‚úì
, the provider is available. If
‚úó
, the provider is occupied.
List Models
‚Äã
# List available models
0g-compute-cli fine-tuning list-models
üìã Available Models Summary
The CLI displays two categories of models: predefined models available across all providers and provider-specific models with unique capabilities.
Predefined Models
‚Äã
These are standard models available across all providers:
Model Name
Type
Price per Million Tokens
Description
Qwen2.5-0.5B-Instruct
Causal LM
0.5 0G
Qwen 2.5 instruction-tuned model (0.5B parameters). More details:
HuggingFace
Qwen3-32B
Causal LM
4 0G
Qwen 3 large language model (32B parameters). More details:
HuggingFace
The output consists of two main sections:
Predefined Models:
Models provided by the system as predefined options. They are built-in, curated, and maintained to ensure quality and reliability.
Provider's Model:
Models offered by external service providers. Providers may customize or fine-tune models to address specific needs.
Model Name Format
Use model names
without
the
Qwen/
prefix when specifying the
--model
parameter. For example:
‚úÖ
--model "Qwen2.5-0.5B-Instruct"
‚ùå
--model "Qwen/Qwen2.5-0.5B-Instruct"
Prepare Configuration File
‚Äã
Use the standard configuration template below and
only modify the parameter values
as needed. Do not add additional parameters.
Standard Configuration Template
‚Äã
{
"neftune_noise_alpha"
:
5
,
"num_train_epochs"
:
1
,
"per_device_train_batch_size"
:
2
,
"learning_rate"
:
0.0002
,
"max_steps"
:
3
}
Important Configuration Rules
Use the template above
- Copy the entire template
Only modify parameter values
- Do not add or remove parameters
Use decimal notation
- Write
0.0002
instead of
2e-4
for
learning_rate
Common mistakes to avoid:
‚ùå Adding extra parameters (e.g.,
"fp16": true
,
"bf16": false
)
‚ùå Removing existing parameters
‚ùå Using scientific notation like
2e-4
Adjustable Parameters
‚Äã
You can modify these parameter values based on your training needs:
Parameter
Description
Notes
neftune_noise_alpha
Noise injection for fine-tuning
0-10 (0 = disabled), typical: 5
num_train_epochs
Number of complete passes through the dataset
Positive integer, typical: 1-3 for fine-tuning
per_device_train_batch_size
Training batch size
1-4, reduce to 1 if out of memory
learning_rate
Learning rate (use decimal notation)
0.00001-0.001, typical: 0.0002
max_steps
Maximum training steps
-1 (use epochs) or positive integer
GPU Memory Management
If you encounter out-of-memory errors,
reduce batch size to 1
The provider automatically handles mixed precision training with
bf16
Note:
For custom models provided by third-party Providers, you can download the usage template including instructions on how to construct the dataset and training configuration using the following command:
0g-compute-cli fine-tuning model-usage --provider <PROVIDER_ADDRESS>  --model <MODEL_NAME>   --output <PATH_TO_SAVE_MODEL_USAGE>
Prepare Your Data
‚Äã
Your dataset must be in
JSONL format
with a
.jsonl
file extension
. Each line is a JSON object representing one training example.
Supported Dataset Formats
‚Äã
Format 1: Instruction-Input-Output
{
"instruction"
:
"Translate to French"
,
"input"
:
"Hello world"
,
"output"
:
"Bonjour le monde"
}
{
"instruction"
:
"Translate to French"
,
"input"
:
"Good morning"
,
"output"
:
"Bonjour"
}
{
"instruction"
:
"Summarize the text"
,
"input"
:
"Long article..."
,
"output"
:
"Brief summary"
}
Format 2: Chat Messages
{
"messages"
:
[
{
"role"
:
"user"
,
"content"
:
"What is 2+2?"
}
,
{
"role"
:
"assistant"
,
"content"
:
"2+2 equals 4."
}
]
}
{
"messages"
:
[
{
"role"
:
"user"
,
"content"
:
"Hello"
}
,
{
"role"
:
"assistant"
,
"content"
:
"Hi there! How can I help you?"
}
]
}
Format 3: Simple Text (for text completion)
{
"text"
:
"The quick brown fox jumps over the lazy dog."
}
{
"text"
:
"Machine learning is a subset of artificial intelligence."
}
Dataset Guidelines
‚Äã
File format
: Must be a
.jsonl
file (JSONL format)
Minimum examples
: At least 10 examples recommended for meaningful fine-tuning
Quality
: Ensure examples are accurate and representative of your use case
Consistency
: Use the same format throughout the dataset
Encoding
: UTF-8 encoding required
Create Task
‚Äã
Create a fine-tuning task. The fee will be
automatically calculated
by the broker based on the actual token count of your dataset.
Option A: Using local dataset file (Recommended)
The CLI will automatically upload the dataset to 0G Storage and create the task in one step:
0g-compute-cli fine-tuning create-task \
--provider <PROVIDER_ADDRESS> \
--model <MODEL_NAME> \
--dataset-path <PATH_TO_DATASET> \
--config-path <PATH_TO_CONFIG_FILE>
Option B: Using dataset root hash
If you prefer to upload the dataset separately first, or need to reuse the same dataset:
Upload your dataset to 0G Storage:
0g-compute-cli fine-tuning upload --data-path <PATH_TO_DATASET>
Output:
Root hash: 0xabc123...
Create the task using the root hash:
0g-compute-cli fine-tuning create-task \
--provider <PROVIDER_ADDRESS> \
--model <MODEL_NAME> \
--dataset <DATASET_ROOT_HASH> \
--config-path <PATH_TO_CONFIG_FILE>
Parameters:
Parameter
Description
--provider
Address of the service provider
--model
Name of the pretrained model (without
Qwen/
prefix)
--dataset-path
Path to local dataset file ‚Äî automatically uploads to 0G Storage (Option A)
--dataset
Root hash of the dataset on 0G Storage ‚Äî mutually exclusive with
--dataset-path
(Option B)
--config-path
Path to the training configuration file
--gas-price
Gas price (optional)
The output will be like:
Verify provider...
Provider verified
Creating task (fee will be calculated automatically)...
Fee will be automatically calculated by the broker based on actual token count
Created Task ID: 6b607314-88b0-4fef-91e7-43227a54de57
Note:
When creating a task for the same provider, you must wait for the previous task to be completed (status
Finished
) before creating a new task. If the provider is currently running other tasks, you will be prompted to choose between adding your task to the waiting queue or canceling the request.
Fee Calculation
‚Äã
The fine-tuning service fee is
automatically calculated
based on your dataset size and training configuration. The fee consists of two components:
Formula
‚Äã
Total Fee = Training Fee + Storage Reserve Fee
Where:
Training Fee
=
(tokenSize / 1,000,000) √ó pricePerMillionTokens √ó trainEpochs
Storage Reserve Fee
= Fixed amount based on model size
Components Explained
‚Äã
Component
Description
tokenSize
Total number of tokens in your dataset (automatically counted)
pricePerMillionTokens
Price per million tokens (model-specific, see
Predefined Models
)
trainEpochs
Number of training epochs (from your config)
Storage Reserve Fee
Fixed fee to reserve storage for the fine-tuned model:
‚Ä¢ Qwen3-32B (~900 MB LoRA): 0.09 0G
‚Ä¢ Qwen2.5-0.5B-Instruct (~100 MB LoRA): 0.01 0G
Example
‚Äã
For a dataset with 10,000 tokens, trained for 3 epochs on Qwen2.5-0.5B-Instruct:
Price per million tokens = 0.5 0G (see
Predefined Models
)
Training Fee = (10,000 / 1,000,000) √ó 0.5 √ó 3 = 0.015 0G
Storage Reserve Fee = 0.01 0G (for Qwen2.5-0.5B-Instruct)
Total Fee = 0.025 0G
tip
The actual fee is calculated during the setup phase after your dataset is analyzed. You can view the final fee using the
get-task
command before training begins.
Monitor Progress
‚Äã
You can monitor the progress of your task by running the following command:
0g-compute-cli fine-tuning get-task --provider <PROVIDER_ADDRESS> --task <TASK_ID>
The output will be like:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Field                             ‚îÇ Value                                                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ID                                ‚îÇ beb6f0d8-4660-4c62-988d-00246ce913d2                                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Created At                        ‚îÇ 2025-03-11T01:20:07.644Z                                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Pre-trained Model Hash            ‚îÇ 0xcb42b5ca9e998c82dd239ef2d20d22a4ae16b3dc0ce0a855c93b52c7c2bab6dc                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Dataset Hash                      ‚îÇ 0xaae9b4e031e06f84b20f10ec629f36c57719ea512992a6b7e2baea93f447a5fa                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Training Params                   ‚îÇ {......}                                                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Fee (neuron)                      ‚îÇ 82                                                                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Progress                          ‚îÇ Delivered                                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Field Descriptions:
ID
: Unique identifier for your fine-tuning task
Pre-trained Model Hash
: Hash identifier for the base model being fine-tuned
Dataset Hash
: Hash identifier for your training dataset (0G Storage root hash)
Training Params
: Configuration parameters used during fine-tuning
Fee (neuron)
: Total cost for the fine-tuning task (automatically calculated based on token count)
Progress
: Task status. Possible values are:
Init
: Task submitted
SettingUp
: Provider is preparing the environment (downloading dataset, etc.)
SetUp
: Provider is ready to start training
Training
: Provider is training the model
Trained
: Provider has finished training
Delivering
: Provider is encrypting and uploading the model to 0G Storage
Delivered
: Fine-tuning result is ready for download
UserAcknowledged
: User has downloaded and confirmed the result
Finished
: Provider has settled fees and shared decryption key ‚Äî task is completed
Failed
: Task failed
View Task Logs
‚Äã
You can view the logs of your task by running the following command:
0g-compute-cli fine-tuning get-log --provider <PROVIDER_ADDRESS> --task <TASK_ID>
The output will be like:
creating task....
Step: 0, Logs: {'loss': ..., 'accuracy': ...}
...
Training model for task beb6f0d8-4660-4c62-988d-00246ce913d2 completed successfully
Download and Acknowledge Model
‚Äã
Use the
Check Task
command to view task status. When the status changes to
Delivered
, the provider has completed fine-tuning and the encrypted model is ready. Download and acknowledge the model:
0g-compute-cli fine-tuning acknowledge-model \
--provider <PROVIDER_ADDRESS> \
--task-id <TASK_ID> \
--data-path <PATH_TO_SAVE_MODEL_FILE>
The CLI will automatically download the encrypted model from 0G Storage. If 0G Storage download fails, it will fall back to downloading directly from the provider's TEE.
48-Hour Deadline
You must download and acknowledge the model within 48 hours after the task status changes to
Delivered
.
If you fail to acknowledge within 48 hours:
The provider will
force settlement
automatically
You will
lose access to the fine-tuned model
30% of the total task fee
will be deducted as compensation for the provider's compute resources
Action required:
Monitor your task status and download promptly when it reaches
Delivered
.
File Path Required
--data-path
must be a file path
, not a directory.
Example:
0g-compute-cli fine-tuning acknowledge-model \
--provider <PROVIDER_ADDRESS> \
--task-id 0e91ef3d-ac0d-422e-a38c-9d42a28c4412 \
--data-path /workspace/output/encrypted_model.bin
Data Integrity Verification
The
acknowledge-model
command performs automatic data integrity verification to ensure the downloaded model matches the root hash that the provider submitted to the blockchain contract. This guarantees you receive the authentic model without corruption or tampering.
Note:
The model file downloaded with the above command is encrypted, and additional steps are required for decryption.
Decrypt Model
‚Äã
After acknowledging the model, the provider automatically settles the fees and uploads the decryption key to the contract (encrypted with your public key). Use the
get-task
command to check the task status.
When the status changes to
Finished
, you can decrypt the model:
0g-compute-cli fine-tuning decrypt-model \
--provider <PROVIDER_ADDRESS> \
--task-id <TASK_ID> \
--encrypted-model <PATH_TO_ENCRYPTED_MODEL_FILE> \
--output <PATH_TO_SAVE_DECRYPTED_MODEL>
Example:
# Use the same file path you specified in acknowledge-model
0g-compute-cli fine-tuning decrypt-model \
--provider <PROVIDER_ADDRESS> \
--task-id 0e91ef3d-ac0d-422e-a38c-9d42a28c4412 \
--encrypted-model /workspace/output/encrypted_model.bin \
--output /workspace/output/model_output.zip
The above command performs the following operations:
Gets the encrypted key from the contract uploaded by the provider
Decrypts the key using the user's private key
Decrypts the model with the decrypted key
Wait for Settlement
After
acknowledge-model
, the provider needs about
1 minute
to settle fees and upload the decryption key. If you decrypt too early (status is still
UserAcknowledged
instead of
Finished
), you may see an error like
second arg must be public key
. Simply wait and retry.
Note:
The decrypted result will be saved as a zip file. Ensure that the
<PATH_TO_SAVE_DECRYPTED_MODEL>
ends with .zip (e.g., model_output.zip). After downloading, unzip the file to access the decrypted model.
Extract LoRA Adapter
‚Äã
After decryption, unzip the model to access the LoRA adapter files:
unzip model_output.zip -d ./lora_adapter/
The extracted folder will contain:
lora_adapter/
‚îú‚îÄ‚îÄ output_model/
‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json       # LoRA configuration
‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors # LoRA weights
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json            # Tokenizer
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
Using the Fine-tuned Model
‚Äã
After fine-tuning, you receive a
LoRA adapter
(Low-Rank Adaptation), not a full model. To use it, you need to:
Download the base model
Load the LoRA adapter on top of the base model
Run inference
Step 1: Download Base Model
‚Äã
Download the same base model that was used for fine-tuning from HuggingFace:
# Install huggingface-cli if not already installed
pip install huggingface_hub
# For Qwen2.5-0.5B-Instruct
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir ./base_model
# For Qwen3-32B (requires ~65GB disk space)
# huggingface-cli download Qwen/Qwen3-32B --local-dir ./base_model
Step 2: Load LoRA with Base Model
‚Äã
Use the following Python code to combine the LoRA adapter with the base model.
For Qwen2.5-0.5B-Instruct:
from
transformers
import
AutoModelForCausalLM
,
AutoTokenizer
from
peft
import
PeftModel
import
torch
# Paths
base_model_path
=
"./base_model"
# or "Qwen/Qwen2.5-0.5B-Instruct"
lora_adapter_path
=
"./lora_adapter/output_model"
# Load tokenizer
tokenizer
=
AutoTokenizer
.
from_pretrained
(
lora_adapter_path
)
# Load base model
base_model
=
AutoModelForCausalLM
.
from_pretrained
(
base_model_path
,
torch_dtype
=
torch
.
bfloat16
,
device_map
=
"auto"
)
# Load LoRA adapter
model
=
PeftModel
.
from_pretrained
(
base_model
,
lora_adapter_path
)
print
(
"Model loaded successfully!"
)
For Qwen3-32B (requires 40GB+ VRAM):
from
transformers
import
AutoModelForCausalLM
,
AutoTokenizer
from
peft
import
PeftModel
import
torch
# Paths
base_model_path
=
"./base_model"
# or "Qwen/Qwen3-32B"
lora_adapter_path
=
"./lora_adapter/output_model"
# Load tokenizer
tokenizer
=
AutoTokenizer
.
from_pretrained
(
lora_adapter_path
)
# Load base model with optimizations for large models
base_model
=
AutoModelForCausalLM
.
from_pretrained
(
base_model_path
,
torch_dtype
=
torch
.
float16
,
# Use fp16 to reduce memory
device_map
=
"auto"
,
# Automatically distribute across GPUs
low_cpu_mem_usage
=
True
,
# Reduce CPU memory usage during loading
trust_remote_code
=
True
# Required for some Qwen models
)
# Load LoRA adapter
model
=
PeftModel
.
from_pretrained
(
base_model
,
lora_adapter_path
)
print
(
"Model loaded successfully!"
)
Memory Optimization for Large Models
If you encounter out-of-memory errors with Qwen3-32B, you can use quantization:
# 8-bit quantization (requires bitsandbytes)
from
transformers
import
BitsAndBytesConfig
quantization_config
=
BitsAndBytesConfig
(
load_in_8bit
=
True
)
base_model
=
AutoModelForCausalLM
.
from_pretrained
(
base_model_path
,
quantization_config
=
quantization_config
,
device_map
=
"auto"
,
trust_remote_code
=
True
)
Step 3: Run Inference
‚Äã
def
generate_response
(
prompt
,
max_new_tokens
=
100
)
:
messages
=
[
{
"role"
:
"user"
,
"content"
:
prompt
}
]
# Apply chat template
text
=
tokenizer
.
apply_chat_template
(
messages
,
tokenize
=
False
,
add_generation_prompt
=
True
)
# Tokenize
inputs
=
tokenizer
(
text
,
return_tensors
=
"pt"
)
.
to
(
model
.
device
)
# Generate
outputs
=
model
.
generate
(
**
inputs
,
max_new_tokens
=
max_new_tokens
,
do_sample
=
True
,
temperature
=
0.7
,
top_p
=
0.9
)
# Decode
response
=
tokenizer
.
decode
(
outputs
[
0
]
[
inputs
[
'input_ids'
]
.
shape
[
1
]
:
]
,
skip_special_tokens
=
True
)
return
response
# Example usage
response
=
generate_response
(
"Hello, how are you?"
)
print
(
response
)
Optional: Merge and Save Full Model
‚Äã
If you want to create a standalone model without needing to load the adapter separately:
from
transformers
import
AutoModelForCausalLM
,
AutoTokenizer
from
peft
import
PeftModel
import
torch
# Load base model and LoRA
base_model
=
AutoModelForCausalLM
.
from_pretrained
(
"Qwen/Qwen2.5-0.5B-Instruct"
,
torch_dtype
=
torch
.
bfloat16
,
device_map
=
"auto"
)
model
=
PeftModel
.
from_pretrained
(
base_model
,
"./lora_adapter/output_model"
)
# Merge LoRA weights into base model
merged_model
=
model
.
merge_and_unload
(
)
# Save the merged model
merged_model
.
save_pretrained
(
"./merged_model"
)
tokenizer
=
AutoTokenizer
.
from_pretrained
(
"./lora_adapter/output_model"
)
tokenizer
.
save_pretrained
(
"./merged_model"
)
print
(
"Merged model saved to ./merged_model"
)
Requirements
‚Äã
Install the required Python packages:
For GPU Environments (Recommended)
‚Äã
If you have an NVIDIA GPU, install PyTorch with CUDA support.
Important:
Match the CUDA version to your environment.
# For CUDA 12.1 (check your CUDA version with: nvidia-smi)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# For CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# Install other ML libraries
pip install transformers peft accelerate
For CPU-Only Environments
‚Äã
pip install torch transformers peft accelerate
Package Requirements
‚Äã
Package
Minimum Version
Purpose
torch
>= 2.0
Deep learning framework
transformers
>= 4.40.0
Model loading and inference
peft
>= 0.10.0
LoRA adapter support
accelerate
>= 0.27.0
Device management
Verify GPU Support
After installation, verify that PyTorch can detect your GPU:
python3 -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available())"
If
CUDA available: False
, you may need to reinstall PyTorch with the correct CUDA version.
Account Management
‚Äã
For comprehensive account management, including viewing balances, managing sub-accounts, and handling refunds, see
Account Management
.
Quick CLI commands:
# Check balance
0g-compute-cli get-account
# View sub-account for a provider
0g-compute-cli get-sub-account --provider <PROVIDER_ADDRESS>
# Request refund from sub-accounts
0g-compute-cli retrieve-fund
Other Commands
‚Äã
Upload Dataset Separately
‚Äã
You can upload a dataset to 0G Storage before creating a task:
0g-compute-cli fine-tuning upload --data-path <PATH_TO_DATASET>
Download Data
‚Äã
You can download previously uploaded datasets from 0G Storage:
0g-compute-cli fine-tuning download --data-path <PATH_TO_SAVE_DATASET> --data-root <DATASET_ROOT_HASH>
View Task List
‚Äã
You can view the list of tasks submitted to a specific provider using the following command:
0g-compute-cli fine-tuning list-tasks  --provider <PROVIDER_ADDRESS>
Cancel a Task
‚Äã
You can cancel a task before it starts running using the following command:
0g-compute-cli fine-tuning cancel-task --provider <PROVIDER_ADDRESS> --task <TASK_ID>
Note:
Tasks that are already in progress or completed cannot be canceled.
Troubleshooting
‚Äã
Error: MinimumDepositRequired
This means the provider's fine-tuning sub-account has insufficient funds. Make sure to include
--service fine-tuning
when transferring funds:
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 2 --service fine-tuning
Error: Provider busy
The provider is processing another task. Options:
Wait and retry later
Use a different provider:
0g-compute-cli fine-tuning list-providers
Queue your task (you'll be prompted)
Error: Insufficient balance
Add more funds:
0g-compute-cli deposit --amount 3
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 2 --service fine-tuning
Error: "second arg must be public key" when decrypting
This means the provider hasn't finished settlement yet. Wait about 1 minute after
acknowledge-model
, then check the task status:
0g-compute-cli fine-tuning get-task --provider <PROVIDER_ADDRESS> --task <TASK_ID>
When
Progress
shows
Finished
, retry the
decrypt-model
command.
Error: "Unexpected non-whitespace character after JSON" when creating task
Check your training configuration JSON file:
Ensure valid JSON format
Use decimal notation for numbers (e.g.,
0.0002
instead of
2e-4
)
Verify no trailing commas


========== [ https://docs.0g.ai/developer-hub/building-on-0g/compute-network/fine-tuning-provider ] ==========

Developer Hub
Building on 0G
0G Compute
For Providers
Fine-tuning Provider
On this page
Become a Fine-tuning Provider
This guide provides a comprehensive walkthrough for setting up and offering computing power as a fine-tuning provider on the 0G Compute Network.
Prerequisites
‚Äã
Docker and Docker Compose
TDX-enabled Intel CPU
Compatible NVIDIA GPU (H100/H200 with TEE support)
Wallet with 0G tokens for gas fees
Publicly accessible server
Preparation
‚Äã
Download the Installation Package
‚Äã
Visit the Releases Page:
0G Serving Broker Releases
Download and Extract:
Get the latest version of the fine-tuning installation package.
Configuration Setup
‚Äã
Copy the Config File:
Duplicate
config.example.yaml
to create
config.local.yaml
.
cp config.example.yaml config.local.yaml
Modify Settings:
Set
servingUrl
to your publicly accessible URL.
Set
privateKeys
using your wallet's private key for the 0G blockchain.
Edit
docker-compose.yml
:
Replace
#PORT#
with the desired port, matching the port in
config.local.yaml
.
# Replace #PORT# with your service port
sed -i 's/#PORT#/8080/g' docker-compose.yml
Supporting Custom Models from Providers
‚Äã
To include custom models, refer to the example configuration below and update your
config.local.yaml
file accordingly. Ensure that all required fields are properly defined to match your specific model setup.
service
:
customizedModels
:
-
name
:
"deepseek-r1-distill-qwen-1.5b"
hash
:
"<MODEL_ROOT_HASH>"
image
:
"deepseek:latest"
dataType
:
"text"
trainingScript
:
"/app/finetune.py"
description
:
"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning."
tokenizer
:
"<TOKENIZER_ROOT_HASH>"
usageFile
:
"<ZIP_FILE>"
-
name
:
"mobilenet_v2"
hash
:
"<MODEL_ROOT_HASH>"
image
:
"mobilenetV2:latest"
dataType
:
"image"
trainingScript
:
"/app/finetune.py"
description
:
"MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224."
tokenizer
:
"<TOKENIZER_ROOT_HASH>"
usageFile
:
"<ZIP_FILE>"
Configuration Fields:
name:
Model identifier
hash:
The root hash of the pre-trained model, obtained after uploading the model to 0G storage.
image:
The Docker image that encapsulates the fine-tuning execution environment.
dataType:
Specifies the type of dataset the model is intended to train on. Valid options include
text
or
image
.
trainingScript:
Specifies the path to the training script within the container. Fine-tuning will be executed using the command
python <trainingScript>
.
description:
A concise overview of the model, highlighting its key features and capabilities.
tokenizer:
The root hash of the tokenizer files used for dataset processing. This value is obtained after uploading the tokenizer files to 0G storage.
usageFile:
The ZIP file (referenced by its name, not the full path) contains detailed usage information for this model, including training configuration examples, build specifications, or sample datasets. Make sure the file is placed in the
./models
directory.
Build the TDX Guest Image
‚Äã
Prerequisites Installation
‚Äã
Install Docker:
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
Add User to Docker Group:
sudo usermod -aG docker $USER
newgrp docker
Verify Installation:
docker --version
docker run hello-world
Build CVM Image
‚Äã
To ensure secure and private execution of fine-tuning tasks, you will build an image suitable for running in a Confidential Virtual Machine (CVM). This process leverages NVIDIA's TEE GPU technology and Intel CPUs with TDX support, enhancing security by running model training in an isolated environment.
Clone Repository:
git clone https://github.com/nearai/private-ml-sdk --recursive
cd private-ml-sdk/
./build.sh
Image Files Location:
Check out
private-ml-sdk/images/
. Available images include:
dstack-nvidia-0.3.0
: Production image without developer tools.
dstack-nvidia-dev-0.3.0
: Development image with tools like
sshd
,
strace
.
Run Application
‚Äã
Run the Local KMS
‚Äã
The Local KMS provides essential keys for CVM initialization, derived from local TEE hardware.
Launch KMS:
cd private-ml-sdk/meta-dstack-nvidia/dstack/key-provider-build/
./run.sh
Run the TDX Guest Image
‚Äã
Ensure you have a TDX host machine with the TDX driver and a compatible NVIDIA GPU.
Update PATH:
pushd private-ml-sdk/meta-dstack-nvidia/scripts/bin
PATH=$PATH:`pwd`
popd
List Available GPUs:
dstack lsgpu
Create CVM Instance:
Replace
#PORT#
with your configured port:
dstack new docker-compose.yaml -o my-gpu-cvm \
--local-key-provider \
--gpu [GPU_ID] \
--image images/dstack-nvidia-0.3.0 \
-c 2 -m 4G -d 100G \
--port tcp:0.0.0.0:#PORT#:#PORT#
Run the CVM
‚Äã
Copy Config File:
cp config.local.yaml private-ml-sdk/my-gpu-cvm/shared/config.local.yaml
Start the CVM:
sudo -E dstack run my-gpu-cvm
Troubleshooting
‚Äã
CVM fails to start
Verify TDX is enabled in BIOS
Check GPU compatibility and drivers
Ensure sufficient resources allocated
Review logs:
sudo dstack logs my-gpu-cvm
Service not accessible
Confirm firewall allows incoming connections
Verify public IP/domain configuration
Check port consistency between config and Docker
Test local connectivity first
Model upload issues
Ensure model files are uploaded to 0G storage
Verify root hash is correctly configured
Check tokenizer files are included
Confirm Docker image exists and is accessible
By following these steps, you will successfully set up your service as a fine-tuning provider on the 0G Compute Network, leveraging secure and verifiable computing environments.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/compute-network/inference ] ==========

Developer Hub
Building on 0G
0G Compute
For Developers
Inference
On this page
0G Compute Inference
0G Compute Network provides decentralized AI inference services, supporting various AI models including Large Language Models (LLM), text-to-image generation, and speech-to-text processing.
Prerequisites
‚Äã
Node.js >= 22.0.0
A wallet with 0G tokens (either testnet or mainnet)
EVM compatible wallet (for Web UI)
Supported Service Types
‚Äã
Chatbot Services
: Conversational AI with models like GPT, DeepSeek, and others
Text-to-Image
: Generate images from text descriptions using Stable Diffusion and similar models
Speech-to-Text
: Transcribe audio to text using Whisper and other speech recognition models
Available Services
‚Äã
Testnet Services
View Testnet Services (2 Available)
#
Model
Type
Provider
Input (per 1M tokens)
Output (per 1M tokens)
1
qwen-2.5-7b-instruct
Chatbot
0xa48f01...
0.05 0G
0.10 0G
2
qwen-image-edit-2511
Image-Edit
0x4b2a9...
-
0.005 0G/image
Available Models by Type:
Chatbots (1 model):
Qwen 2.5 7B Instruct
: Fast and efficient conversational model
Image-Edit (1 model):
Qwen Image Edit 2511
: Advanced image editing and manipulation model
All testnet services feature TeeML verifiability and are ideal for development and testing.
Mainnet Services
View Mainnet Services (6 Available)
#
Model
Type
Provider
Input (per 1M tokens)
Output (per 1M tokens)
1
GLM-5-FP8
Chatbot
0xd9966e...
1 0G
3.2 0G
2
deepseek-chat-v3-0324
Chatbot
0x1B3AAe...
0.30 0G
1.00 0G
3
gpt-oss-120b
Chatbot
0xBB3f5b...
0.10 0G
0.49 0G
4
qwen3-vl-30b-a3b-instruct
Chatbot
0x4415ef...
0.49 0G
0.49 0G
5
whisper-large-v3
Speech-to-Text
0x36aCff...
0.05 0G
0.11 0G
6
z-image
Text-to-Image
0xE29a72...
-
0.003 0G/image
Available Models by Type:
Chatbots (4 models):
GLM-5-FP8
: High-performance reasoning model (FP8 quantized)
GPT-OSS-120B
: Large-scale open-source GPT model
Qwen3 VL 30B A3B Instruct
: Vision-language multimodal model
DeepSeek Chat V3
: Optimized conversational model
Speech-to-Text (1 model):
Whisper Large V3
: OpenAI's state-of-the-art transcription model
Text-to-Image (1 model):
Z-Image
: Fast high-quality image generation
All mainnet services feature TeeML verifiability for trusted execution in production environments.
Choose Your Interface
‚Äã
Feature
Web UI
CLI
SDK
Setup time
~1 min
~2 min
~5 min
Interactive chat
‚úÖ
‚ùå
‚ùå
Automation
‚ùå
‚úÖ
‚úÖ
App integration
‚ùå
‚ùå
‚úÖ
Direct API access
‚ùå
‚ùå
‚úÖ
Web UI
CLI
SDK
Best for:
Quick testing, experimentation and direct frontend integration.
Option 1: Use the Hosted Web UI
‚Äã
Visit the official 0G Compute Marketplace directly ‚Äî no installation required:
https://compute-marketplace.0g.ai/inference
Option 2: Run Locally
‚Äã
Installation
‚Äã
pnpm add @0glabs/0g-serving-broker -g
Launch Web UI
‚Äã
0g-compute-cli ui start-web
Open
http://localhost:3090
in your browser.
Getting Started
‚Äã
1. Connect & Fund
‚Äã
Connect your wallet
(MetaMask recommended)
Deposit some 0G tokens
using the account dashboard
Browse available AI models
and their pricing
2. Start Using AI Services
‚Äã
Option A: Chat Interface
Click "Chat" on any chatbot provider
Start conversations immediately
Perfect for testing and experimentation
Option B: Get API Integration
Click "Build" on any provider
Get step-by-step integration guides
Copy-paste ready code examples
Best for:
Automation, scripting, and server environments
Installation
‚Äã
pnpm add @0glabs/0g-serving-broker -g
Setup Environment
‚Äã
Choose Network
‚Äã
0g-compute-cli setup-network
Login with Wallet
‚Äã
Enter your wallet private key when prompted. This will be used for account management and service payments.
0g-compute-cli login
Create Account & Add Funds
‚Äã
Before using inference services, you need to fund your account. For detailed account management, see
Account Management
.
0g-compute-cli deposit --amount 10
0g-compute-cli get-account
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 5
CLI Commands
‚Äã
List Providers
‚Äã
0g-compute-cli inference list-providers
Verify Provider
‚Äã
Check provider's TEE attestation and reliability before using:
0g-compute-cli inference verify --provider <PROVIDER_ADDRESS>
This command outputs the provider's report and verifies their Trusted Execution Environment (TEE) status.
Acknowledge Provider
‚Äã
Before using a provider, acknowledge them on-chain:
0g-compute-cli inference acknowledge-provider --provider <PROVIDER_ADDRESS>
Direct API Access
‚Äã
Generate an authentication token for direct API calls:
0g-compute-cli inference get-secret --provider <PROVIDER_ADDRESS>
This generates a Bearer token in the format
app-sk-<SECRET>
that you can use for direct API calls.
API Usage Examples
‚Äã
Chatbot
Text-to-Image
Speech-to-Text
Use for conversational AI and text generation.
cURL
JavaScript
Python
curl <service_url>/v1/proxy/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer app-sk-<YOUR_SECRET>" \
-d '{
"model": <service.model>,
"messages": [
{
"role": "system",
"content": "You are a helpful assistant."
},
{
"role": "user",
"content": "Hello!"
}
]
}`
const
OpenAI
=
require
(
'openai'
)
;
const
client
=
new
OpenAI
(
{
baseURL
:
`
${
service
.
url
}
/v1/proxy
`
,
apiKey
:
'app-sk-<YOUR_SECRET>'
}
)
;
const
completion
=
await
client
.
chat
.
completions
.
create
(
{
model
:
service
.
model
,
messages
:
[
{
role
:
'system'
,
content
:
'You are a helpful assistant.'
}
,
{
role
:
'user'
,
content
:
'Hello!'
}
]
}
)
;
console
.
log
(
completion
.
choices
[
0
]
.
message
)
;
from
openai
import
OpenAI
client
=
OpenAI
(
base_url
=
`$
{
service
.
url
}
/
v1
/
proxy`
,
api_key
=
'app-sk-<YOUR_SECRET>'
)
completion
=
client
.
chat
.
completions
.
create
(
model
=
service
.
model
,
messages
=
[
{
'role'
:
'system'
,
'content'
:
'You are a helpful assistant.'
}
,
{
'role'
:
'user'
,
'content'
:
'Hello!'
}
]
)
print
(
completion
.
choices
[
0
]
.
message
)
Generate images from text descriptions.
cURL
JavaScript
Python
curl <service_url>/v1/proxy/images/generations \
-H "Content-Type: application/json" \
-H "Authorization: Bearer app-sk-<YOUR_SECRET>" \
-d '{
"model": <service.model>,
"prompt": "A cute baby sea otter playing in the water",
"n": 1,
"size": "1024x1024"
}'
const
OpenAI
=
require
(
'openai'
)
;
const
client
=
new
OpenAI
(
{
baseURL
:
`
${
service
.
url
}
/v1/proxy
`
,
apiKey
:
'app-sk-<YOUR_SECRET>'
}
)
;
const
response
=
await
client
.
images
.
generate
(
{
model
:
service
.
model
,
prompt
:
'A cute baby sea otter playing in the water'
,
n
:
1
,
size
:
'1024x1024'
}
)
;
console
.
log
(
response
.
data
)
;
from
openai
import
OpenAI
client
=
OpenAI
(
base_url
=
`$
{
service
.
url
}
/
v1
/
proxy`
,
api_key
=
'app-sk-<YOUR_SECRET>'
)
response
=
client
.
images
.
generate
(
model
=
service
.
model
,
prompt
=
'A cute baby sea otter playing in the water'
,
n
=
1
,
size
=
'1024x1024'
)
print
(
response
.
data
)
Transcribe audio files to text.
cURL
JavaScript
Python
curl <service_url>/v1/proxy/audio/transcriptions \
-H "Authorization: Bearer app-sk-<YOUR_SECRET>" \
-H "Content-Type: multipart/form-data" \
-F "file=@audio.ogg" \
-F "model=whisper-large-v3" \
-F "response_format=json"
const
OpenAI
=
require
(
'openai'
)
;
const
fs
=
require
(
'fs'
)
;
const
client
=
new
OpenAI
(
{
baseURL
:
`
${
service
.
url
}
/v1/proxy
`
,
apiKey
:
'app-sk-<YOUR_SECRET>'
}
)
;
const
transcription
=
await
client
.
audio
.
transcriptions
.
create
(
{
file
:
fs
.
createReadStream
(
'audio.ogg'
)
,
model
:
'whisper-large-v3'
,
response_format
:
'json'
}
)
;
console
.
log
(
transcription
.
text
)
;
from
openai
import
OpenAI
client
=
OpenAI
(
base_url
=
`$
{
service
.
url
}
/
v1
/
proxy`
,
api_key
=
'app-sk-<YOUR_SECRET>'
)
with
open
(
'audio.ogg'
,
'rb'
)
as
audio_file
:
transcription
=
client
.
audio
.
transcriptions
.
create
(
file
=
audio_file
,
model
=
'whisper-large-v3'
,
response_format
=
'json'
)
print
(
transcription
.
text
)
Start Local Proxy Server
‚Äã
Run a local OpenAI-compatible server:
# Start server on port 3000 (default)
0g-compute-cli inference serve --provider <PROVIDER_ADDRESS>
# Custom port
0g-compute-cli inference serve --provider <PROVIDER_ADDRESS> --port 8080
Then use any OpenAI-compatible client to connect to
http://localhost:3000
.
Best for:
Application integration and programmatic access
Installation
‚Äã
pnpm add @0glabs/0g-serving-broker
Starter Kits Available
Get up and running quickly with our comprehensive TypeScript starter kit within minutes.
TypeScript Starter Kit
- Complete examples with TypeScript and CLI tool
Initialize the Broker
‚Äã
Node.js
Browser
import
{
ethers
}
from
"ethers"
;
import
{
createZGComputeNetworkBroker
}
from
"@0glabs/0g-serving-broker"
;
// Choose your network
const
RPC_URL
=
process
.
env
.
NODE_ENV
===
'production'
?
"https://evmrpc.0g.ai"
// Mainnet
:
"https://evmrpc-testnet.0g.ai"
;
// Testnet
const
provider
=
new
ethers
.
JsonRpcProvider
(
RPC_URL
)
;
const
wallet
=
new
ethers
.
Wallet
(
process
.
env
.
PRIVATE_KEY
!
,
provider
)
;
const
broker
=
await
createZGComputeNetworkBroker
(
wallet
)
;
import
{
BrowserProvider
}
from
"ethers"
;
import
{
createZGComputeNetworkBroker
}
from
"@0glabs/0g-serving-broker"
;
// Check if MetaMask is installed
if
(
typeof
window
.
ethereum
===
"undefined"
)
{
throw
new
Error
(
"Please install MetaMask"
)
;
}
const
provider
=
new
BrowserProvider
(
window
.
ethereum
)
;
const
signer
=
await
provider
.
getSigner
(
)
;
const
broker
=
await
createZGComputeNetworkBroker
(
signer
)
;
Browser Compatibility
@0glabs/0g-serving-broker
requires polyfills for Node.js built-in modules.
Vite example:
pnpm add -D vite-plugin-node-polyfills
// vite.config.js
import
{
nodePolyfills
}
from
'vite-plugin-node-polyfills'
;
export
default
{
plugins
:
[
nodePolyfills
(
{
include
:
[
'crypto'
,
'stream'
,
'util'
,
'buffer'
,
'process'
]
,
globals
:
{
Buffer
:
true
,
global
:
true
,
process
:
true
}
}
)
]
}
;
Discover Services
‚Äã
// List all available services
const
services
=
await
broker
.
inference
.
listService
(
)
;
// Filter by service type
const
chatbotServices
=
services
.
filter
(
s
=>
s
.
serviceType
===
'chatbot'
)
;
const
imageServices
=
services
.
filter
(
s
=>
s
.
serviceType
===
'text-to-image'
)
;
const
speechServices
=
services
.
filter
(
s
=>
s
.
serviceType
===
'speech-to-text'
)
;
Account Management
‚Äã
For detailed account operations, see
Account Management
.
const
account
=
await
broker
.
ledger
.
getLedger
(
)
;
await
broker
.
ledger
.
depositFund
(
10
)
;
// Required before first use of a provider
await
broker
.
inference
.
acknowledgeProviderSigner
(
providerAddress
)
;
Make Inference Requests
‚Äã
Chatbot
Text-to-Image
Speech-to-Text
const
messages
=
[
{
role
:
"user"
,
content
:
"Hello!"
}
]
;
// Get service metadata
const
{
endpoint
,
model
}
=
await
broker
.
inference
.
getServiceMetadata
(
providerAddress
)
;
// Generate auth headers
const
headers
=
await
broker
.
inference
.
getRequestHeaders
(
providerAddress
)
;
// Make request
const
response
=
await
fetch
(
`
${
endpoint
}
/chat/completions
`
,
{
method
:
"POST"
,
headers
:
{
"Content-Type"
:
"application/json"
,
...
headers
}
,
body
:
JSON
.
stringify
(
{
messages
,
model
}
)
}
)
;
const
data
=
await
response
.
json
(
)
;
const
answer
=
data
.
choices
[
0
]
.
message
.
content
;
const
prompt
=
"A cute baby sea otter"
;
const
body
=
JSON
.
stringify
(
{
model
,
prompt
,
n
:
1
,
size
:
"1024x1024"
}
)
;
// Get service metadata
const
{
endpoint
,
model
}
=
await
broker
.
inference
.
getServiceMetadata
(
providerAddress
)
;
// Generate auth headers
const
headers
=
await
broker
.
inference
.
getRequestHeaders
(
providerAddress
,
body
)
;
// Make request
const
response
=
await
fetch
(
`
${
endpoint
}
/images/generations
`
,
{
method
:
"POST"
,
headers
:
{
"Content-Type"
:
"application/json"
,
...
headers
}
,
body
:
JSON
.
stringify
(
{
model
,
prompt
,
n
:
1
,
size
:
"1024x1024"
}
)
}
)
;
const
data
=
await
response
.
json
(
)
;
const
imageUrl
=
data
.
data
[
0
]
.
url
;
const
formData
=
new
FormData
(
)
;
formData
.
append
(
'file'
,
audioFile
)
;
// audioFile is a File or Blob
formData
.
append
(
'model'
,
model
)
;
formData
.
append
(
'response_format'
,
'json'
)
;
// Get service metadata
const
{
endpoint
,
model
}
=
await
broker
.
inference
.
getServiceMetadata
(
providerAddress
)
;
// Generate auth headers
const
headers
=
await
broker
.
inference
.
getRequestHeaders
(
providerAddress
)
;
// Make request
const
response
=
await
fetch
(
`
${
endpoint
}
/audio/transcriptions
`
,
{
method
:
"POST"
,
headers
:
{
...
headers
}
,
body
:
formData
}
)
;
const
data
=
await
response
.
json
(
)
;
const
transcription
=
data
.
text
;
Response Verification
‚Äã
The
processResponse
method handles response verification and automatic fee management. Both parameters are optional:
receivedContent
: The usage data from the service response. When provided, the SDK caches accumulated usage and automatically transfers funds from your main account to the provider's sub-account to prevent service interruptions.
chatID
: Response identifier for verifiable TEE services. Different service types handle this differently.
Chatbot
Text-to-Image
Speech-to-Text
Streaming Responses
For chatbot services, pass the usage data from the response to enable automatic fee management:
// Standard chat completion
const
response
=
await
fetch
(
`
${
endpoint
}
/chat/completions
`
,
{
method
:
"POST"
,
headers
:
{
"Content-Type"
:
"application/json"
,
...
headers
}
,
body
:
JSON
.
stringify
(
{
messages
,
model
}
)
}
)
;
const
data
=
await
response
.
json
(
)
;
// Process response for automatic fee management
if
(
data
.
usage
)
{
await
broker
.
inference
.
processResponse
(
providerAddress
,
undefined
,
// chatID is undefined for non-verifiable responses
JSON
.
stringify
(
data
.
usage
)
// Pass usage data for fee calculation
)
;
}
// For verifiable TEE services with chatID
// Check response headers first
let
chatID
=
response
.
headers
.
get
(
"ZG-Res-Key"
)
||
response
.
headers
.
get
(
"zg-res-key"
)
;
// If not found in response headers, check response body
if
(
!
chatID
)
{
chatID
=
data
.
id
||
data
.
chatID
;
}
if
(
chatID
)
{
const
isValid
=
await
broker
.
inference
.
processResponse
(
providerAddress
,
chatID
,
// Verify the response integrity
JSON
.
stringify
(
data
.
usage
)
// Also manage fees
)
;
}
For text-to-image services, pass the original request data for input-based fee calculation:
const
requestBody
=
{
model
,
prompt
:
"A cute baby sea otter"
,
size
:
"1024x1024"
,
n
:
1
}
;
const
response
=
await
fetch
(
`
${
endpoint
}
/images/generations
`
,
{
method
:
"POST"
,
headers
:
{
"Content-Type"
:
"application/json"
,
...
headers
}
,
body
:
JSON
.
stringify
(
requestBody
)
}
)
;
const
data
=
await
response
.
json
(
)
;
// Get chatID from response headers for verification
const
chatID
=
response
.
headers
.
get
(
"ZG-Res-Key"
)
||
response
.
headers
.
get
(
"zg-res-key"
)
;
if
(
chatID
)
{
// Process response with chatID for verification and fee calculation
const
isValid
=
await
broker
.
inference
.
processResponse
(
providerAddress
,
chatID
// Verify the response integrity
)
;
console
.
log
(
"Response valid:"
,
isValid
)
;
}
else
{
// Fallback: process without verification if no chatID
await
broker
.
inference
.
processResponse
(
providerAddress
,
undefined
// No chatID available
)
;
}
For speech-to-text services, pass the usage data if available:
const
formData
=
new
FormData
(
)
;
formData
.
append
(
'file'
,
audioFile
)
;
formData
.
append
(
'model'
,
model
)
;
const
response
=
await
fetch
(
`
${
endpoint
}
/audio/transcriptions
`
,
{
method
:
"POST"
,
headers
:
{
...
headers
}
,
body
:
formData
}
)
;
const
data
=
await
response
.
json
(
)
;
// Get chatID from response headers for verification
const
chatID
=
response
.
headers
.
get
(
"ZG-Res-Key"
)
||
response
.
headers
.
get
(
"zg-res-key"
)
;
if
(
chatID
)
{
// Process response with chatID for verification and fee calculation
const
isValid
=
await
broker
.
inference
.
processResponse
(
providerAddress
,
chatID
,
// Verify the response integrity
JSON
.
stringify
(
data
.
usage
||
{
}
)
// Pass usage for fee calculation
)
;
console
.
log
(
"Response valid:"
,
isValid
)
;
}
else
if
(
data
.
usage
)
{
// Fallback: process without verification if no chatID but usage available
await
broker
.
inference
.
processResponse
(
providerAddress
,
undefined
,
// No chatID available
JSON
.
stringify
(
data
.
usage
)
// Pass usage for fee calculation
)
;
}
For streaming responses, handle chatID differently based on service type:
Chatbot Streaming
Speech-to-Text Streaming
// For chatbot streaming, first check headers then try to get ID from stream
let
chatID
=
response
.
headers
.
get
(
"ZG-Res-Key"
)
||
response
.
headers
.
get
(
"zg-res-key"
)
;
let
usage
=
null
;
let
streamChatID
=
null
;
// Will try to get from stream data
const
decoder
=
new
TextDecoder
(
)
;
const
reader
=
response
.
body
.
getReader
(
)
;
// Process stream
let
rawBody
=
''
;
while
(
true
)
{
const
{
done
,
value
}
=
await
reader
.
read
(
)
;
if
(
done
)
break
;
rawBody
+=
decoder
.
decode
(
value
,
{
stream
:
true
}
)
;
}
// Parse usage and chatID from stream data
for
(
const
line
of
rawBody
.
split
(
'\n'
)
)
{
const
trimmed
=
line
.
trim
(
)
;
if
(
!
trimmed
||
trimmed
===
'data: [DONE]'
)
continue
;
try
{
const
jsonStr
=
trimmed
.
startsWith
(
'data:'
)
?
trimmed
.
slice
(
5
)
.
trim
(
)
:
trimmed
;
const
message
=
JSON
.
parse
(
jsonStr
)
;
// For chatbot, try to get ID from stream data
if
(
!
streamChatID
&&
(
message
.
id
||
message
.
chatID
)
)
{
streamChatID
=
message
.
id
||
message
.
chatID
;
}
if
(
message
.
usage
)
{
usage
=
message
.
usage
;
}
}
catch
{
}
}
// Use chatID from header if available, otherwise use chatID from stream data
const
finalChatID
=
chatID
||
streamChatID
;
// Process with chatID for verification if available
if
(
finalChatID
)
{
const
isValid
=
await
broker
.
inference
.
processResponse
(
providerAddress
,
finalChatID
,
JSON
.
stringify
(
usage
||
{
}
)
)
;
console
.
log
(
"Chatbot streaming response valid:"
,
isValid
)
;
}
else
if
(
usage
)
{
// Fallback: process without verification
await
broker
.
inference
.
processResponse
(
providerAddress
,
undefined
,
JSON
.
stringify
(
usage
)
)
;
}
// For speech-to-text streaming, get chatID from headers
const
chatID
=
response
.
headers
.
get
(
"ZG-Res-Key"
)
||
response
.
headers
.
get
(
"zg-res-key"
)
;
let
usage
=
null
;
const
decoder
=
new
TextDecoder
(
)
;
const
reader
=
response
.
body
.
getReader
(
)
;
// Process stream
let
rawBody
=
''
;
while
(
true
)
{
const
{
done
,
value
}
=
await
reader
.
read
(
)
;
if
(
done
)
break
;
rawBody
+=
decoder
.
decode
(
value
,
{
stream
:
true
}
)
;
}
// Parse usage from stream data
for
(
const
line
of
rawBody
.
split
(
'\n'
)
)
{
const
trimmed
=
line
.
trim
(
)
;
if
(
!
trimmed
||
trimmed
===
'data: [DONE]'
)
continue
;
try
{
const
jsonStr
=
trimmed
.
startsWith
(
'data:'
)
?
trimmed
.
slice
(
5
)
.
trim
(
)
:
trimmed
;
const
message
=
JSON
.
parse
(
jsonStr
)
;
if
(
message
.
usage
)
{
usage
=
message
.
usage
;
}
}
catch
{
}
}
// Process with chatID for verification if available
if
(
chatID
)
{
const
isValid
=
await
broker
.
inference
.
processResponse
(
providerAddress
,
chatID
,
JSON
.
stringify
(
usage
||
{
}
)
)
;
console
.
log
(
"Audio streaming response valid:"
,
isValid
)
;
}
else
if
(
usage
)
{
// Fallback: process without verification
await
broker
.
inference
.
processResponse
(
providerAddress
,
undefined
,
JSON
.
stringify
(
usage
)
)
;
}
Key Points:
Always call
processResponse
after receiving responses to maintain proper fee management
The SDK automatically handles fund transfers to prevent service interruptions
For verifiable TEE services, the method also validates response integrity
chatID retrieval principle
: Always prioritize
ZG-Res-Key
from response headers. Only use fallback methods when header is not present.
chatID retrieval varies by service type:
Chatbot
: First try
ZG-Res-Key
header, then check
data.id
(completion ID from response body) as fallback
Text-to-Image & Speech-to-Text
: Always get chatID from
ZG-Res-Key
response header
Streaming responses
:
Chatbot streaming
: Check headers first, then try to get
id
from stream data as fallback
Speech-to-text streaming
: Get chatID from
ZG-Res-Key
header immediately
Usage data format varies by service type but typically includes token counts or request metrics
Troubleshooting
‚Äã
Common Issues
‚Äã
Error: Insufficient balance
Your account doesn't have enough funds. Add more using CLI or SDK:
CLI:
Deposit to Main Account
‚Äã
0g-compute-cli deposit --amount 5
Transfer to Provider Sub-Account
‚Äã
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 5
SDK:
await
broker
.
ledger
.
depositFund
(
1
)
;
Error: Provider not acknowledged
You need to acknowledge the provider before using their service:
CLI:
0g-compute-cli inference acknowledge-provider --provider <PROVIDER_ADDRESS>
SDK:
await
broker
.
inference
.
acknowledgeProviderSigner
(
providerAddress
)
;
Error: No funds in provider sub-account
Transfer funds to the specific provider sub-account:
0g-compute-cli transfer-fund --provider <PROVIDER_ADDRESS> --amount 5
Check your account balance:
0g-compute-cli get-account
Web UI not starting
If the web UI fails to start:
Check if another service is using port 3090:
0g-compute-cli ui start-web --port 3091
Ensure the package was installed globally:
pnpm add @0glabs/0g-serving-broker -g
Next Steps
‚Äã
Manage Accounts
‚Üí
Account Management Guide
Fine-tune Models
‚Üí
Fine-tuning Guide
Become a Provider
‚Üí
Provider Setup
View Examples
‚Üí
GitHub
Questions? Join our
Discord
for support.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/compute-network/inference-provider ] ==========

Developer Hub
Building on 0G
0G Compute
For Providers
Inference Provider
On this page
Become an Inference Provider
Transform your AI services into verifiable, revenue-generating endpoints on the 0G Compute Network. This guide covers setting up your service and connecting it through the provider broker.
Why Become a Provider?
‚Äã
Monetize Your Infrastructure
: Turn idle GPU resources into revenue
Automated Settlements
: The broker handles billing and payments automatically
Trust Through Verification
: Offer verifiable services for premium rates
Prerequisites
‚Äã
Docker Compose 1.27+
OpenAI-compatible model service
Wallet with 0G tokens for gas fees
Setup Process
‚Äã
Prepare Your Model Service
‚Äã
Service Interface Requirements
‚Äã
Your AI service must implement the
OpenAI API Interface
for compatibility. This ensures consistent user experience across all providers.
Verification Interfaces
‚Äã
To ensure the integrity and trustworthiness of services, different verification mechanisms are employed. Each mechanism comes with its own specific set of protocols and requirements to ensure service verification and security.
TEE Verification (TeeML)
OPML, ZKML (Coming Soon)
TEE (Trusted Execution Environment) verification ensures your computations are tamper-proof. Services running in TEE:
Generate signing keys within the secure environment
Provide CPU and GPU attestations
Sign all inference results
These attestations should include the public key of the signing key, verifying its creation within the TEE. All inference results must be signed with this signing key.
Hardware Requirements
‚Äã
CPU
: Intel TDX (Trusted Domain Extensions) enabled
GPU
: NVIDIA H100 or H200 with TEE support
TEE Node Setup
‚Äã
There are two ways to start a TEE node for your inference service:
Method 1: Using Dstack
‚Äã
Follow the
Dstack Getting Started Guide
to prepare your TEE node using Dstack.
Method 2: Using Cryptopilot
‚Äã
Follow the
0G-TAPP README
to set up your TEE node using Cryptopilot.
Download and Configure Inference Broker
‚Äã
To register and manage TEE services, handle user request proxies, and perform settlements, you need to use the Inference Broker.
Please visit the
releases page
to download and extract the latest version of the installation package. After extracting, use the executable
config
file to generate the configuration file and docker-compose.yml file according to your setup.
# Download from releases page
tar -xzf inference-broker.tar.gz
cd inference-broker
# Generate configuration files
./config
Support for additional verification methods including:
OPML
: Optimistic Machine Learning proofs
ZKML
: Zero-knowledge ML verification
Stay tuned for updates.
Launch Provider Broker
‚Äã
Follow the instructions in
Dstack
or
0G-TAPP
documentation to start the service using the config file and docker-compose.yml file generated in the previous step.
The broker will:
Register your service on the network
Handle user authentication and request routing
Manage automatic settlement of payments
Troubleshooting
‚Äã
Broker fails to start
Verify Docker Compose is installed correctly
Check port availability
Ensure config.local.yaml syntax is valid
Review logs:
docker compose logs
Service not accessible
Confirm firewall allows incoming connections
Verify public IP/domain is correct
Test local service:
curl http://localhost:8000/chat/completions
Settlement issues
The automatic settlement engine handles payments. If issues occur:
Check wallet has sufficient gas
Verify network connectivity
Monitor settlement logs in broker output
Next Steps
‚Äã
Join Community
‚Üí
Discord
for support
Explore Inference
‚Üí
Inference Documentation
for integration details


========== [ https://docs.0g.ai/developer-hub/building-on-0g/compute-network/marketplace ] ==========

Marketplace (coming soon)
Coming soon


========== [ https://docs.0g.ai/developer-hub/building-on-0g/compute-network/overview ] ==========

Developer Hub
Building on 0G
0G Compute
Overview
On this page
0G Compute Network
Access affordable GPU computing power for AI workloads through a decentralized marketplace.
AI Computing Costs Are Crushing Innovation
‚Äã
Running AI models today means choosing between:
Cloud Providers
: $5,000-50,000/month for dedicated GPUs
API Services
: $0.03+ per request, adding up to thousands monthly
Building Infrastructure
: Millions in hardware investment
Result
: Only well-funded companies can afford AI at scale.
Decentralized GPU Marketplace
‚Äã
0G Compute Network connects idle GPU owners with AI developers, creating a marketplace that's:
90% Cheaper
: Pay only for compute used, no monthly minimums
Instantly Available
: Access 1000s of GPUs globally
Verifiable
: Cryptographic proofs ensure computation integrity
Think of it as "Uber for GPUs" - matching supply with demand efficiently.
Architecture Overview
‚Äã
The network consists of:
Smart Contracts
: Handle payments and verification
Provider Network
: GPU owners running compute services
Client SDKs
: Easy integration for developers
Verification Layer
: Ensures computation integrity
Key Components
‚Äã
ü§ñ Supported Services
‚Äã
Service Type
What It Does
Status
Inference
Run pre-trained models (LLMs)
‚úÖ Live
Fine-tuning
Fine-tune models with your data
‚úÖ Live
Training
Train models from scratch
üîú Coming
üîê Trust & Verification
‚Äã
Verifiable Computation
: Proof that work was done correctly
TEE (Trusted Execution Environment) for secure processing
Cryptographic signatures on all results
Can't fake or manipulate outputs
What makes it trustworthy?
Smart Contract Escrow
: Funds held until service delivered
Like eBay's payment protection
Automatic settlement on completion
No payment disputes
Quick Start Paths
‚Äã
üë®‚Äçüíª "I want to use AI services"
‚Äã
Build AI-powered applications without infrastructure:
Install SDK
- 5 minute setup
Fund your account
- Pre-pay for usage
Send requests
-
OpenAI SDK compatible
üñ•Ô∏è "I have GPUs to monetize"
‚Äã
Turn idle hardware into revenue:
Check
hardware requirements
Set up provider software
üéØ "I need to fine-tune AI models"
‚Äã
Fine-tune models with your data:
Install CLI tools
Prepare your dataset
Start fine-tuning
Frequently Asked Questions
‚Äã
How much can I save compared to OpenAI?
Typically 90%+ savings:
OpenAI GPT-4: ~$0.03 per 1K tokens
0G Compute: ~$0.003 per 1K tokens
Bulk usage: Even greater discounts
Is my data secure?
Yes, through multiple layers:
TEE (Trusted Execution Environment) processing
No data retention by providers
Verifiable computation proofs
How fast is it compared to centralized services?
Inference: 50-100ms latency (comparable to centralized)
Geographic distribution reduces latency
0G Compute Network: Democratizing AI computing for everyone.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/deploy-contracts ] ==========

Developer Hub
Building on 0G
0G Chain
On this page
Deploy Smart Contracts on 0G Chain
Deploy smart contracts on 0G Chain - an EVM-compatible blockchain with built-in AI capabilities.
Why Deploy on 0G Chain?
‚Äã
‚ö° Performance Benefits
‚Äã
11,000 TPS per Shard
: Higher throughput than Ethereum
Low Fees
: Fraction of mainnet costs
Sub-second Finality
: Near-instant transaction confirmation
üîß Latest EVM Compatibility
‚Äã
Pectra & Cancun-Deneb Support
: Leverage newest Ethereum capabilities
Future-Ready
: Architecture designed for quick integration of upcoming EVM upgrades
Familiar Tools
: Use Hardhat, Foundry, Remix
No Learning Curve
: Deploy like any EVM chain
Prerequisites
‚Äã
Before deploying contracts on 0G Chain, ensure you have:
Node.js 16+ installed (for Hardhat/Truffle)
Rust installed (for Foundry)
A wallet with testnet 0G tokens (
get from faucet
)
Basic Solidity knowledge
Steps to Deploy Your Contract
‚Äã
Step 1: Prepare Your Smart Contract Code
‚Äã
Write your contract code as you would for any Ethereum-compatible blockchain, ensuring that it meets the requirements for your specific use case.
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.19;
contract MyToken {
mapping(address => uint256) public balances;
uint256 public totalSupply;
constructor(uint256 _initialSupply) {
totalSupply = _initialSupply;
balances[msg.sender] = _initialSupply;
}
function transfer(address to, uint256 amount) public returns (bool) {
require(balances[msg.sender] >= amount, "Insufficient balance");
balances[msg.sender] -= amount;
balances[to] += amount;
return true;
}
}
Step 2: Compile Your Smart Contract
‚Äã
Use
solc
or another compatible Solidity compiler to compile your smart contract.
Important
: When compiling, specify
--evm-version cancun
to ensure compatibility with the latest EVM upgrades supported by 0G Chain.
Using solc directly
:
solc --evm-version cancun --bin --abi MyToken.sol
Using Hardhat
:
// hardhat.config.js
module
.
exports
=
{
solidity
:
{
version
:
"0.8.19"
,
settings
:
{
evmVersion
:
"cancun"
,
optimizer
:
{
enabled
:
true
,
runs
:
200
,
}
,
}
,
}
,
}
;
Using Foundry
:
# foundry.toml
[profile.default]
evm_version = "cancun"
This step will generate the binary and ABI (Application Binary Interface) for your contract.
Step 3: Deploy the Contract on 0G Chain
‚Äã
Once compiled, you can use your preferred Ethereum-compatible deployment tools, such as
web3.js
,
ethers.js
, or
hardhat
, to deploy the contract on 0G Chain.
Configure Network Connection
:
// For Hardhat
networks
:
{
"testnet"
:
{
url
:
"https://evmrpc-testnet.0g.ai"
,
chainId
:
16602
,
accounts
:
[
process
.
env
.
PRIVATE_KEY
]
}
,
"mainnet"
:
{
url
:
"https://evmrpc.0g.ai"
,
chainId
:
16661
,
accounts
:
[
process
.
env
.
PRIVATE_KEY
]
}
}
// For Foundry
[
rpc_endpoints
]
0g_testnet
=
"https://evmrpc-testnet.0g.ai"
0g_mainnet
=
"https://evmrpc.0g.ai"
Deploy Using Your Preferred Tool
:
Hardhat Deployment
// scripts/deploy.js
async
function
main
(
)
{
const
MyToken
=
await
ethers
.
getContractFactory
(
"MyToken"
)
;
const
token
=
await
MyToken
.
deploy
(
1000000
)
;
// 1M initial supply
await
token
.
deployed
(
)
;
console
.
log
(
"Token deployed to:"
,
token
.
address
)
;
}
main
(
)
.
catch
(
(
error
)
=>
{
console
.
error
(
error
)
;
process
.
exitCode
=
1
;
}
)
;
Run:
npx hardhat run scripts/deploy.js --network 0g-testnet
Foundry Deployment
forge create --rpc-url https://evmrpc-testnet.0g.ai \
--private-key $PRIVATE_KEY \
--evm-version cancun \
src/MyToken.sol:MyToken \
--constructor-args 1000000
Truffle Deployment
// migrations/2_deploy_token.js
module
.
exports
=
function
(
deployer
)
{
deployer
.
deploy
(
MyToken
,
1000000
)
;
}
;
Run:
truffle migrate --network 0g-testnet
Follow the same deployment steps as you would on Ethereum, using your 0G Chain node or RPC endpoint.
For complete working examples using different frameworks, check out the official deployment scripts repository: üîó
0G Deployment Scripts
Step 4: Verify Deployment Results on 0G Chain Scan
‚Äã
After deployment, you can verify your contract on 0G Chain Scan, the block explorer for
0G Chain
or via the provided API below:
Hardhat
Forge
Make sure you have the following plugins installed:
npm install --save-dev @nomicfoundation/hardhat-verify @nomicfoundation/viem @nomicfoundation/hardhat-toolbox-viem dotenv
To verify your contract using Hardhat, please use the following settings in your
hardhat.config.js
:
solidity
:
{
...
settings
:
{
evmVersion
:
"cancun"
,
// Make sure this matches your compiler setting
optimizer
:
{
enabled
:
true
,
runs
:
200
,
// Adjust based on your optimization needs
}
,
viaIR
:
true
,
// Enable if your contract uses inline assembly
metadata
:
{
bytecodeHash
:
"none"
,
// Optional: Set to "none" to exclude metadata hash
}
,
}
,
}
Add the network configuration:
networks
:
{
"testnet"
:
{
url
:
"https://evmrpc-testnet.0g.ai"
,
chainId
:
16602
,
accounts
:
[
process
.
env
.
PRIVATE_KEY
]
}
,
"mainnet"
:
{
url
:
"https://evmrpc.0g.ai"
,
chainId
:
16661
,
accounts
:
[
process
.
env
.
PRIVATE_KEY
]
}
}
and finally, add the etherscan configuration:
etherscan
:
{
apiKey
:
{
testnet
:
"YOUR_API_KEY"
,
// Use a placeholder if you don't have one
mainnet
:
"YOUR_API_KEY"
// Use a placeholder if you don't have one
}
,
customChains
:
[
{
// Testnet
network
:
"testnet"
,
chainId
:
16602
,
urls
:
{
apiURL
:
"https://chainscan-galileo.0g.ai/open/api"
,
browserURL
:
"https://chainscan-galileo.0g.ai"
,
}
,
}
,
{
// Mainnet
network
:
"mainnet"
,
chainId
:
16661
,
urls
:
{
apiURL
:
"https://chainscan.0g.ai/open/api"
,
browserURL
:
"https://chainscan.0g.ai"
,
}
,
}
,
]
,
}
,
To verify your contract, run the following command:
npx hardhat verify DEPLOYED_CONTRACT_ADDRESS --network <Network>
You should get a success message like this:
Successfully submitted source code for contract
contracts/Contract.sol:ContractName at DEPLOYED_CONTRACT_ADDRESS
for verification on the block explorer. Waiting for verification result...
Successfully verified contract TokenDist on the block explorer.
https://chainscan.0g.ai/address/<DEPLOYED_CONTRACT_ADDRESS>#code
On Foundry, you can verify your contract using the
forge verify-contract
command. Make sure to set your compiler settings in
foundry.toml
as needed.
Precompile
Verifier URL
Testnet
https://chainscan-galileo.0g.ai/open/api
Mainnet
https://chainscan.0g.ai/open/api
forge verify-contract \
--chain-id <CHAIN_ID> \
--num-of-optimizations <NUM_OPTIMIZATIONS> \
--verifier custom \
--verifier-api-key "PLACEHOLDER" \
--compiler-version <COMPILER_VERSION> \
<CONTRACT_ADDRESS> \
src/Counter.sol:Counter \
--verifier-url <VERIFIER_URL> \
You should get a success message like this:
Submitted contract for verification:
Response: OK
GUID: <GUID>
URL: https://chainscan-galileo.0g.ai/open/address/<CONTRACT_ADDRESS>
Using 0G Precompiles
‚Äã
Available Precompiles
‚Äã
Precompile
Address
Purpose
DASigners
0x...1000
Data availability signatures
Wrapped0GBase
0x...1002
Wrapped 0G token operations
Troubleshooting
‚Äã
Transaction failing with "invalid opcode"?
If you're using newer experimental opcodes from unreleased Ethereum upgrades and see "invalid opcode" errors, consider:
Use
--evm-version cancun
in your compiler settings
Downgrade to an earlier Solidity compiler version (e.g., from 0.8.26 to 0.8.19)
Can't connect to RPC?
Try alternative endpoints:
QuikNode:
Get endpoint
ThirdWeb:
Get endpoint
What's Next?
‚Äã
Learn Precompiles
:
Precompiles Overview
Storage Integration
:
0G Storage SDK
Compute Integration
:
0G Compute Guide
Need help? Join our
Discord
for developer support.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/precompiles/precompiles-dasigners ] ==========

Developer Hub
Building on 0G
0G Chain
Precompiles
DASigners
On this page
Overview
DAsigners is a wrapper for the
x/dasigners
module in the 0g chain, allowing querying the state of this module from EVM calls.
Address
0x0000000000000000000000000000000000001000
Interface
https://github.com/0gfoundation/0g-chain/blob/dev/precompiles/interfaces/contracts/IDASigners.sol
Structs
‚Äã
SignerDetail
‚Äã
struct SignerDetail {
address signer;
string socket;
BN254.G1Point pkG1;
BN254.G2Point pkG2;
}
Description
: Contains details of a signer, including the address, socket, and bn254 public keys (G1 and G2 points).
Fields
:
signer
: The address of the signer.
socket
: The socket associated with the signer.
pkG1
: The G1 public key of the signer.
pkG2
: The G2 public key of the signer.
Params
‚Äã
struct Params {
uint tokensPerVote;
uint maxVotesPerSigner;
uint maxQuorums;
uint epochBlocks;
uint encodedSlices;
}
Description
: Defines parameters for the DAsigners module.
Fields
:
tokensPerVote
: The number of tokens required for one vote.
maxVotesPerSigner
: The maximum number of votes a signer can cast.
maxQuorums
: The maximum number of quorums allowed.
epochBlocks
: The number of blocks in an epoch.
encodedSlices
: The number of encoded slices in one DA blob.
Functions
‚Äã
params()
‚Äã
function params() external view returns (Params memory);
Description
: Retrieves the current parameters of the DAsigners module.
Returns
:
Params
structure containing the current module parameters.
epochNumber()
‚Äã
function epochNumber() external view returns (uint);
Description
: Returns the current epoch number.
Returns
:
uint
representing the current epoch number.
quorumCount(uint _epoch)
‚Äã
function quorumCount(uint _epoch) external view returns (uint);
Description
: Returns the number of quorums for a given epoch.
Parameters
:
_epoch
: The epoch number.
Returns
:
uint
representing the quorum count for the given epoch.
isSigner(address _account)
‚Äã
function isSigner(address _account) external view returns (bool);
Description
: Checks if a given account is a registered signer.
Parameters
:
_account
: The address to check.
Returns
:
bool
indicating whether the account is a signer.
getSigner(address[] memory _account)
‚Äã
function getSigner(
address[] memory _account
) external view returns (SignerDetail[] memory);
Description
: Retrieves details for the signers of the provided addresses.
Parameters
:
_account
: An array of addresses to fetch the signer details for.
Returns
: An array of
SignerDetail
structures for each signer.
getQuorum(uint _epoch, uint _quorumId)
‚Äã
function getQuorum(
uint _epoch,
uint _quorumId
) external view returns (address[] memory);
Description
: Returns the addresses of the members in a specific quorum for a given epoch.
Parameters
:
_epoch
: The epoch number.
_quorumId
: The ID of the quorum.
Returns
: An array of addresses that are members of the quorum.
getQuorumRow(uint _epoch, uint _quorumId, uint32 _rowIndex)
‚Äã
function getQuorumRow(
uint _epoch,
uint _quorumId,
uint32 _rowIndex
) external view returns (address);
Description
: Retrieves a specific address from a quorum's row for a given epoch and quorum ID.
Parameters
:
_epoch
: The epoch number.
_quorumId
: The quorum ID.
_rowIndex
: The row index within the quorum.
Returns
: The address at the specified row index in the quorum.
registerSigner(SignerDetail memory _signer, BN254.G1Point memory _signature)
‚Äã
function registerSigner(
SignerDetail memory _signer,
BN254.G1Point memory _signature
) external;
Description
: Registers a new signer with the provided details and signature.
Parameters
:
_signer
: The details of the signer to register.
_signature
: The signature to verify the registration.
updateSocket(string memory _socket)
‚Äã
function updateSocket(string memory _socket) external;
Description
: Updates the socket used by the module.
Parameters
:
_socket
: The new socket address to update.
registeredEpoch(address _account, uint _epoch)
‚Äã
function registeredEpoch(
address _account,
uint _epoch
) external view returns (bool);
Description
: Checks if a specific account is registered in a given epoch.
Parameters
:
_account
: The address to check.
_epoch
: The epoch number.
Returns
:
bool
indicating whether the account is registered for the specified epoch.
registerNextEpoch(BN254.G1Point memory _signature)
‚Äã
function registerNextEpoch(BN254.G1Point memory _signature) external;
Description
: Registers the next epoch using the provided signature.
Parameters
:
_signature
: The signature used to register the next epoch.
getAggPkG1(uint _epoch, uint _quorumId, bytes memory _quorumBitmap)
‚Äã
function getAggPkG1(
uint _epoch,
uint _quorumId,
bytes memory _quorumBitmap
) external view returns (BN254.G1Point memory aggPkG1, uint total, uint hit);
Description
: Retrieves the aggregated public key for a given epoch and quorum ID.
Parameters
:
_epoch
: The epoch number.
_quorumId
: The quorum ID.
_quorumBitmap
: The quorum bitmap.
Returns
:
aggPkG1
: The aggregated public key.
total
: The number of rows.
hit
: The number of rows that contributed to the aggregation.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/precompiles/precompiles-overview ] ==========

Developer Hub
Building on 0G
0G Chain
Precompiles
On this page
0G Chain Precompiles
Precompiled contracts that extend 0G Chain with powerful native features for AI and blockchain operations.
What Are Precompiles?
‚Äã
Precompiles are special contracts deployed at fixed addresses that execute native code instead of EVM bytecode. They provide:
Gas Efficiency
: 10-100x cheaper than Solidity implementations
Native Features
: Access chain-level functionality
Complex Operations
: Cryptographic functions and state management
0G Chain Precompiles
‚Äã
Beyond standard Ethereum precompiles, 0G Chain adds specialized contracts for decentralized AI infrastructure:
üîê
DASigners
‚Äã
0x0000000000000000000000000000000000001000
Manages data availability signatures for 0G's DA layer.
Key Features
:
Register and manage DA node signers
Query quorum information
Verify data availability proofs
Common Use Case
: Building applications that need to verify data availability directly on-chain.
ü™ô
Wrapped0GBase
‚Äã
0x0000000000000000000000000000000000001002
Wrapped version of native 0G token for DeFi compatibility.
Key Features
:
Wrap/unwrap native 0G tokens
ERC20-compatible interface
Efficient gas operations
Common Use Case
: Integrating 0G tokens with DEXs, lending protocols, or other DeFi applications.
Questions? Get help in our
Discord
#dev-support channel.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/precompiles/precompiles-wrappedogbase ] ==========

Developer Hub
Building on 0G
0G Chain
Precompiles
Wrapped 0G Base
On this page
Overview
Wrapped0GBase is a wrapper for the
x/wrapped-og-base
module in the 0g chain. W0G is a wrapped ERC20 token for native 0G. It supports quota-based mint/burn functions based on native 0G transfers, on top of traditional wrapped token implementation. The minting/burning quota for each address will be determined through governance voting.
x/wrapped-og-base
is the module that supports and maintains the minting/burning quota.
In most cases this precompile should be only called by wrapped 0G contract.
Address
0x0000000000000000000000000000000000001002
Wrapped 0G Token Contract Address
:
0x1Cd0690fF9a693f5EF2dD976660a8dAFc81A109c
This is the official address of the wrapped 0G (W0G) ERC20 token on the 0G chain. Use this address if you want to interact directly with the wrapped 0G token contract for transfers, approvals, or other ERC20 operations.
Interface
https://github.com/0gfoundation/0g-chain/blob/dev/precompiles/interfaces/contracts/IWrappedA0GIBase.sol
Structs
‚Äã
Supply
‚Äã
struct Supply {
uint256 cap;
uint256 initialSupply;
uint256 supply;
}
Description
: Defines the supply details of a minter, including the cap, initial supply, and the current supply.
Fields
:
cap
: The maximum allowed mint supply for the minter.
initialSupply
: The initial mint supply to the minter, equivalent to the initial allowed burn amount.
supply
: The current mint supply used by the minter, set to
initialSupply
at beginning.
Functions
‚Äã
getWA0GI()
‚Äã
function getWA0GI() external view returns (address);
Description
: Retrieves the address of the wrapped 0G token from the wrapped 0G precompile.
Returns
:
address
of the W0G contract.
minterSupply(address minter)
‚Äã
function minterSupply(address minter) external view returns (Supply memory);
Description
: Retrieves the mint supply details for a given minter.
Parameters
:
minter
: The address of the minter.
Returns
: A
Supply
structure containing the mint cap, initial supply, and current supply of the specified minter.
mint(address minter, uint256 amount)
‚Äã
function mint(address minter, uint256 amount) external;
Description
: Mints 0G to WA0GI contract and adds the corresponding amount to the minter's mint supply. If the minter's final mint supply exceeds their mint cap, the transaction will revert.
Parameters
:
minter
: The address of the minter.
amount
: The amount of 0G to mint.
Restrictions
: Can only be called by the WA0GI contract.
burn(address minter, uint256 amount)
‚Äã
function burn(address minter, uint256 amount) external;
Description
: Burns the specified amount of 0G in WA0GI contract on behalf of the minter and reduces the corresponding amount from the minter's mint supply.
Parameters
:
minter
: The address of the minter.
amount
: The amount of 0G to burn.
Restrictions
: Can only be called by the W0G contract.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/staking-interfaces ] ==========

Developer Hub
Building on 0G
0G Chain
Staking Interfaces
On this page
Staking Interfaces
Welcome to the 0G Chain Staking Interfaces documentation. This guide provides comprehensive information about interacting with the 0G Chain staking system through smart contracts, enabling you to build applications that leverage validator operations and delegations.
Running a Validator?
If you want to set up and initialize a validator, see the
Validator Initialization Guide
below.
Quick Navigation
‚Äã
Validator Initialization Guide
- Complete step-by-step setup for becoming a validator
Contract Interfaces
- Smart contract reference documentation
Examples
- Smart contract code examples
Overview
‚Äã
The 0G Chain staking system enables 0G token holders to participate in network consensus and earn rewards through two primary mechanisms:
Becoming a Validator
: Run infrastructure to validate transactions and produce blocks
Delegating to Validators
: Stake tokens with existing validators to earn rewards without running infrastructure
The staking system is built on two core smart contract interfaces:
IStakingContract
: Central registry managing validators and global staking parameters
IValidatorContract
: Individual validator operations including delegations and reward distribution
Prerequisites
‚Äã
Before working with the staking interfaces:
Familiarity with Solidity and smart contract development
Basic knowledge of consensus mechanisms and staking concepts
Quick Start
‚Äã
// Create a validator
IStakingContract staking = IStakingContract(0xea224dBB52F57752044c0C86aD50930091F561B9);
address validator = staking.createAndInitializeValidatorIfNecessary{value: msg.value}(
description, commissionRate, withdrawalFee, pubkey, signature
);
// Delegate to validator
IValidatorContract(validator).delegate{value: msg.value}(msg.sender);
Core Concepts
‚Äã
Validators
‚Äã
Validators process transactions and produce blocks:
Unique Identity
: Identified by 48-byte consensus public key
Operator Control
: Managed by an Ethereum address
Commission
: Set their own reward commission rates
Self-Delegation
: Required minimum stake from operator
Delegations
‚Äã
Token holders earn rewards by delegating to validators:
Share-Based
: Delegations represented as shares in validator pool
Proportional Rewards
: Earnings based on share percentage
Withdrawal Delay
: Undelegation subject to network delay period
Reward Distribution
‚Äã
Rewards flow through multiple layers:
Community Tax
: Applied to all rewards first
Validator Commission
: Taken from remaining rewards
Delegator Distribution
: Proportional to shares held
Contract Interfaces
‚Äã
IStakingContract
‚Äã
0xea224dBB52F57752044c0C86aD50930091F561B9
(Mainnet)
Central registry for validators and global parameters.
Validator Management
‚Äã
// Create validator contract
function createValidator(bytes calldata pubkey) external returns (address);
// Initialize validator with self-delegation
function initializeValidator(
Description calldata description,
uint32 commissionRate,
uint96 withdrawalFeeInGwei,
bytes calldata pubkey,
bytes calldata signature
) external payable;
// Create and initialize in one call
function createAndInitializeValidatorIfNecessary(
Description calldata description,
uint32 commissionRate,
uint96 withdrawalFeeInGwei,
bytes calldata pubkey,
bytes calldata signature
) external payable;
Query Functions
‚Äã
function getValidator(bytes memory pubkey) external view returns (address);
function computeValidatorAddress(bytes calldata pubkey) external view returns (address);
function validatorCount() external view returns (uint32);
function maxValidatorCount() external view returns (uint32);
IValidatorContract
‚Äã
Individual validator operations and delegation management.
Delegation Management
‚Äã
// Delegate tokens (msg.value = amount)
function delegate(address delegatorAddress) external payable returns (uint);
// Undelegate shares (msg.value = withdrawal fee)
function undelegate(address withdrawalAddress, uint shares) external payable returns (uint);
// Withdraw validator commission (only validator operator)
function withdrawCommission(address withdrawalAddress) external returns (uint);
Access Control
The
withdrawCommission
function is restricted to the validator operator only - the address that originally created and manages the validator.
Information Queries
‚Äã
function tokens() external view returns (uint);           // Total tokens (delegated + rewards)
function delegatorShares() external view returns (uint);  // Total shares issued
function getDelegation(address delegator) external view returns (address, uint);
function commissionRate() external view returns (uint32);
function withdrawalFeeInGwei() external view returns (uint96);
Understanding tokens()
The
tokens()
function returns the complete validator balance, including both the original delegated amounts and any accumulated rewards that haven't been distributed yet.
Examples
‚Äã
Creating a Validator
‚Äã
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;
import "./IStakingContract.sol";
contract ValidatorExample {
IStakingContract constant STAKING = IStakingContract(0xea224dBB52F57752044c0C86aD50930091F561B9);
function createValidator(
bytes calldata pubkey,
bytes calldata signature
) external payable {
Description memory desc = Description({
moniker: "My Validator",
identity: "keybase-id",
website: "https://validator.example.com",
securityContact: "security@example.com",
details: "A reliable 0G Chain validator"
});
STAKING.createAndInitializeValidatorIfNecessary{value: msg.value}(
desc,
50000,  // 5% commission
1,      // 1 Gwei withdrawal fee
pubkey,
signature
);
}
}
Delegation Management
‚Äã
contract DelegationHelper {
IStakingContract constant STAKING = IStakingContract(0xea224dBB52F57752044c0C86aD50930091F561B9);
function delegateToValidator(bytes calldata pubkey) external payable {
address validator = STAKING.getValidator(pubkey);
require(validator != address(0), "Validator not found");
IValidatorContract(validator).delegate{value: msg.value}(msg.sender);
}
function getDelegationInfo(
bytes calldata pubkey,
address delegator
) external view returns (uint shares, uint estimatedTokens) {
address validator = STAKING.getValidator(pubkey);
IValidatorContract v = IValidatorContract(validator);
(, shares) = v.getDelegation(delegator);
uint totalTokens = v.tokens();
uint totalShares = v.delegatorShares();
if (totalShares > 0) {
estimatedTokens = (shares * totalTokens) / totalShares;
}
}
}
Validator Initialization
‚Äã
This section covers the complete workflow for setting up and initializing a validator on the 0G Chain.
Step 1: Generate Validator Signature
‚Äã
The validator signature creation process is simplified with a single command:
# Set your environment variables
HOMEDIR={your data path}/0g-home/0gchaind-home
STAKING_ADDRESS=0xea224dBB52F57752044c0C86aD50930091F561B9
AMOUNT=500000000000  # Amount in wei (e.g., 500 for 500 0G tokens)
# Generate validator signature
./bin/0gchaind deposit create-delegation-validator \
$STAKING_ADDRESS \
$AMOUNT \
$HOMEDIR/config/genesis.json \
--home $HOMEDIR \
--chaincfg.chain-spec=mainnet \
--override-rpc-url \
--rpc-dial-url https://evmrpc.0g.ai
Output:
‚úÖ Staking message created successfully!
Note: This is NOT a transaction receipt; use these values to create a validator initialize transaction by Staking Contract.
stakingAddress: 0xea224dBB52F57752044c0C86aD50930091F561B9
pubkey: 0x8497312cd37eef3a7a50017cfbebcb00a9bc400c5881ffb1011cba1c3f29e5d005a980880b7b919b558b95565bc1e628
validatorAddress: 0xA47171b1be26C75732766Ea3433a90A724b3590d
amount: 500000000000
signature: 0xb1dae1164d931c46178785246203eb1c4496b403a7c417bfb33bdfd3c26b552bdbec8e466ed6712ade0b99cc9b0ee8b004cc766687565ba5b0929a1382997a6cc548cf5e390b69f849933c7ac017fbddc612cb3de285fdf89e6fe32e0ccbfc43
Step 2: Validate the Signature
‚Äã
Before submitting the validator initialization transaction, validate the signature:
# Validate the deposit message
./bin/0gchaind deposit validate-delegation \
{pubkey} \
{staking_address} \
{amount} \
{signature} \
$HOMEDIR/config/genesis.json \
--home $HOMEDIR \
--chaincfg.chain-spec=mainnet \
--override-rpc-url \
--rpc-dial-url https://evmrpc.0g.ai
Example:
./bin/0gchaind deposit validate-delegation \
0x8497312cd37eef3a7a50017cfbebcb00a9bc400c5881ffb1011cba1c3f29e5d005a980880b7b919b558b95565bc1e628 \
0xea224dBB52F57752044c0C86aD50930091F561B9 \
500000000000 \
0xb1dae1164d931c46178785246203eb1c4496b403a7c417bfb33bdfd3c26b552bdbec8e466ed6712ade0b99cc9b0ee8b004cc766687565ba5b0929a1382997a6cc548cf5e390b69f849933c7ac017fbddc612cb3de285fdf89e6fe32e0ccbfc43 \
$HOMEDIR/config/genesis.json \
--home $HOMEDIR \
--chaincfg.chain-spec=mainnet \
--override-rpc-url \
--rpc-dial-url https://evmrpc.0g.ai
Output:
‚úÖ Deposit message is valid!
Step 3: Prepare Validator Description and Settings
‚Äã
Description Structure
‚Äã
The Description struct contains your validator's public information. All fields have character limits that must be respected:
Field
Max Length
Description
moniker
70 chars
Your validator's display name
identity
100 chars
Optional:
Keybase identity
website
140 chars
Your validator website URL
securityContact
140 chars
Security contact email
details
200 chars
Additional validator description
Example Description Object:
{
moniker
:
"Your Validator Name"
,
// Max 70 chars
identity
:
"keybase_id"
,
// Optional
website
:
"https://yoursite.com"
,
// Max 140 chars
securityContact
:
"security@you.com"
,
// Max 140 chars
details
:
"Professional validator"
// Max 200 chars
}
Commission Rate Configuration
‚Äã
The commission rate determines what percentage of staking rewards your validator keeps
Value
Commission
100
0.01%
1000
0.1%
10000
1%
50000
5%
100000
10%
Withdrawal Fee Configuration
‚Äã
The withdrawal fee (in Gwei) is charged when delegators undelegate from your validator.
Recommended value:
1
(equivalent to 1 Gneuron, ~1 Gwei)
Step 4: Execute Initialization Transaction
‚Äã
0G Chain Scan (Recommended)
MetaMask / Web3 Wallet
Ethers.js (Programmatic)
The easiest way to initialize your validator using the web interface:
Navigate to
https://chainscan.0g.ai/address/0xea224dBB52F57752044c0C86aD50930091F561B9
Under
Contracts
Tab, click on the
Write As Proxy
button
Find and click on
createAndInitializeValidatorIfNecessary
Fill in all the required parameters:
description
(struct):
moniker
: Your validator name (max 70 chars)
identity
: Keybase ID (optional)
website
: Your website URL
securityContact
: Security contact email
details
: Additional description
commissionRate
: Commission percentage (e.g., 10000 for 1%)
withdrawalFeeInGwei
: Withdrawal fee in Gwei (e.g.,1 Gneuron ~ 1 Gwei)
pubkey
: The public key from Step 1
signature
: The signature from Step 1
Set the
payable amount
to
500
OG tokens
Connect your wallet and execute the transaction
Tip
Using the Chain Scan interface requires no coding knowledge and is the safest option for most users.
For users comfortable with wallet interactions:
Ensure your wallet is connected to
0G Chain Mainnet
Go to the contract address:
0xea224dBB52F57752044c0C86aD50930091F561B9
Use a contract interaction tool like:
0G Chain Scan
Your wallet's built-in contract interaction features
Call
createAndInitializeValidatorIfNecessary
with:
description
: Struct with all validator details
commissionRate
: Commission percentage (e.g., 10000 for 1%)
withdrawalFeeInGwei
: Withdrawal fee in Gwei (~1 Gneuron equivalent)
pubkey
: Your validator's public key
signature
: Your validator's signature
Set transaction value to
500 OG tokens
(500000000000000000000 wei)
Confirm the transaction in your wallet
Important
Ensure your wallet has sufficient funds:
500 OG tokens for initialization
Additional gas fees for the transaction
For developers who want to automate the process:
const
{
ethers
}
=
require
(
"ethers"
)
;
// Initialize provider and wallet
const
provider
=
new
ethers
.
JsonRpcProvider
(
"https://evmrpc.0g.ai"
)
;
const
wallet
=
new
ethers
.
Wallet
(
process
.
env
.
PRIVATE_KEY
,
provider
)
;
// Staking contract ABI (minimal)
const
stakingABI
=
[
{
"inputs"
:
[
{
"components"
:
[
{
"internalType"
:
"string"
,
"name"
:
"moniker"
,
"type"
:
"string"
}
,
{
"internalType"
:
"string"
,
"name"
:
"identity"
,
"type"
:
"string"
}
,
{
"internalType"
:
"string"
,
"name"
:
"website"
,
"type"
:
"string"
}
,
{
"internalType"
:
"string"
,
"name"
:
"securityContact"
,
"type"
:
"string"
}
,
{
"internalType"
:
"string"
,
"name"
:
"details"
,
"type"
:
"string"
}
]
,
"internalType"
:
"struct IStakingContract.Description"
,
"name"
:
"description"
,
"type"
:
"tuple"
}
,
{
"internalType"
:
"uint32"
,
"name"
:
"commissionRate"
,
"type"
:
"uint32"
}
,
{
"internalType"
:
"uint96"
,
"name"
:
"withdrawalFeeInGwei"
,
"type"
:
"uint96"
}
,
{
"internalType"
:
"bytes"
,
"name"
:
"pubkey"
,
"type"
:
"bytes"
}
,
{
"internalType"
:
"bytes"
,
"name"
:
"signature"
,
"type"
:
"bytes"
}
]
,
"name"
:
"createAndInitializeValidatorIfNecessary"
,
"outputs"
:
[
{
"internalType"
:
"address"
,
"name"
:
""
,
"type"
:
"address"
}
]
,
"stateMutability"
:
"payable"
,
"type"
:
"function"
}
]
;
async
function
initializeValidator
(
)
{
const
stakingContract
=
new
ethers
.
Contract
(
"0xea224dBB52F57752044c0C86aD50930091F561B9"
,
stakingABI
,
wallet
)
;
const
description
=
{
moniker
:
"Your Validator Name"
,
identity
:
"keybase_id"
,
website
:
"https://yourvalidator.com"
,
securityContact
:
"security@yourvalidator.com"
,
details
:
"Professional 0G Chain validator"
}
;
try
{
const
tx
=
await
stakingContract
.
createAndInitializeValidatorIfNecessary
(
description
,
10000
,
// 1% commission
1000000
,
// 1 Gwei withdrawal fee
"0x..."
,
// Your pubkey
"0x..."
,
// Your signature
{
value
:
ethers
.
parseEther
(
"500"
)
}
// 500 OG tokens
)
;
console
.
log
(
"Transaction hash:"
,
tx
.
hash
)
;
const
receipt
=
await
tx
.
wait
(
)
;
console
.
log
(
"Validator initialized successfully!"
)
;
console
.
log
(
"Transaction receipt:"
,
receipt
)
;
}
catch
(
error
)
{
console
.
error
(
"Error initializing validator:"
,
error
)
;
}
}
initializeValidator
(
)
;
Environment Setup
Make sure to set
PRIVATE_KEY
in your
.env
file before running the script.
Step 5: Verify Initialization
‚Äã
After successful initialization, you can verify your validator status:
Check the transaction on
0G Chain Scan
:
https://chainscan.0g.ai
Verify your validator status on
0G Explorer
:
https://explorer.0g.ai/mainnet/validators
Activation Time
Your validator may initially appear as
inactive
on the explorer. This is normal. Validators typically take
30-60 minutes
to activate on the network after successful initialization.
You can check the transaction status and logs to confirm the initialization was successful while waiting for activation.
Troubleshooting
‚Äã
Error: "Insufficient funds"
Ensure you have at least 500 OG tokens plus gas fees in your wallet.
# Check balance
cast balance $YOUR_ADDRESS --rpc-url https://evmrpc.0g.ai
Error: "Validator already exists"
Your validator has already been created. Use the
getValidator
function to retrieve your validator address:
const
validatorAddress
=
await
stakingContract
.
getValidator
(
"0x..."
)
;
Error: "Invalid signature"
Regenerate your signature using 0gchaind with the correct validator contract address and delegation amount:
./bin/0gchaind deposit create-delegation-validator \
0xea224dBB52F57752044c0C86aD50930091F561B9 \
500000000000 \
$HOMEDIR/config/genesis.json \
--home $HOMEDIR \
--chaincfg.chain-spec=mainnet \
--override-rpc-url \
--rpc-dial-url https://evmrpc.0g.ai
Error: "Description field too long"
Ensure all Description fields are within character limits:
moniker
: max 70 chars
identity
: max 100 chars
website
: max 140 chars
securityContact
: max 140 chars
details
: max 200 chars
Data Structures
‚Äã
Description Struct
struct Description {
string moniker;         // max 70 chars - Validator display name
string identity;        // max 100 chars - Keybase identity
string website;         // max 140 chars - Website URL
string securityContact; // max 140 chars - Security contact
string details;         // max 200 chars - Additional details
}
Withdrawal Entry
struct WithdrawEntry {
uint completionHeight;  // Block height when withdrawal completes
address delegatorAddress; // Address receiving withdrawal
uint amount;            // Amount being withdrawn
}
Configuration Parameters
‚Äã
Parameter
Description
maxValidatorCount
Maximum validators allowed
minActivationStakesInGwei
Minimum stake for activation
maxEffectiveStakesInGwei
Maximum effective stake
communityTaxRate
Tax on all rewards
minWithdrawabilityDelay
Withdrawal delay blocks
General Troubleshooting
‚Äã
Error: "Validator not found"
The validator hasn't been created yet. Use
createValidator()
first:
address validator = staking.createValidator(pubkey);
Error: "DelegationBelowMinimum"
Your delegation amount is below the minimum required. Check:
uint96 minDelegation = staking.effectiveDelegationInGwei();
require(msg.value >= minDelegation * 1 gwei, "Insufficient delegation");
Error: "NotEnoughWithdrawalFee"
Include the withdrawal fee when undelegating:
uint96 fee = validator.withdrawalFeeInGwei();
validator.undelegate{value: fee * 1 gwei}(recipient, shares);
Contract Addresses
‚Äã
Network
Staking Contract
Mainnet
0xea224dBB52F57752044c0C86aD50930091F561B9
Resources
‚Äã
Run Validator Node
:
Validator Setup Guide
GitHub Repository
:
0G Chain Contracts
Deploy Contracts
:
Contract Deployment
Need help? Join our
Discord
for developer support.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/contracts-on-0g/validator-contract-functions ] ==========

Developer Hub
Building on 0G
0G Chain
Validator Contract Functions
On this page
Validator Contract Functions
Complete function reference for individual validator contracts on 0G Chain.
Quick Links
Validator Initialization
- Set up a new validator
Staking Interfaces
- Main staking system overview
Function Types
‚Äã
View Functions (Free to Call)
‚Äã
Query validator state without gas costs.
Write Functions (Require Gas)
‚Äã
Modify validator state - cost gas to execute.
View Functions
‚Äã
Validator Information
‚Äã
consensusPubkey()
‚Äã
Returns the validator's BLS public key.
Returns
: 48-byte BLS public key
operatorAddress()
‚Äã
Returns the validator operator's wallet address.
Returns
: Operator address
description()
‚Äã
Returns validator metadata.
Returns
:
Moniker (name)
Identity (verification key)
Website
Security contact
Details
commissionRate()
‚Äã
Returns current commission rate.
Returns
: Rate in parts per million (e.g., 100000 = 10%)
withdrawalFeeInGwei()
‚Äã
Returns fee charged for withdrawals.
Returns
: Fee in Gwei
bondStatus()
‚Äã
Returns validator's current status.
Returns
:
Unspecified
- Not activated
Bonded
- Active
Unbonding
- Exiting
Unbonded
- Fully exited
Delegation Queries
‚Äã
tokens()
‚Äã
Returns total tokens delegated to this validator.
Returns
: Total tokens in Wei
delegatorShares()
‚Äã
Returns total shares issued.
Returns
: Total shares
getDelegation(address delegator)
‚Äã
Returns delegation info for a specific delegator.
Parameters
:
delegator
- Delegator's address
Returns
:
Validator address
Number of shares owned
convertToTokens(uint shares)
‚Äã
Converts shares to token amount.
Parameters
:
shares
- Number of shares
Returns
: Equivalent token amount
Use Case
: Calculate token value of your shares
convertToShares(uint tokens)
‚Äã
Converts tokens to shares.
Parameters
:
tokens
- Token amount
Returns
: Equivalent shares
Use Case
: Calculate shares you'll receive when delegating
Rewards & Earnings
‚Äã
rewards()
‚Äã
Returns accumulated rewards pending distribution.
Returns
: Reward amount in Wei
commission()
‚Äã
Returns accumulated commission earned by operator.
Returns
: Commission in Wei
tipFee()
‚Äã
Returns withdrawable tip fees.
Returns
: Tip fee amount in Wei
Calculation
: Contract balance - (commission + rewards + stakes + pending withdrawals)
stakes()
‚Äã
Returns amount actively staked in beacon chain.
Returns
: Staked amount in Wei
annualPercentageYield()
‚Äã
Returns current APY.
Returns
: APY in basis points (e.g., 1500 = 15%)
Withdrawal Queue
‚Äã
withdrawCount()
‚Äã
Returns number of pending withdrawals.
Returns
: Count of pending withdrawals
getWithdraw(uint64 index)
‚Äã
Returns details of a specific withdrawal.
Parameters
:
index
- Position in queue (0-based)
Returns
:
Completion height
Delegator address
Amount
committedWithdrawAmount(uint blockHeight)
‚Äã
Returns total withdrawal amount committed up to a block height.
Parameters
:
blockHeight
- Block number
Returns
: Total committed amount
nextDepositAmount()
‚Äã
Returns amount pending deposit to chain.
Returns
: Pending deposit in Wei
nextWithdrawalAmount()
‚Äã
Returns amount pending withdrawal from chain.
Returns
: Pending withdrawal in Wei
failedWithdrawCount()
‚Äã
Returns count of failed withdrawals.
Returns
: Number of failed withdrawals
failedWithdrawAmount()
‚Äã
Returns total amount in failed withdrawals.
Returns
: Failed withdrawal amount
Write Functions
‚Äã
Validator Configuration
‚Äã
setCommissionRate(uint32 commissionRate_)
‚Äã
Updates validator commission rate.
Who Can Call
: Operator only
Parameters
:
commissionRate_
- New rate (parts per million)
Constraints
: Must be ‚â§ protocol maximum
setWithdrawalFeeInGwei(uint96 withdrawalFeeInGwei_)
‚Äã
Updates withdrawal fee.
Who Can Call
: Operator only
Parameters
:
withdrawalFeeInGwei_
- New fee in Gwei
Constraints
: Must be ‚â§ protocol maximum
setDescription(Description description_)
‚Äã
Updates validator description.
Who Can Call
: Operator only
Parameters
:
description_
- New description struct
Delegation Operations
‚Äã
delegate(address delegatorAddress)
‚Äã
Delegate tokens to this validator.
Who Can Call
: Anyone
Parameters
:
delegatorAddress
- Address to credit with shares
Payment Required
: Yes (minimum 1 Gwei)
Example
:
validator.delegate{value: 100 ether}(msg.sender);
undelegate(address withdrawalAddress, uint shares)
‚Äã
Undelegate tokens from validator.
Who Can Call
: Anyone with shares
Parameters
:
withdrawalAddress
- Address to receive tokens
shares
- Number of shares to undelegate
Payment Required
: Yes (must pay withdrawal fee)
Constraints
:
Must own enough shares
Operator must maintain minimum self-delegation
Tokens released after withdrawal delay
Example
:
uint96 fee = validator.withdrawalFeeInGwei();
validator.undelegate{value: fee * 1 gwei}(recipient, shares);
Earnings Management
‚Äã
withdrawCommission(address withdrawalAddress)
‚Äã
Withdraw accumulated commission.
Who Can Call
: Operator only
Parameters
:
withdrawalAddress
- Address to receive commission
Constraints
:
Must have ‚â•1 Gwei commission
Goes to withdrawal queue (not instant)
withdrawTipFee(address withdrawalAddress)
‚Äã
Withdraw accumulated tip fees.
Who Can Call
: Operator only
Parameters
:
withdrawalAddress
- Address to receive tips
Constraints
:
Only withdraws excess balance
Immediate transfer (not queued)
System Operations
‚Äã
distributeRewards()
‚Äã
Distribute rewards to delegators and commission to operator.
Who Can Call
: Anyone (called by system)
Process
:
Community tax deducted
Commission calculated
Remaining distributed to delegators
processWithdrawQueue()
‚Äã
Process pending withdrawals that are ready.
Who Can Call
: Anyone
When
: After withdrawal delay period
Process
:
Checks ready withdrawals
Transfers funds
Failed transfers go to failed stack
processFailedWithdrawStack()
‚Äã
Retry failed withdrawals.
Who Can Call
: Anyone
Process
:
Retries all failed withdrawals
Still-failed amounts sent to community pool
Common Use Cases
‚Äã
For Delegators
‚Äã
Before Delegating:
// Check validator settings
uint32 commission = validator.commissionRate();
uint96 withdrawalFee = validator.withdrawalFeeInGwei();
uint bondStatus = validator.bondStatus(); // Should be 1 (Bonded)
uint apy = validator.annualPercentageYield();
Delegate Tokens:
validator.delegate{value: amount}(yourAddress);
Check Your Delegation:
(, uint shares) = validator.getDelegation(yourAddress);
uint tokens = validator.convertToTokens(shares);
Undelegate:
uint96 fee = validator.withdrawalFeeInGwei();
validator.undelegate{value: fee * 1 gwei}(withdrawalAddress, shares);
// Wait for withdrawal delay, then call processWithdrawQueue()
For Validator Operators
‚Äã
Update Settings:
validator.setCommissionRate(50000); // 5%
validator.setWithdrawalFeeInGwei(1); // 1 Gwei
Check Earnings:
uint commissionAmount = validator.commission();
uint tipFeeAmount = validator.tipFee();
Withdraw Earnings:
// Withdraw commission (queued)
validator.withdrawCommission(yourAddress);
// Withdraw tips (immediate)
validator.withdrawTipFee(yourAddress);
Monitor Status:
uint totalDelegated = validator.tokens();
uint activeStake = validator.stakes();
uint pendingWithdrawals = validator.withdrawCount();
Important Notes
‚Äã
Key Constraints
Withdrawal Delays
: Undelegations enter a queue with delay period
Minimum Amounts
: Most operations require ‚â•1 Gwei
Fee Requirements
: Undelegation requires prepaid withdrawal fee
Self-Delegation
: Operators must maintain minimum or lose commission
Best Practices
Always check
bondStatus()
before delegating
Use
convertToTokens()
to calculate delegation value
Monitor
failedWithdrawCount()
and process if needed
Operators should regularly claim commission and tips
Related Documentation
‚Äã
Validator Initialization
- Set up a new validator
Staking Interfaces
- Full staking system guide
Run Validator Node
- Node setup guide
Need help? Join our
Discord
for support.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/da-deep-dive ] ==========

Developer Hub
Building on 0G
0G DA
Technical Deep Dive
On this page
0G DA Technical Deep Dive
The Data Availability (DA) module allows users to submit a piece of data, referred to as a
DA blob
. This data is redundantly encoded by the client's proxy and divided into several slices, which are then sent to DA nodes.
DA nodes
gain eligibility to verify the correctness of DA slices by staking. Each DA node verifies the integrity and correctness of its slice and signs it. Once more than 2/3 of the aggregated signatures are on-chain, the data behind the related hash is considered to be published decentrally.
To incentivize DA nodes to store the signed data for a period, the signing process itself does not provide any rewards. Instead, rewards are distributed through a process called
DA Sampling
. During each DA Sample round, any DA slice within a certain timeframe can generate a lottery chance for a reward. DA nodes must store the corresponding slice to redeem the lottery chance and claim the reward.
The process of generating DA nodes is the same as the underlying chain's PoS process, both achieved through staking. During each DA epoch (approximately 8 hours), DA nodes are assigned to several quorums. Within each quorum, nodes are assigned numbers 0 through 3071. Each number is assigned to exactly one node, but a node may be assigned to multiple quorums, depending on its staking weight.
DA Processing Flow
‚Äã
DA takes an input of data up to 32,505,852 bytes in length and processes it as follows:
Padding and Size Encoding:
Pad the data with zeros until it reaches 32,505,852 bytes
Add a little-endian format 4-byte integer at the end to indicate the original input size
Matrix Formation:
Slice the padded data into a 1024-row by 1024-column matrix, filling each row consecutively, with each element being 31 bytes
Pad each 31-byte element with an additional 1-byte zero, making it 32 bytes per element
Redundant Encoding:
Expand the data to a 3072-row by 1024-column matrix using redundancy coding
Calculate the
erasure commitment
and
data root
of the expanded matrix
Submission to DA Contract:
Submit the erasure commitment and data root to
the DA contract
and pay the fee
The DA contract will determine the epoch to which the data belongs and assign a quorum
Data Distribution:
Send the erasure commitment, data root, each row of the matrix, and necessary proofs of correctness to the corresponding DA nodes
Signature Aggregation:
More than 2/3 of the DA nodes sign the erasure commitment and data root
Aggregate the signatures using the BLS signature algorithm and submit the aggregated signature to the DA contract
Details of Erasure Encoding
‚Äã
After matrix formation, each element is processed into a 32-byte data unit, which can be viewed interchangeably as either 32 bytes of data or a 256-bit little-endian integer. Denote the element in the
i
i
i
-th row and
j
j
j
-th column as
c
i
,
j
c_{i,j}
c
i
,
j
‚Äã
.
Let the finite field
F
\mathbb{F}
F
be the scalar field of the
BN254 curve
. Each element
c
i
,
j
c_{i,j}
c
i
,
j
‚Äã
is also considered an integer within the finite field
F
\mathbb{F}
F
. Let
p
p
p
be the order of
F
\mathbb{F}
F
, a known large number that can be expressed as
2
28
√ó
A
+
1
2^{28} \times A + 1
2
28
√ó
A
+
1
, where
A
A
A
is an odd number. The number 3 is a generator of the multiplicative group of the
F
\mathbb{F}
F
. Define
u
=
3
2
6
√ó
A
u = 3^{2^6 \times A}
u
=
3
2
6
√ó
A
and
w
=
3
2
8
√ó
A
w=3^{2^8\times A}
w
=
3
2
8
√ó
A
, so
w
2
20
=
1
w^{2^{20}} = 1
w
2
20
=
1
and
u
4
=
w
u^4=w
u
4
=
w
.
Now we define a polynomial
f
f
f
over
F
‚Üí
F
\mathbb{F}\rightarrow\mathbb{F}
F
‚Üí
F
with degree
d
=
2
20
‚àí
1
d=2^{20}-1
d
=
2
20
‚àí
1
satisfying
‚àÄ
0
‚â§
i
<
1024
,
0
‚â§
j
<
1024
,
f
(
w
1024
j
+
i
)
=
c
i
,
j
\forall\, 0\le i< 1024,\, 0\le j< 1024,\,f\left(w^{1024j+i}\right)=c_{i,j}
‚àÄ
0
‚â§
i
<
1024
,
0
‚â§
j
<
1024
,
f
(
w
1024
j
+
i
)
=
c
i
,
j
‚Äã
Then we extend the
1024
√ó
1024
1024\times1024
1024
√ó
1024
matrix into
1024
√ó
3072
1024\times 3072
1024
√ó
3072
matrix, where
‚àÄ
1024
‚â§
i
<
2048
,
0
‚â§
j
<
1024
,
c
i
,
j
=
f
(
u
2
‚ãÖ
w
1024
j
+
(
i
‚àí
1024
)
)
\forall\, 1024\le i< 2048,\, 0\le j< 1024,\,c_{i,j}=f\left(u^2\cdot w^{1024j+(i-1024)}\right)
‚àÄ
1024
‚â§
i
<
2048
,
0
‚â§
j
<
1024
,
c
i
,
j
‚Äã
=
f
(
u
2
‚ãÖ
w
1024
j
+
(
i
‚àí
1024
)
)
‚àÄ
2048
‚â§
i
<
3072
,
0
‚â§
j
<
1024
,
c
i
,
j
=
f
(
u
‚ãÖ
w
1024
j
+
(
i
‚àí
2048
)
)
\forall\, 2048\le i< 3072,\, 0\le j< 1024,\,c_{i,j}=f\left(u\cdot w^{1024j+(i-2048)}\right)
‚àÄ
2048
‚â§
i
<
3072
,
0
‚â§
j
<
1024
,
c
i
,
j
‚Äã
=
f
(
u
‚ãÖ
w
1024
j
+
(
i
‚àí
2048
)
)
The
erasure commitment
is the KZG commitment of
f
f
f
, defined as
f
(
œÑ
)
‚ãÖ
G
f(\tau)\cdot G
f
(
œÑ
)
‚ãÖ
G
, where
G
G
G
is the starting point of BN254 G1 curve, and
œÑ
\tau
œÑ
is a latent parameter from the
perpetual powers of tau trusted setup ceremony
.
The
data root
is defined as the input root by treating the 1024*3072 32-byte elements as a continuous storage submission input. Specifically, according to the storage submission requirements, these data does not need to pad any zeros, and will be divided into a 16384-element sector array and an 8192-element sector array.
DA nodes need to verify two parts:
The consistency between the received slice and the data root, mainly achieved through Merkle proofs
The consistency between the received slice and the erasure commitment, verified using KZG proofs. Here, we use the AMT protocol optimization introduced in
LVMT
to reduce the proving overhead
DA Sampling
‚Äã
The blockchain will periodically release DA Sampling tasks at preset height every
SAMPLE_PERIOD
blocks, with the parent block hash of these heights used as the
sampleSeed
for DA Sampling.
List of Parameters
‚Äã
Constant parameters
Parameter
Requirement
Default value
MAX_PODAS_TARGET
2^256 / 128 - 1
Admin adjustable parameters
Parameter
Requirement
Default value
Code
TARGET_SUBMITS
20
Link
EPOCH_WINDOW_SIZE
300 (about 3 months)
Link
SAMPLE_PERIOD
30 (about 1.5 minutes)
Link
Responses
‚Äã
During each period, each DA slice (one row) can be divided into 32 sub-lines. For each sub-line, the
podasQuality
will be computed using the
dataRoot
and assigned
epoch
and
quorumId
of its corresponding DA blob.
note
By default, all integers are in 256-bit big-endian format when computing hash values.
lineIndex
is the only exception, which is in 64-bit big-endian format.
The hash value can be viewed interchangeably as either 32 bytes of data or a 256-bit big-endian integer.
lineQuality
=
keccak256
(
sampleSeed
,
epoch
,
quorumId
,
dataRoot
,
lineIndex
)
;
dataQuality
=
keccak256
(
lineQuality
,
sublineIndex
,
data
)
;
podasQuality
=
lineQuality
+
dataQuality
If the
podasQuality
is less than the current
podasTarget
in the DA contract and the
epoch
falls within
[currentEpoch - EPOCH_WINDOW_SIZE, currentEpoch)
, then this sub-line is regarded as a
valid DAS response
and is eligible for the reward. The DA node assigned to this row can claim the reward.
During a sample period, at most
TARGET_SUBMITS √ó 2
DAS responses can be submitted and rewarded; any submissions exceeding this limit will be rejected.
Difficulty Adjustment
‚Äã
TARGET_SUBMITS
valid responses are expected in a sample period. If more or fewer responses are submitted during a sample period, the
podasTarget
will be adjusted as follows:
podasTarget
-=
podasTarget
*
(
actualSubmits
-
TARGET_SUBMITS
)
/
TARGET_SUBMITS
/
8
Economic Model
‚Äã
List of Parameters
‚Äã
Admin adjustable parameters
Parameter
Requirement
Default value
Code
BASE_REWARD
0
Link
BLOB_PRICE
0
Link
SERVICE_FEE_RATE_BP
0
Link
REWARD_RATIO
[1]
1,200,000
Link
[1]
TARGET_SUBMITS
√ó Time elapsed for
EPOCH_WINDOW_SIZE
epochs / Time elapsed in
SAMPLE_PERIOD
/
REWARD_RATIO
should be approximately 0.5 to 2.
Pricing
‚Äã
When users submit the metadata for a DA blob, they need to pay a fee in amount of
BLOB_PRICE
.
Reward
‚Äã
When a DA epoch ends, all the rewards from that DA epoch will be stored in the DA reward pool. Each time a valid response is submitted,
1 / REWARD_RATIO
of the reward pool will be distributed to the corresponding DA node.
System Rewards
‚Äã
In the early stages of the ecosystem, the foundation can reserve a portion of tokens for system rewards. When the DA node submits a valid response, an additional reward of
BASE_REWARD
will be issued.
The funds for the base reward will be manually deposited into the reward contract and tracked separately. If the balance for the base reward is insufficient to cover a single base reward, miners will not be able to receive the full base reward.
Service Fee
‚Äã
A system service fee is charged as a proportion of the DA fees paid by the user, according to the parameter
SERVICE_FEE_RATE_BP
.
Run a Node
‚Äã
See
here
for instructions to become DA signer and run your own node.
Ready to dive deeper into 0G DA? Join our
Discord
for technical discussions.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/da-integration ] ==========

Developer Hub
Building on 0G
0G DA
DA Client Nodes
On this page
0G Data Availability (DA): Integration
To submit data to the 0G DA, you must run a DA Client node and the Encoder node. The DA client interfaces with the Encoder for data encoding and the Retriever for data access.
Overview
‚Äã
Maximum Blob Size
‚Äã
Users can submit data blobs up to 32,505,852 bytes in length, which are then processed, encoded, and distributed across a network of DA nodes. The system employs a sophisticated data processing flow that includes padding, matrix formation, redundant encoding, and signature aggregation.
Fee Market
‚Äã
As the DA user, you pay a fee which is the (BLOB_PRICE) when submitting DA blob data.
Submitting Data
‚Äã
See example here:
https://github.com/0gfoundation/0g-da-example-rust/blob/main/src/disperser.proto
Hardware Requirements
‚Äã
The following table outlines the hardware requirements for different types of DA Client nodes:
Node Type
Memory
CPU
Disk
Bandwidth
Additional Notes
DA Client
8 GB
2 cores
-
100 MBps
For Download / Upload
DA Encoder
-
-
-
-
NVIDIA Drivers: 12.04 on the RTX 4090*
DA Retriever
8 GB
2 cores
-
100 MBps
For Download / Upload
Standing up DA Client, Encoder, Retriever
‚Äã
DA Client
DA Encoder
DA Retriever
DA Client Node Installation
‚Äã
1. Clone the DA Client Node Repo
git clone https://github.com/0gfoundation/0g-da-client.git
2. Build the Docker Image
cd 0g-da-client
docker build -t 0g-da-client -f combined.Dockerfile .
3. Set Environment Variables
Create a file named
envfile.env
with the following content. Be sure you paste in your private key.
# envfile.env
COMBINED_SERVER_CHAIN_RPC=https://evmrpc-testnet.0g.ai
COMBINED_SERVER_PRIVATE_KEY=YOUR_PRIVATE_KEY
ENTRANCE_CONTRACT_ADDR=0x857C0A28A8634614BB2C96039Cf4a20AFF709Aa9
COMBINED_SERVER_RECEIPT_POLLING_ROUNDS=180
COMBINED_SERVER_RECEIPT_POLLING_INTERVAL=1s
COMBINED_SERVER_TX_GAS_LIMIT=2000000
COMBINED_SERVER_USE_MEMORY_DB=true
COMBINED_SERVER_KV_DB_PATH=/runtime/
COMBINED_SERVER_TimeToExpire=2592000
DISPERSER_SERVER_GRPC_PORT=51001
BATCHER_DASIGNERS_CONTRACT_ADDRESS=0x0000000000000000000000000000000000001000
BATCHER_FINALIZER_INTERVAL=20s
BATCHER_CONFIRMER_NUM=3
BATCHER_MAX_NUM_RETRIES_PER_BLOB=3
BATCHER_FINALIZED_BLOCK_COUNT=50
BATCHER_BATCH_SIZE_LIMIT=500
BATCHER_ENCODING_INTERVAL=3s
BATCHER_ENCODING_REQUEST_QUEUE_SIZE=1
BATCHER_PULL_INTERVAL=10s
BATCHER_SIGNING_INTERVAL=3s
BATCHER_SIGNED_PULL_INTERVAL=20s
BATCHER_EXPIRATION_POLL_INTERVAL=3600
BATCHER_ENCODER_ADDRESS=DA_ENCODER_SERVER
BATCHER_ENCODING_TIMEOUT=300s
BATCHER_SIGNING_TIMEOUT=60s
BATCHER_CHAIN_READ_TIMEOUT=12s
BATCHER_CHAIN_WRITE_TIMEOUT=13s
4. Run the Docker Node
docker run -d --env-file envfile.env --name 0g-da-client -v ./run:/runtime -p 51001:51001 0g-da-client combined
Configuration
‚Äã
Field
Description
--chain.rpc
JSON RPC node endpoint for the blockchain network.
--chain.private-key
Hex-encoded signer private key.
--chain.receipt-wait-rounds
Maximum retries to wait for transaction receipt.
--chain.receipt-wait-interval
Interval between retries when waiting for transaction receipt.
--chain.gas-limit
Transaction gas limit.
--combined-server.use-memory-db
Whether to use mem-db for blob storage.
--combined-server.storage.kv-db-path
Path for level db.
--combined-server.storage.time-to-expire
Expiration duration for blobs in level db.
--combined-server.log.level-file
File log level.
--combined-server.log.level-std
Standard output log level.
--combined-server.log.path
Log file path.
--disperser-server.grpc-port
Server listening port.
--disperser-server.retriever-address
GRPC host for retriever.
--batcher.da-entrance-contract
Hex-encoded da-entrance contract address.
--batcher.da-signers-contract
Hex-encoded da-signers contract address.
--batcher.finalizer-interval
Interval for finalizing operations.
--batcher.finalized-block-count
Default number of blocks between finalized block and latest block.
--batcher.confirmer-num
Number of Confirmer threads.
--batcher.max-num-retries-for-sign
Number of retries before signing fails.
--batcher.batch-size-limit
Maximum batch size in MiB.
--batcher.encoding-request-queue-size
Size of the encoding request queue.
--batcher.encoding-interval
Interval between blob encoding requests.
--batcher.pull-interval
Interval for pulling from the encoded queue.
--batcher.signing-interval
Interval between slice signing requests.
--batcher.signed-pull-interval
Interval for pulling from the signed queue.
--encoder-socket
GRPC host of the encoder.
--encoding-timeout
Total time to wait for a response from encoder.
--signing-timeout
Total time to wait for a response from signer.
Features
‚Äã
parallel
: Uses parallel algorithms for computations, maximizing CPU resource utilization.
cuda
: Uses GPU for computations, applicable only on platforms with NVIDIA GPUs.
note
GPU support is currently tested with NVIDIA 12.04 drivers on the RTX 4090. Other NVIDIA GPUs may require parameter adjustments and have not been tuned yet.
Preparation
‚Äã
Install Rust
‚Äã
Ensure you have curl installed.
Run the following command to install Rust:
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
After installation, add the cargo bin directory to your PATH environment variable:
source $HOME/.cargo/env
Verify the installation:
rustc --version
Install other dependencies
‚Äã
# Install Protocol Buffers Compiler
sudo apt-get install -y protobuf-compiler
# Install a specific nightly Rust toolchain and rustfmt
rustup toolchain install nightly-2024-02-04-x86_64-unknown-linux-gnu
rustup component add --toolchain nightly-2024-02-04-x86_64-unknown-linux-gnu rustfmt
# Add the necessary Rust target
rustup target add x86_64-unknown-linux-gnu
Install CUDA (for GPU feature)
‚Äã
Ensure you have an NVIDIA GPU with the required drivers. Then follow the instructions from
CUDA Toolkit
.
Verify the installation:
nvidia-smi
nvcc --version
Building Public Parameters
‚Äã
The public parameters for the cryptographic protocol are built in two steps:
1. Download and process the perpetual power of tau
‚Äã
We use the challenge_0084 file from the nearly most recent submission.
curl https://pse-trusted-setup-ppot.s3.eu-central-1.amazonaws.com/challenge_0084 -o challenge_0084
2. Build the AMT parameters
‚Äã
You can either construct these parameters yourself or download pre-built files.
Choice 1: Download the pre-built files
‚Äã
./dev-support/download_params.sh
Choice 2: Construct the parameters yourself
‚Äã
./dev_support/build_params.sh challenge_0084
Running the Server
‚Äã
Run the server with the following command:
cargo run -r -p server --features grpc/parallel,grpc/cuda -- --config run/config.toml
note
If you do not have a CUDA environment, remove the cuda feature.
DA Encoder will serve on port 34000 with specified gRPC interface.
Using the Verification Logic
‚Äã
Add the following to
Cargo.toml
of your crate:
zg-encoder = { git = "https://github.com/0gfoundation/0g-da-encoder.git" }
Use the
zg_encoder::EncodedSlice::verify
function for verifying.
Benchmark the Performance
‚Äã
Run the following task:
cargo bench -p grpc --features grpc/parallel,grpc/cuda --bench process_data --features zg-encoder/production_mode -- --nocapture
Development and Testing
‚Äã
Run the following script for complete testing:
./dev_support/test.sh
DA Retriever Node Installation
‚Äã
1. Clone the DA Retriever Node Repo
git clone https://github.com/0gfoundation/0g-da-retriever.git
cd 0g-da-retriever
2. Edit Files
Add the following line to Dockerfile.dockerignore file:
!/run/config.toml
Replace Dockerfile with the following:
# Dockerfile
FROM rust:alpine3.20 as builder
WORKDIR /0g-da-retriever
COPY . .
RUN apk update && apk add --no-cache make protobuf-dev musl-dev
RUN cargo build --release
FROM alpine:3.20
WORKDIR /0g-da-retriever
COPY --from=builder /0g-da-retriever/target/release/retriever /usr/local/bin/retriever
# Copy the config file into the container
COPY --from=builder /0g-da-retriever/run/config.toml ./run/config.toml
# Set the entrypoint to run the retriever binary
CMD ["/usr/local/bin/retriever"]
Replace the Config impl in
/retriever/src/config.rs
with the following:
impl
Config
{
pub
fn
from_cli_file
(
)
->
Result
<
Self
>
{
let
matches
=
cli
::
cli_app
(
)
.
get_matches
(
)
;
let
config_file
=
matches
.
get_one
::
<
String
>
(
"config"
)
.
map
(
|
s
|
s
.
as_str
(
)
)
.
unwrap_or
(
"/0g-da-retriever/run/config.toml"
)
;
let
c
=
RawConfig
(
config
::
Config
::
builder
(
)
.
add_source
(
config
::
File
::
with_name
(
config_file
)
)
.
build
(
)
?
,
)
;
Ok
(
Self
{
log_level
:
c
.
get_string
(
"log_level"
)
?
,
eth_rpc_url
:
c
.
get_string
(
"eth_rpc_endpoint"
)
?
,
grpc_listen_address
:
c
.
get_string
(
"grpc_listen_address"
)
?
,
max_ongoing_retrieve_request
:
c
.
get_u64_opt
(
"max_ongoing_retrieve_request"
)
?
,
}
)
}
}
3. Update Configuration
Update configuration file
run/config.toml
as needed with context below.
Field
Description
log_level
Set log level.
grpc_listen_address
Server listening address.
eth_rpc_endpoint
JSON RPC node endpoint for the blockchain network.
4. Build and Run the Docker Node
docker build -t 0g-da-retriever .
docker run -d --name 0g-da-retriever -p 34005:34005 0g-da-retriever
Troubleshooting
‚Äã
DA Client connection issues
Verify the RPC endpoint is accessible
Check that your private key has sufficient funds for gas
Ensure the contract addresses are correct for your network
Review logs:
docker logs 0g-da-client
Encoder GPU not detected
Verify NVIDIA drivers are installed:
nvidia-smi
Check CUDA installation:
nvcc --version
Ensure Docker has GPU access if using containers
Try running without cuda feature if GPU is not available
Retriever fails to start
Check that port 34005 is not already in use
Verify the Ethereum RPC endpoint is accessible
Ensure config.toml is properly formatted
Review container logs for specific errors
Next Steps
‚Äã
Integration Examples
‚Üí
DA Examples Repository
Join Community
‚Üí
Discord
for support
Run a DA Node
‚Üí
DA Node Guide
Ready to integrate 0G DA into your application? Start with the DA Client and connect to the network.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/indexing/goldsky ] ==========

Developer Hub
Building on 0G
0G Chain
Indexing
Goldsky Subgraphs
On this page
Indexing 0G with Goldsky
Goldsky is Web3's real-time data platform, giving developers the fastest way to query, stream, and scale onchain data without worrying about maintaining infrastructure.
With resilient subgraphs and flexible data streaming pipelines, you can focus on building great user experiences while Goldsky handles the heavy lifting of indexing.
Why Goldsky?
‚Äã
Reliable Performance
: Scalable infra built to handle data challenges as your app grows.
Intuitive Tools
: Easily integrate without being slowed down by complex data engineering.
24/7 Support
: Goldsky offers
support
when you need to fix bugs or optimize deployments.
Goldsky Products
‚Äã
Subgraphs
‚Äã
Subgraphs
make blockchain data queryable with GraphQL endpoints so you can easily fetch specifically only the data your app needs.
Fast Queries
: Optimized for low latency and high throughput.
Customizable
: Tailor indexing logic to your app's exact needs.
Organized & Scalable
: Tagging and versioning keep data clean as your project grows.
Typical Use Cases
: dApps, NFT marketplaces, gaming, DAOs, or any app needing reliable onchain data.
Mirror
‚Äã
Mirror
streams decoded blockchain data directly into your database so you can own your data and have full control over what to do with it.
Realtime Streaming
: Stream onchain activity straight into your existing systems.
Combine Data Sources
: Access data across multiple chains and join it with your own offchain data.
Continuously Synced
: Automatic updates keep everything accurate and fresh.
Typical Use Cases
: Advanced analytics, loyalty programs, points/leaderboards, user progress tracking, custom dashboards, or app requiring rich onchain data joined with other datasets.
Getting Started
‚Äã
Check out the
Goldsky docs
to start indexing today. Build smarter, scale faster, and deliver seamless experiences to your users.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/inft/erc7857 ] ==========

Developer Hub
Building on 0G
INFTs (Intelligent NFTs)
ERC-7857 Standard
On this page
ERC-7857: Technical Standard
Overview
‚Äã
ERC-7857 extends ERC-721 to support encrypted metadata, specifically designed for tokenizing AI agents and sensitive digital assets.
Prerequisites
Understanding of ERC-721 NFT standard
Basic cryptography knowledge (encryption, hashing)
Smart contract development experience
Familiarity with oracle systems
Document Purpose
‚Äã
This page provides the technical specification, implementation details, and security considerations for ERC-7857. For high-level concepts, see the
INFT Overview
.
Key Technical Features
‚Äã
Feature
Description
Benefit
Encrypted Metadata
Store sensitive data securely
Protects proprietary AI models
Secure Re-encryption
Transfer without data exposure
Maintains privacy during ownership changes
Oracle Verification
TEE/ZKP proof validation
Ensures transfer integrity
Authorized Usage
Grant access without ownership
Enables AI-as-a-Service models
Technical Specification
‚Äã
Core Interface
‚Äã
interface IERC7857 is IERC721 {
// Transfer with metadata re-encryption
function transfer(
address from,
address to,
uint256 tokenId,
bytes calldata sealedKey,
bytes calldata proof
) external;
// Clone token with same metadata
function clone(
address to,
uint256 tokenId,
bytes calldata sealedKey,
bytes calldata proof
) external returns (uint256 newTokenId);
// Authorize usage without revealing data
function authorizeUsage(
uint256 tokenId,
address executor,
bytes calldata permissions
) external;
}
Transfer Architecture
‚Äã
Security Guarantees:
‚úÖ Metadata remains encrypted throughout process
‚úÖ Only new owner can decrypt transferred data
‚úÖ Transfer integrity cryptographically verified
‚úÖ No intermediary can access sensitive information
Oracle Implementations
‚Äã
ERC-7857 supports two oracle types for secure metadata re-encryption:
TEE (Trusted Execution Environment)
‚Äã
How it works:
Sender transmits encrypted data + key to TEE
TEE securely decrypts data in isolated environment
TEE generates new key and re-encrypts metadata
TEE encrypts new key with receiver's public key
TEE outputs sealed key and hash values
Advantages:
Hardware-level security guarantees
TEE can generate cryptographically secure keys
Attestation provides proof of secure execution
TEE Implementation Example
‚Äã
class
TEEOracle
{
async
processTransfer
(
encryptedData
,
oldKey
,
receiverPublicKey
)
{
// All operations happen inside secure enclave
try
{
// Step 1: Decrypt original data
const
data
=
await
this
.
decryptSecurely
(
encryptedData
,
oldKey
)
;
// Step 2: Generate new encryption key
const
newKey
=
await
this
.
generateSecureKey
(
)
;
// Step 3: Re-encrypt with new key
const
newEncryptedData
=
await
this
.
encryptSecurely
(
data
,
newKey
)
;
// Step 4: Seal key for receiver
const
sealedKey
=
await
this
.
sealForReceiver
(
newKey
,
receiverPublicKey
)
;
// Step 5: Generate attestation proof
const
proof
=
await
this
.
generateAttestation
(
{
originalHash
:
hash
(
encryptedData
)
,
newHash
:
hash
(
newEncryptedData
)
,
receiverKey
:
receiverPublicKey
}
)
;
return
{
newEncryptedData
,
sealedKey
,
proof
}
;
}
catch
(
error
)
{
throw
new
Error
(
`
TEE processing failed:
${
error
.
message
}
`
)
;
}
}
}
ZKP (Zero-Knowledge Proof)
‚Äã
How it works:
Sender provides old and new keys to ZKP system
ZKP circuit verifies correct re-encryption
Proof generated without revealing keys or data
Smart contract validates ZKP proof
Considerations:
Cannot independently generate new keys
Requires sender to handle key generation
Receivers should rotate keys post-transfer
Computationally intensive proof generation
ZKP Circuit Example
‚Äã
// ZKP circuit for verifying re-encryption
use
ark_relations
::
r1cs
::
SynthesisError
;
pub
struct
ReencryptionCircuit
{
// Public inputs (known to verifier)
pub
old_data_hash
:
Option
<
Fr
>
,
pub
new_data_hash
:
Option
<
Fr
>
,
pub
receiver_pubkey
:
Option
<
Fr
>
,
// Private inputs (known only to prover)
pub
encrypted_data
:
Option
<
Vec
<
u8
>>
,
pub
old_key
:
Option
<
Vec
<
u8
>>
,
pub
new_key
:
Option
<
Vec
<
u8
>>
,
pub
plaintext_data
:
Option
<
Vec
<
u8
>>
,
}
impl
ConstraintSynthesizer
<
Fr
>
for
ReencryptionCircuit
{
fn
generate_constraints
(
self
,
cs
:
ConstraintSystemRef
<
Fr
>
,
)
->
Result
<
(
)
,
SynthesisError
>
{
// Step 1: Verify decryption of original data
let
decrypted
=
decrypt_constraint
(
cs
.
clone
(
)
,
&
self
.
encrypted_data
?
,
&
self
.
old_key
?
)
?
;
// Step 2: Verify plaintext matches decrypted data
enforce_equal
(
cs
.
clone
(
)
,
&
decrypted
,
&
self
.
plaintext_data
?
)
?
;
// Step 3: Verify re-encryption with new key
let
reencrypted
=
encrypt_constraint
(
cs
.
clone
(
)
,
&
self
.
plaintext_data
?
,
&
self
.
new_key
?
)
?
;
// Step 4: Verify hash consistency
let
computed_hash
=
hash_constraint
(
cs
.
clone
(
)
,
&
reencrypted
)
?
;
enforce_equal
(
cs
,
&
computed_hash
,
&
self
.
new_data_hash
?
)
?
;
Ok
(
(
)
)
}
}
Implementation Guidelines
‚Äã
Smart Contract Architecture
‚Äã
pragma solidity ^0.8.19;
import "@openzeppelin/contracts/token/ERC721/ERC721.sol";
import "@openzeppelin/contracts/access/Ownable.sol";
import "@openzeppelin/contracts/security/ReentrancyGuard.sol";
contract ERC7857 is ERC721, Ownable, ReentrancyGuard {
// State variables
mapping(uint256 => bytes32) private _metadataHashes;
mapping(uint256 => string) private _encryptedURIs;
mapping(uint256 => mapping(address => bytes)) private _authorizations;
// Oracle configuration
address public oracle;
uint256 public constant PROOF_VALIDITY_PERIOD = 1 hours;
// Events
event MetadataUpdated(uint256 indexed tokenId, bytes32 newHash);
event UsageAuthorized(uint256 indexed tokenId, address indexed executor);
event OracleUpdated(address oldOracle, address newOracle);
modifier validProof(bytes calldata proof) {
require(oracle != address(0), "Oracle not set");
require(IOracle(oracle).verifyProof(proof), "Invalid proof");
_;
}
function transfer(
address from,
address to,
uint256 tokenId,
bytes calldata sealedKey,
bytes calldata proof
) external nonReentrant validProof(proof) {
require(ownerOf(tokenId) == from, "Not owner");
require(to != address(0), "Invalid recipient");
// Update metadata access for new owner
_updateMetadataAccess(tokenId, to, sealedKey, proof);
// Transfer NFT ownership
_transfer(from, to, tokenId);
emit MetadataUpdated(tokenId, keccak256(sealedKey));
}
function _updateMetadataAccess(
uint256 tokenId,
address newOwner,
bytes calldata sealedKey,
bytes calldata proof
) internal {
// Verify proof contains correct metadata hash
bytes32 expectedHash = _extractHashFromProof(proof);
_metadataHashes[tokenId] = expectedHash;
// Store new encrypted URI if provided
string memory newURI = _extractURIFromProof(proof);
if (bytes(newURI).length > 0) {
_encryptedURIs[tokenId] = newURI;
}
}
}
Metadata Management
‚Äã
class
MetadataManager
{
constructor
(
storageProvider
,
encryptionService
,
options
=
{
}
)
{
this
.
storage
=
storageProvider
;
this
.
encryption
=
encryptionService
;
this
.
options
=
{
keySize
:
256
,
algorithm
:
'AES-GCM'
,
...
options
}
;
}
async
storeMetadata
(
data
,
ownerPublicKey
)
{
try
{
// Validate input data
this
.
_validateMetadata
(
data
)
;
// Generate encryption key
const
key
=
await
this
.
encryption
.
generateKey
(
{
size
:
this
.
options
.
keySize
,
algorithm
:
this
.
options
.
algorithm
}
)
;
// Encrypt metadata
const
encrypted
=
await
this
.
encryption
.
encrypt
(
data
,
key
,
{
includeMac
:
true
,
version
:
'1.0'
}
)
;
// Store encrypted data on distributed storage
const
uri
=
await
this
.
storage
.
store
(
encrypted
,
{
redundancy
:
3
,
availability
:
'99.9%'
}
)
;
// Seal key for owner using their public key
const
sealedKey
=
await
this
.
encryption
.
sealForOwner
(
key
,
ownerPublicKey
)
;
// Generate metadata hash for verification
const
metadataHash
=
await
this
.
encryption
.
hash
(
encrypted
)
;
return
{
uri
,
sealedKey
,
metadataHash
,
algorithm
:
this
.
options
.
algorithm
,
version
:
'1.0'
}
;
}
catch
(
error
)
{
throw
new
Error
(
`
Metadata storage failed:
${
error
.
message
}
`
)
;
}
}
async
retrieveMetadata
(
uri
,
sealedKey
,
ownerPrivateKey
)
{
try
{
// Fetch encrypted data from storage
const
encrypted
=
await
this
.
storage
.
retrieve
(
uri
)
;
// Unseal the encryption key
const
key
=
await
this
.
encryption
.
unsealKey
(
sealedKey
,
ownerPrivateKey
)
;
// Decrypt and return metadata
const
decrypted
=
await
this
.
encryption
.
decrypt
(
encrypted
,
key
)
;
return
decrypted
;
}
catch
(
error
)
{
throw
new
Error
(
`
Metadata retrieval failed:
${
error
.
message
}
`
)
;
}
}
_validateMetadata
(
data
)
{
if
(
!
data
||
typeof
data
!==
'object'
)
{
throw
new
Error
(
'Invalid metadata format'
)
;
}
const
maxSize
=
10
*
1024
*
1024
;
// 10MB limit
const
serialized
=
JSON
.
stringify
(
data
)
;
if
(
serialized
.
length
>
maxSize
)
{
throw
new
Error
(
'Metadata exceeds size limit'
)
;
}
}
}
Security Considerations
‚Äã
üîë Key Management
‚Äã
Best Practices:
Use hardware security modules (HSM) when available
Implement automatic key rotation every 90 days
Store private keys in secure enclaves or hardware wallets
Never log or expose private keys in error messages
Implementation:
class
SecureKeyManager
{
constructor
(
hsmProvider
)
{
this
.
hsm
=
hsmProvider
;
this
.
keyRotationInterval
=
90
*
24
*
60
*
60
*
1000
;
// 90 days
}
async
generateKey
(
)
{
// Use HSM if available, fallback to secure random
return
this
.
hsm
?
await
this
.
hsm
.
generateKey
(
)
:
await
crypto
.
subtle
.
generateKey
(
/*...*/
)
;
;
}
}
üîÆ Oracle Security
‚Äã
TEE Verification:
Always verify TEE attestations before accepting proofs
Validate enclave signatures and measurement values
Implement attestation freshness checks
ZKP Auditing:
Audit circuit implementations thoroughly
Verify trusted setup parameters
Test edge cases and malformed inputs
Fallback Mechanisms:
contract OracleManager {
address[] public oracles;
uint256 public minConfirmations = 2;
function verifyWithFallback(bytes calldata proof) external view returns (bool) {
uint256 confirmations = 0;
for (uint i = 0; i < oracles.length; i++) {
if (IOracle(oracles[i]).verifyProof(proof)) {
confirmations++;
}
}
return confirmations >= minConfirmations;
}
}
üõ°Ô∏è Metadata Privacy
‚Äã
Encryption Standards:
Use AES-256-GCM for symmetric encryption
Implement RSA-4096 or ECC-P384 for key sealing
Always include authentication tags
Storage Security:
Encrypt metadata before network transmission
Use 0G Storage for decentralized, tamper-proof storage
Implement zero-knowledge access controls
Access Patterns:
// Secure metadata access pattern
async
function
accessMetadata
(
tokenId
,
requesterKey
)
{
// 1. Verify ownership or authorization
const
isAuthorized
=
await
verifyAccess
(
tokenId
,
requesterKey
)
;
if
(
!
isAuthorized
)
throw
new
Error
(
'Unauthorized'
)
;
// 2. Retrieve encrypted metadata
const
encrypted
=
await
storage
.
retrieve
(
getMetadataURI
(
tokenId
)
)
;
// 3. Decrypt only if authorized
const
decrypted
=
await
decrypt
(
encrypted
,
requesterKey
)
;
return
decrypted
;
}
Advanced Features
‚Äã
Clone Functionality
‚Äã
The
clone()
function allows creating copies of INFTs while maintaining metadata security:
function clone(
address to,
uint256 tokenId,
bytes calldata sealedKey,
bytes calldata proof
) external returns (uint256) {
require(canClone(tokenId, msg.sender), "Not authorized");
uint256 newTokenId = _mint(to);
_copyMetadata(tokenId, newTokenId, sealedKey, proof);
return newTokenId;
}
Authorized Usage
‚Äã
Enable third parties to use INFT capabilities without ownership:
function authorizeUsage(
uint256 tokenId,
address executor,
bytes calldata permissions
) external {
require(ownerOf(tokenId) == msg.sender, "Not owner");
_authorizations[tokenId][executor] = permissions;
emit UsageAuthorized(tokenId, executor);
}
0G Infrastructure Integration
‚Äã
0G Storage Integration
‚Äã
// Store encrypted AI agent metadata
const
metadata
=
{
model
:
aiAgent
.
serializedModel
,
weights
:
aiAgent
.
trainedWeights
,
config
:
aiAgent
.
configuration
}
;
const
encrypted
=
await
encryptMetadata
(
metadata
,
ownerPublicKey
)
;
const
storageResult
=
await
ogStorage
.
store
(
encrypted
,
{
redundancy
:
3
,
durability
:
'99.999%'
}
)
;
console
.
log
(
`
Metadata stored at:
${
storageResult
.
uri
}
`
)
;
0G Compute Integration
‚Äã
// Execute secure inference without exposing model
const
inferenceResult
=
await
ogCompute
.
executeSecure
(
{
tokenId
:
inftId
,
executor
:
authorizedExecutor
,
input
:
userQuery
,
verificationMode
:
'TEE'
// or 'ZKP'
}
)
;
// Result includes proof of correct execution
console
.
log
(
`
Inference result:
${
inferenceResult
.
output
}
`
)
;
console
.
log
(
`
Verification proof:
${
inferenceResult
.
proof
}
`
)
;
0G Chain Deployment
‚Äã
// Deploy INFT contract to 0G Chain
const
ERC7857Factory
=
await
ethers
.
getContractFactory
(
'ERC7857'
)
;
const
inftContract
=
await
ERC7857Factory
.
deploy
(
'AI Agent NFTs'
,
'AINFT'
,
oracleAddress
,
ogStorageAddress
)
;
await
inftContract
.
deployed
(
)
;
console
.
log
(
`
INFT contract deployed at:
${
inftContract
.
address
}
`
)
;
Resources & References
‚Äã
Official Documentation
‚Äã
üìú
EIP-7857 Specification
- Official Ethereum standard proposal
üíª
Reference Implementation
- Complete codebase with examples
üîí
Security Audit Reports
- Third-party security assessments (coming soon)
Community & Support
‚Äã
üí¨
Developer Forum
- Technical discussions and Q&A
üêõ
GitHub Issues
- Bug reports and feature requests
üìö
Knowledge Base
- Common implementation patterns
Standards & Specifications
‚Äã
üìÑ
ERC-721 Standard
- Base NFT standard
üîê
Encryption Standards
- NIST cryptography guidelines
üõ°Ô∏è
TEE Specifications
- Intel SGX documentation
Next Steps
‚Äã
For Implementation
‚Äã
üöÄ
Integration Guide
- Step-by-step development guide
üéØ
Use Cases
- Real-world implementation examples
üìã
Best Practices Guide
- Production deployment guidelines (coming soon)
For Testing
‚Äã
üß™
Testnet Deployment
- Test your implementation
üóóÔ∏è
Oracle Testing
- Verify TEE and ZKP implementations
üîç
Security Testing
- Audit your contracts
Community
‚Äã
üí¨
Join Discussions
- Share implementations and get feedback
üöÄ
Contribute
- Help improve the standard and tooling
üìö
Learn
- Explore advanced features and optimizations


========== [ https://docs.0g.ai/developer-hub/building-on-0g/inft/inft-overview ] ==========

Developer Hub
Building on 0G
INFTs (Intelligent NFTs)
INFTs Overview
On this page
INFTs: Tokenizing AI Agents
What Are INFTs?
‚Äã
The rapid growth of AI agents necessitates new methods for managing their ownership, transfer, and capabilities within Web3 ecosystems.
INFTs (Intelligent Non-Fungible Tokens)
represent a significant advancement in this space, enabling the tokenization of AI agents with:
Transferability
: Move AI agents between owners securely
Decentralized control
: No single point of failure
Full asset ownership
: Complete control over AI capabilities
Royalty potential
: Monetize AI agent usage and transfers
Navigation Guide
This page
: High-level concepts and use cases
ERC-7857 Standard
: Technical implementation details
Integration Guide
: Step-by-step development guide
Why Traditional NFTs Don't Work for AI
‚Äã
Traditional NFT standards like ERC-721 and ERC-1155 have significant limitations when applied to AI agents:
Key Problems
‚Äã
üîì Static and Public Metadata
Existing standards link to static, publicly accessible metadata
AI agents need dynamic metadata that reflects learning and evolution
Sensitive AI data requires privacy protection
üö´ Insecure Metadata Transfer
ERC-721 transfers only move ownership identifiers
The underlying AI "intelligence" doesn't transfer
New owners receive incomplete or non-functional agents
üîí No Native Encryption
Current standards lack built-in encryption support
Proprietary AI models remain exposed
Sensitive user data can't be protected
The INFT Solution: ERC-7857
‚Äã
ERC-7857 is a new NFT standard specifically designed to address AI agent requirements. It enables the creation, ownership, and secure transfer of INFTs with their complete intelligence intact.
Revolutionary Features
‚Äã
üõ°Ô∏è Privacy-Preserving Metadata
Encrypts sensitive AI "intelligence" data
Protects proprietary information from exposure
Maintains privacy throughout transfers
üîÑ Secure Metadata Transfers
Both ownership AND encrypted metadata transfer together
Verifiable transfer process ensures integrity
New owners receive fully functional agents
‚ö° Dynamic Data Management
Supports evolving AI agent capabilities
Secure updates to agent state and behaviors
Maintains functionality within NFT framework
üåê Decentralized Storage Integration
Works with 0G Storage for permanent, tamper-proof storage
Distributed access management
No single point of failure
‚úÖ Verifiable Ownership & Control
Cryptographic proofs validate all transfers
Oracle-based verification ensures integrity
Transparent ownership verification
ü§ñ AI-Specific Functionality
Built-in agent lifecycle management
Pre-execution ownership verification
Specialized features for AI use cases
How INFT Transfers Work
‚Äã
The transfer mechanism ensures both token ownership and encrypted metadata transfer securely together.
Simple Transfer Flow
‚Äã
1. üì¶ Encrypt & Commit    ‚Üí  2. üîÑ Oracle Processing
‚Üì                           ‚Üì
6. ‚úÖ Access Granted     ‚Üê  3. üîê Re-encrypt for Receiver
‚Üë                           ‚Üì
5. ‚úì Verify & Finalize   ‚Üê  4. üóùÔ∏è Secure Key Delivery
Detailed Step-by-Step Process
Encryption & Commitment
AI agent metadata gets encrypted
Hash commitment created as authenticity proof
Content remains hidden
Secure Transfer Initiation
Trusted oracle (using TEEs) decrypts original metadata
Process happens in secure environment
Re-encryption for Receiver
Oracle generates new encryption key
Re-encrypts metadata with new key
Stores new encrypted metadata (e.g., on 0G Storage)
Key Delivery
New encryption key encrypted with receiver's public key
Only intended owner can access metadata key
Verification & Finalization
Smart contract verifies multiple proofs:
Sender's access rights
Oracle validation of metadata matching
Receiver's signed acknowledgment
If valid: ownership transfers + receiver gets encrypted key
Access Granted
Receiver uses private key to decrypt metadata key
Full access to agent's encrypted intelligence granted
Technical Implementation
For detailed oracle implementations (TEE vs ZKP), security considerations, and code examples, see the
ERC-7857 Technical Standard
.
Additional Capabilities
‚Äã
üß¨ Clone Function
Creates new token with same AI metadata
Preserves original while enabling distribution
Useful for AI agent templates
üîê Authorized Usage
Grant usage rights without ownership transfer
Sealed executor processes metadata securely
Enable AI-as-a-Service models
Real-World Applications
‚Äã
Secure AI agent tokenization opens up transformative possibilities:
üè™ AI Agent Marketplaces
‚Äã
Buy and sell trained AI agents with guaranteed capability transfer
Secure marketplaces with verified agent functionality
Transparent pricing and capability verification
üéØ Personalized Automation
‚Äã
Own AI agents tailored for specific tasks:
DeFi trading strategies
Airdrop claiming automation
Social media management
Research and analysis
üè¢ Enterprise AI Solutions
‚Äã
Build proprietary AI agents for internal use
Securely transfer or lease agents to clients
Maintain control over sensitive business logic
üíº AI-as-a-Service (AIaaS)
‚Äã
Tokenize AI agents for subscription models
Granular usage permissions and billing
Scalable service delivery
ü§ù Agent Collaboration
‚Äã
Combine multiple INFT agents for enhanced capabilities
Create composite AI solutions
Build AI agent ecosystems
üí∞ IP Monetization
‚Äã
AI developers monetize models as NFTs
Maintain usage control and royalty collection
Protect proprietary algorithms
Powered by 0G Infrastructure
‚Äã
INFTs leverage the complete 0G ecosystem for optimal performance:
Component
Role in INFTs
Key Benefits
0G Storage
Encrypted metadata storage
Secure, permanent, owner-only access
0G DA
Transfer proof verification
Guaranteed metadata availability
0G Chain
Smart contract execution
Fast, low-cost INFT operations
0G Compute
Secure AI inference
Private agent execution
Why This Matters
‚Äã
By combining INFTs with 0G's comprehensive AI infrastructure, developers can create sophisticated, transferable AI agents that maintain their intelligence, privacy, and functionality throughout their entire lifecycle.
Complete AI Stack
0G provides the only complete infrastructure stack specifically designed for AI applications, making it the ideal foundation for INFT development.
Next Steps
‚Äã
For Developers
‚Äã
üöÄ
Integration Guide
- Start building with INFTs
üìã
ERC-7857 Standard
- Technical implementation details
üíª
GitHub Repository
- Sample code and examples
For Users
‚Äã
üõí
AI Agent Marketplace
- Browse available AI agents (coming soon)
üìö
User Guide
- How to buy, transfer, and use INFTs (coming soon)
Get Support
‚Äã
üí¨
Discord Community
- Ask questions and get help
üìñ
Documentation Hub
- Complete 0G ecosystem guides
Web3 Compatible
ERC-7857 is designed to be compatible with existing Web3 infrastructure while providing enhanced security and functionality for AI agent tokenization.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/inft/integration ] ==========

Developer Hub
Building on 0G
INFTs (Intelligent NFTs)
INFT Integration Guide
On this page
INFT Integration Guide
Overview
‚Äã
This step-by-step guide shows you how to integrate INFTs into your applications using the 0G ecosystem. You'll learn to deploy contracts, manage metadata, and implement secure transfers.
Quick Navigation
New to INFTs?
Start with
INFT Overview
Need technical details?
See
ERC-7857 Standard
Ready to build?
Continue with this guide
Prerequisites
‚Äã
Knowledge Requirements
‚Äã
‚úÖ
NFT Standards
- Understanding of ERC-721 basics
‚úÖ
Smart Contracts
- Solidity development experience
‚úÖ
Cryptography
- Basic encryption and key management concepts
‚úÖ
0G Ecosystem
- Familiarity with 0G infrastructure components
Technical Setup
‚Äã
‚úÖ
Development Environment
- Node.js 16+, Hardhat/Foundry
‚úÖ
0G Testnet Account
- Wallet with testnet tokens
‚úÖ
API Access
- Keys for 0G Storage and Compute services
Quick Setup Checklist
# Install dependencies
npm install @0glabs/0g-ts-sdk ethers hardhat
# Set environment variables
export PRIVATE_KEY="your-private-key"
export OG_RPC_URL="https://evmrpc-testnet.0g.ai"
export OG_STORAGE_URL="https://storage-testnet.0g.ai"
export OG_COMPUTE_URL="https://compute-testnet.0g.ai"
Understanding 0G Integration
‚Äã
INFTs work seamlessly with 0G's complete AI infrastructure:
Component
Purpose
INFT Integration
0G Storage
Encrypted metadata storage
Stores AI agent data securely
0G DA
Proof verification
Validates transfer integrity
0G Chain
Smart contract execution
Hosts INFT contracts
0G Compute
Secure AI inference
Runs agent computations privately
Why This Architecture Matters
This integration ensures that AI agents maintain their intelligence, privacy, and functionality throughout their entire lifecycle while remaining fully decentralized.
Step-by-Step Implementation
‚Äã
Step 1: Initialize Your Project
‚Äã
# Create new project
mkdir my-inft-project && cd my-inft-project
npm init -y
# Install required dependencies
npm install @0glabs/0g-ts-sdk @openzeppelin/contracts ethers hardhat
npm install --save-dev @nomicfoundation/hardhat-toolbox
# Initialize Hardhat
npx hardhat init
Configure environment:
# Create .env file
cat > .env << EOF
PRIVATE_KEY=your_private_key_here
OG_RPC_URL=https://evmrpc-testnet.0g.ai
OG_STORAGE_URL=https://storage-testnet.0g.ai
OG_COMPUTE_URL=https://compute-testnet.0g.ai
EOF
Step 2: Create INFT Smart Contract
‚Äã
// contracts/INFT.sol
pragma solidity ^0.8.19;
import "@openzeppelin/contracts/token/ERC721/ERC721.sol";
import "@openzeppelin/contracts/access/Ownable.sol";
import "@openzeppelin/contracts/security/ReentrancyGuard.sol";
interface IOracle {
function verifyProof(bytes calldata proof) external view returns (bool);
}
contract INFT is ERC721, Ownable, ReentrancyGuard {
// State variables
mapping(uint256 => bytes32) private _metadataHashes;
mapping(uint256 => string) private _encryptedURIs;
mapping(uint256 => mapping(address => bytes)) private _authorizations;
address public oracle;
uint256 private _nextTokenId = 1;
// Events
event MetadataUpdated(uint256 indexed tokenId, bytes32 newHash);
event UsageAuthorized(uint256 indexed tokenId, address indexed executor);
constructor(
string memory name,
string memory symbol,
address _oracle
) ERC721(name, symbol) {
oracle = _oracle;
}
function mint(
address to,
string calldata encryptedURI,
bytes32 metadataHash
) external onlyOwner returns (uint256) {
uint256 tokenId = _nextTokenId++;
_safeMint(to, tokenId);
_encryptedURIs[tokenId] = encryptedURI;
_metadataHashes[tokenId] = metadataHash;
return tokenId;
}
function transfer(
address from,
address to,
uint256 tokenId,
bytes calldata sealedKey,
bytes calldata proof
) external nonReentrant {
require(ownerOf(tokenId) == from, "Not owner");
require(IOracle(oracle).verifyProof(proof), "Invalid proof");
// Update metadata access for new owner
_updateMetadataAccess(tokenId, to, sealedKey, proof);
// Transfer token ownership
_transfer(from, to, tokenId);
emit MetadataUpdated(tokenId, keccak256(sealedKey));
}
function authorizeUsage(
uint256 tokenId,
address executor,
bytes calldata permissions
) external {
require(ownerOf(tokenId) == msg.sender, "Not owner");
_authorizations[tokenId][executor] = permissions;
emit UsageAuthorized(tokenId, executor);
}
function _updateMetadataAccess(
uint256 tokenId,
address newOwner,
bytes calldata sealedKey,
bytes calldata proof
) internal {
// Extract new metadata hash from proof
bytes32 newHash = bytes32(proof[0:32]);
_metadataHashes[tokenId] = newHash;
// Update encrypted URI if provided in proof
if (proof.length > 64) {
string memory newURI = string(proof[64:]);
_encryptedURIs[tokenId] = newURI;
}
}
function getMetadataHash(uint256 tokenId) external view returns (bytes32) {
return _metadataHashes[tokenId];
}
function getEncryptedURI(uint256 tokenId) external view returns (string memory) {
return _encryptedURIs[tokenId];
}
}
Step 3: Deploy and Initialize Contract
‚Äã
Create deployment script:
// scripts/deploy.js
const
{
ethers
}
=
require
(
"hardhat"
)
;
async
function
main
(
)
{
const
[
deployer
]
=
await
ethers
.
getSigners
(
)
;
console
.
log
(
"Deploying contracts with account:"
,
deployer
.
address
)
;
// Deploy mock oracle for testing (replace with real oracle in production)
const
MockOracle
=
await
ethers
.
getContractFactory
(
"MockOracle"
)
;
const
oracle
=
await
MockOracle
.
deploy
(
)
;
await
oracle
.
deployed
(
)
;
// Deploy INFT contract
const
INFT
=
await
ethers
.
getContractFactory
(
"INFT"
)
;
const
inft
=
await
INFT
.
deploy
(
"AI Agent NFTs"
,
"AINFT"
,
oracle
.
address
)
;
await
inft
.
deployed
(
)
;
console
.
log
(
"Oracle deployed to:"
,
oracle
.
address
)
;
console
.
log
(
"INFT deployed to:"
,
inft
.
address
)
;
}
main
(
)
.
catch
(
(
error
)
=>
{
console
.
error
(
error
)
;
process
.
exitCode
=
1
;
}
)
;
Deploy to 0G testnet:
npx hardhat run scripts/deploy.js --network og-testnet
Step 4: Implement Metadata Management
‚Äã
Create metadata manager:
// lib/MetadataManager.js
const
{
ethers
}
=
require
(
'ethers'
)
;
const
crypto
=
require
(
'crypto'
)
;
class
MetadataManager
{
constructor
(
ogStorage
,
encryptionService
)
{
this
.
storage
=
ogStorage
;
this
.
encryption
=
encryptionService
;
}
async
createAIAgent
(
aiModelData
,
ownerPublicKey
)
{
try
{
// Prepare AI agent metadata
const
metadata
=
{
model
:
aiModelData
.
model
,
weights
:
aiModelData
.
weights
,
config
:
aiModelData
.
config
,
capabilities
:
aiModelData
.
capabilities
,
version
:
'1.0'
,
createdAt
:
Date
.
now
(
)
}
;
// Generate encryption key
const
encryptionKey
=
crypto
.
randomBytes
(
32
)
;
// Encrypt metadata
const
encryptedData
=
await
this
.
encryption
.
encrypt
(
JSON
.
stringify
(
metadata
)
,
encryptionKey
)
;
// Store on 0G Storage
const
storageResult
=
await
this
.
storage
.
store
(
encryptedData
)
;
// Seal key for owner
const
sealedKey
=
await
this
.
encryption
.
sealKey
(
encryptionKey
,
ownerPublicKey
)
;
// Generate metadata hash
const
metadataHash
=
ethers
.
utils
.
keccak256
(
ethers
.
utils
.
toUtf8Bytes
(
JSON
.
stringify
(
metadata
)
)
)
;
return
{
encryptedURI
:
storageResult
.
uri
,
sealedKey
,
metadataHash
}
;
}
catch
(
error
)
{
throw
new
Error
(
`
Failed to create AI agent:
${
error
.
message
}
`
)
;
}
}
async
mintINFT
(
contract
,
recipient
,
aiAgentData
)
{
const
{
encryptedURI
,
sealedKey
,
metadataHash
}
=
aiAgentData
;
const
tx
=
await
contract
.
mint
(
recipient
,
encryptedURI
,
metadataHash
)
;
const
receipt
=
await
tx
.
wait
(
)
;
const
tokenId
=
receipt
.
events
[
0
]
.
args
.
tokenId
;
return
{
tokenId
,
sealedKey
,
transactionHash
:
receipt
.
transactionHash
}
;
}
}
module
.
exports
=
MetadataManager
;
Step 5: Implement Secure Transfers
‚Äã
Transfer preparation:
// lib/TransferManager.js
class
TransferManager
{
constructor
(
oracle
,
metadataManager
)
{
this
.
oracle
=
oracle
;
this
.
metadata
=
metadataManager
;
}
async
prepareTransfer
(
tokenId
,
fromAddress
,
toAddress
,
toPublicKey
)
{
try
{
// Retrieve current metadata
const
currentURI
=
await
this
.
metadata
.
getEncryptedURI
(
tokenId
)
;
const
encryptedData
=
await
this
.
storage
.
retrieve
(
currentURI
)
;
// Request oracle to re-encrypt for new owner
const
transferRequest
=
{
tokenId
,
encryptedData
,
fromAddress
,
toAddress
,
toPublicKey
}
;
// Get oracle proof and new sealed key
const
oracleResponse
=
await
this
.
oracle
.
processTransfer
(
transferRequest
)
;
return
{
sealedKey
:
oracleResponse
.
sealedKey
,
proof
:
oracleResponse
.
proof
,
newEncryptedURI
:
oracleResponse
.
newURI
}
;
}
catch
(
error
)
{
throw
new
Error
(
`
Transfer preparation failed:
${
error
.
message
}
`
)
;
}
}
async
executeTransfer
(
contract
,
transferData
)
{
const
{
from
,
to
,
tokenId
,
sealedKey
,
proof
}
=
transferData
;
const
tx
=
await
contract
.
transfer
(
from
,
to
,
tokenId
,
sealedKey
,
proof
)
;
return
await
tx
.
wait
(
)
;
}
}
Best Practices
‚Äã
üîí Security Guidelines
‚Äã
Key Management:
Store private keys in hardware wallets or HSMs
Never expose keys in code or logs
Implement automatic key rotation
Use multi-signature wallets for critical operations
Metadata Protection:
// Example: Secure metadata handling
class
SecureMetadata
{
constructor
(
)
{
this
.
encryptionAlgorithm
=
'aes-256-gcm'
;
this
.
keyDerivation
=
'pbkdf2'
;
}
async
encryptMetadata
(
data
,
password
)
{
const
salt
=
crypto
.
randomBytes
(
16
)
;
const
key
=
crypto
.
pbkdf2Sync
(
password
,
salt
,
100000
,
32
,
'sha512'
)
;
const
iv
=
crypto
.
randomBytes
(
16
)
;
const
cipher
=
crypto
.
createCipher
(
this
.
encryptionAlgorithm
,
key
,
iv
)
;
// ... encryption logic
}
}
‚ö° Performance Optimization
‚Äã
Efficient Storage Patterns:
Compress metadata before encryption
Use appropriate storage tiers based on access patterns
Implement lazy loading for large AI models
Cache frequently accessed data locally
Batch Operations:
// Batch multiple operations
async
function
batchMintINFTs
(
agents
,
recipients
)
{
const
operations
=
agents
.
map
(
(
agent
,
i
)
=>
metadataManager
.
createAIAgent
(
agent
,
recipients
[
i
]
)
)
;
const
results
=
await
Promise
.
all
(
operations
)
;
return
results
;
}
üß™ Testing Strategy
‚Äã
Comprehensive Test Suite:
// test/INFT.test.js
describe
(
'INFT Contract'
,
function
(
)
{
it
(
'should mint INFT with encrypted metadata'
,
async
function
(
)
{
const
metadata
=
await
createTestMetadata
(
)
;
const
result
=
await
inft
.
mint
(
owner
.
address
,
metadata
.
uri
,
metadata
.
hash
)
;
expect
(
result
)
.
to
.
emit
(
inft
,
'Transfer'
)
;
}
)
;
it
(
'should transfer with re-encryption'
,
async
function
(
)
{
// Test secure transfer logic
}
)
;
it
(
'should authorize usage without ownership transfer'
,
async
function
(
)
{
// Test authorization functionality
}
)
;
}
)
;
Security Testing:
Test with malformed proofs
Verify access controls
Check for reentrancy vulnerabilities
Validate oracle responses
Real-World Use Cases
‚Äã
üè™ AI Agent Marketplace
‚Äã
Complete marketplace integration:
// marketplace/AgentMarketplace.js
class
AgentMarketplace
{
constructor
(
inftContract
,
paymentToken
)
{
this
.
inft
=
inftContract
;
this
.
payment
=
paymentToken
;
this
.
listings
=
new
Map
(
)
;
}
async
listAgent
(
tokenId
,
price
,
description
)
{
// Verify ownership
const
owner
=
await
this
.
inft
.
ownerOf
(
tokenId
)
;
require
(
owner
===
msg
.
sender
,
'Not owner'
)
;
// Create listing
const
listing
=
{
tokenId
,
price
,
description
,
seller
:
owner
,
isActive
:
true
}
;
this
.
listings
.
set
(
tokenId
,
listing
)
;
// Approve marketplace for transfer
await
this
.
inft
.
approve
(
this
.
address
,
tokenId
)
;
return
listing
;
}
async
purchaseAgent
(
tokenId
,
buyerPublicKey
)
{
const
listing
=
this
.
listings
.
get
(
tokenId
)
;
require
(
listing
&&
listing
.
isActive
,
'Agent not for sale'
)
;
// Prepare secure transfer
const
transferData
=
await
this
.
prepareTransfer
(
tokenId
,
listing
.
seller
,
msg
.
sender
,
buyerPublicKey
)
;
// Execute payment
await
this
.
payment
.
transferFrom
(
msg
.
sender
,
listing
.
seller
,
listing
.
price
)
;
// Execute secure transfer
await
this
.
inft
.
transfer
(
listing
.
seller
,
msg
.
sender
,
tokenId
,
transferData
.
sealedKey
,
transferData
.
proof
)
;
// Remove listing
this
.
listings
.
delete
(
tokenId
)
;
}
}
üíº AI-as-a-Service Platform
‚Äã
Usage authorization system:
// services/AIaaS.js
class
AIaaSPlatform
{
async
createSubscription
(
tokenId
,
subscriber
,
duration
,
permissions
)
{
// Verify agent ownership
const
owner
=
await
this
.
inft
.
ownerOf
(
tokenId
)
;
// Create usage authorization
const
authData
=
{
subscriber
,
expiresAt
:
Date
.
now
(
)
+
duration
,
permissions
:
{
maxRequests
:
permissions
.
maxRequests
,
allowedOperations
:
permissions
.
operations
,
rateLimit
:
permissions
.
rateLimit
}
}
;
// Grant usage rights
await
this
.
inft
.
authorizeUsage
(
tokenId
,
subscriber
,
ethers
.
utils
.
toUtf8Bytes
(
JSON
.
stringify
(
authData
)
)
)
;
return
authData
;
}
async
executeAuthorizedInference
(
tokenId
,
input
,
subscriber
)
{
// Verify authorization
const
auth
=
await
this
.
getAuthorization
(
tokenId
,
subscriber
)
;
require
(
auth
&&
auth
.
expiresAt
>
Date
.
now
(
)
,
'Unauthorized'
)
;
// Execute inference on 0G Compute
const
result
=
await
this
.
ogCompute
.
executeSecure
(
{
tokenId
,
executor
:
subscriber
,
input
,
verificationMode
:
'TEE'
}
)
;
// Update usage metrics
await
this
.
updateUsageMetrics
(
tokenId
,
subscriber
)
;
return
result
;
}
}
ü§ù Multi-Agent Collaboration
‚Äã
Agent composition framework:
// collaboration/AgentComposer.js
class
AgentComposer
{
async
composeAgents
(
agentTokenIds
,
compositionRules
)
{
// Verify ownership of all agents
for
(
const
tokenId
of
agentTokenIds
)
{
const
owner
=
await
this
.
inft
.
ownerOf
(
tokenId
)
;
require
(
owner
===
msg
.
sender
,
`
Not owner of agent
${
tokenId
}
`
)
;
}
// Create composite agent metadata
const
compositeMetadata
=
{
type
:
'composite'
,
agents
:
agentTokenIds
,
rules
:
compositionRules
,
createdAt
:
Date
.
now
(
)
}
;
// Encrypt and store composite metadata
const
encryptedComposite
=
await
this
.
metadataManager
.
createAIAgent
(
compositeMetadata
,
msg
.
sender
)
;
// Mint new INFT for composite agent
const
result
=
await
this
.
inft
.
mint
(
msg
.
sender
,
encryptedComposite
.
encryptedURI
,
encryptedComposite
.
metadataHash
)
;
return
result
.
tokenId
;
}
async
executeCompositeInference
(
compositeTokenId
,
input
)
{
// Retrieve composite metadata
const
metadata
=
await
this
.
getDecryptedMetadata
(
compositeTokenId
)
;
// Execute inference on each component agent
const
agentResults
=
await
Promise
.
all
(
metadata
.
agents
.
map
(
agentId
=>
this
.
executeAgentInference
(
agentId
,
input
)
)
)
;
// Apply composition rules to combine results
const
finalResult
=
this
.
applyCompositionRules
(
agentResults
,
metadata
.
rules
)
;
return
finalResult
;
}
}
Troubleshooting
‚Äã
Common Issues & Solutions
‚Äã
Transfer Failures
Problem
: INFT transfer transaction reverts
Causes & Solutions
:
Invalid proof
: Verify oracle is online and proof is correctly formatted
Expired proof
: Generate new proof (proofs have limited validity)
Wrong owner
: Ensure
from
address matches actual token owner
Oracle unavailable
: Check oracle service status
// Debug transfer issues
async
function
debugTransfer
(
tokenId
,
proof
)
{
const
owner
=
await
inft
.
ownerOf
(
tokenId
)
;
console
.
log
(
`
Token owner:
${
owner
}
`
)
;
const
isValidProof
=
await
oracle
.
verifyProof
(
proof
)
;
console
.
log
(
`
Proof valid:
${
isValidProof
}
`
)
;
// Check oracle status
const
oracleStatus
=
await
oracle
.
getStatus
(
)
;
console
.
log
(
`
Oracle status:
${
oracleStatus
}
`
)
;
}
Metadata Access Issues
Problem
: Cannot decrypt or access AI agent metadata
Solutions
:
Verify private key corresponds to sealed key
Check storage URI accessibility
Ensure metadata hasn't been corrupted
Validate encryption algorithm compatibility
// Test metadata access
async
function
testMetadataAccess
(
tokenId
,
privateKey
)
{
try
{
const
encryptedURI
=
await
inft
.
getEncryptedURI
(
tokenId
)
;
const
encryptedData
=
await
storage
.
retrieve
(
encryptedURI
)
;
// Attempt decryption
const
sealedKey
=
await
getSealedKey
(
tokenId
)
;
const
key
=
await
unsealKey
(
sealedKey
,
privateKey
)
;
const
metadata
=
await
decrypt
(
encryptedData
,
key
)
;
console
.
log
(
'Metadata accessible:'
,
!
!
metadata
)
;
return
metadata
;
}
catch
(
error
)
{
console
.
error
(
'Metadata access failed:'
,
error
.
message
)
;
}
}
High Gas Costs
Optimization strategies
:
Compress proofs before submission
Use batch operations for multiple transfers
Optimize storage patterns
Consider Layer 2 solutions
// Optimize gas usage
async
function
optimizedTransfer
(
transfers
)
{
// Batch multiple transfers
const
batchData
=
transfers
.
map
(
t
=>
(
{
tokenId
:
t
.
tokenId
,
from
:
t
.
from
,
to
:
t
.
to
,
sealedKey
:
compressData
(
t
.
sealedKey
)
,
proof
:
compressProof
(
t
.
proof
)
}
)
)
;
return
await
inft
.
batchTransfer
(
batchData
)
;
}
Get Support
‚Äã
üêõ
GitHub Issues
- Report bugs and feature requests
üí¨
Discord Community
- Get help from developers
üìñ
Documentation
- Technical reference
üìö
Knowledge Base
- Common solutions
Next Steps
‚Äã
Continue Learning
‚Äã
üìã
ERC-7857 Technical Standard
- Deep dive into implementation details
üéØ
INFT Use Cases
- Explore more applications
üíª
Example Implementations
- Reference code
Production Deployment
‚Äã
üöÄ
Mainnet Migration
- Deploy to 0G mainnet when ready
üîí
Security Audit
- Get your contracts audited
üìä
Monitoring Setup
- Implement monitoring and alerts
Community
‚Äã
ü§ù
Developer Community
- Share your implementation
üí¨
Technical Discussions
- Join conversations about best practices
üë•
Contribute
- Help improve the INFT ecosystem
Ready to Deploy?
Once you've tested your implementation thoroughly, consider getting a security audit before deploying to mainnet. The 0G team can recommend trusted auditing partners.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/introduction ] ==========

Developer Hub
Building on 0G
On this page
Building on 0G
Build AI-powered applications using 0G's modular infrastructure - with or without migrating your existing code.
Keep your current blockchain and add 0G services as needed.
Our infrastructure works with:
‚úÖ Any EVM chain (Ethereum, Polygon, BNB, Arbitrum)
‚úÖ Non-EVM chains (Solana, Near, Cosmos)
‚úÖ Traditional Web2 applications
Prerequisites
‚Äã
Before building:
Get testnet tokens
from the
faucet
Install relevant SDK
for your language
Review service documentation
for your chosen components
What's Next?
‚Äã
Based on your needs, dive into:
Dapp Migration?
‚Üí
Deploy on 0G Chain
Need Storage?
‚Üí
Storage SDK Guide
Need Compute?
‚Üí
Compute Integration
Building a Rollup?
‚Üí
DA Integration
Creating INFTs?
‚Üí
INFT Overview
üí°
Pro Tip
: Start with one service, prove the value, then expand. Most successful projects begin with 0G Storage or Compute before exploring other services.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/rollup-as-a-service/caldera-on-0g-da ] ==========

Caldera on 0G DA
Under construction...


========== [ https://docs.0g.ai/developer-hub/building-on-0g/rollups-and-appchains/arbitrum-nitro-on-0g-da ] ==========

Developer Hub
Building on 0G
0G DA
Rollups and Appchains
Arbitrum Nitro on 0G DA
On this page
Run an Arbitrum Nitro Rollup on 0G DA
Arbitrum Nitro is a high-performance Ethereum rollup that uses a new consensus mechanism called "Nitro" to achieve scalability and efficiency. 0G DA is a high-performance data availability layer that can be used for Arbitrum Nitro to provide a cost-effective and secure solution for storing transaction data.
Overview
‚Äã
DA Provider Implementation
‚Äã
The Arbitrum Nitro code includes a
DataAvailabilityProvider
interface, which is utilized throughout the codebase for storing and retrieving data from various providers, including EIP-4844 blobs, Anytrust, and now 0G.
This integration adds an implementation of the
DataAvailabilityProvider
interface specifically for 0G. The main functionality for posting and retrieving data is found in the
das/zerogravity.go
file, where the data is stored on 0G.
GitHub Repository
‚Äã
Find the repository for this integration at:
https://github.com/0gfoundation/nitro
Prerequisites
‚Äã
Before setting up your Arbitrum Nitro rollup with 0G DA, ensure you have:
0G DA Client Node
0G DA Encoder Node
0G Arbitrum Nitro Rollup Kit
Beta Integration
This is a beta integration and we are working on resolving open issues. Please check the repository for the latest updates and known issues.
Next Steps
‚Äã
For detailed setup instructions and configuration:
Set up DA infrastructure
‚Üí Follow the
DA integration guide
Clone the integration
‚Üí Visit the
0G Nitro repository
Follow setup guide
‚Üí Check the repository README for specific deployment steps
Support
‚Äã
Technical Issues
‚Üí
GitHub Issues
Community Support
‚Üí
Discord
Stay tuned for more detailed documentation as this integration matures.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/rollups-and-appchains/op-stack-on-0g-da ] ==========

Developer Hub
Building on 0G
0G DA
Rollups and Appchains
OP Stack on 0G DA
On this page
Run an OP Stack Rollup on 0G DA
Optimism is a lightning-fast Ethereum L2 blockchain, built with the OP Stack. 0G DA is a high-performance data availability layer that can be used with Optimism to provide a cost-effective and secure solution for storing transaction data.
Overview
‚Äã
To implement this server specification, 0G DA provides a
da-server
that runs as a sidecar process alongside the OP Stack rollup node. This server connects to a 0G DA client to securely communicate with the 0G DA network.
Required Components
‚Äã
0G DA client node
0G DA encoder node
0G DA Server (deployment guide below)
OP Stack components with configuration adjustments
GitHub Repository
‚Äã
Find the repository for this integration at:
https://github.com/0gfoundation/0g-da-op-plasma
The Optimism codebase has been extended to integrate with the 0G DA
da-server
. This server utilizes the 0G DA Open API to efficiently store and retrieve rollup data.
Deployment Steps
‚Äã
1. Deploy DA Server
‚Äã
Run with Docker
Build from Source
Build the Docker image:
docker build -t 0g-da-op-plasma .
Run the Docker container:
Adjust commands and parameters as required for your setup:
docker run -p 3100:3100 0g-da-op-plasma:latest da-server \
--addr 0.0.0.0 \
--port 3100 \
--zg.server rpc_to_a_da_client  # default: 127.0.0.1:51001
Build DA Server:
git clone https://github.com/0gfoundation/0g-da-op-plasma.git
cd 0g-da-op-plasma
make da-server
Run DA Server:
./bin/da-server \
--addr 127.0.0.1 \
--port 3100 \
--zg.server rpc_to_a_da_client  # default: 127.0.0.1:51001
DA Server Configuration Flags:
Flag
Description
Default
--zg.server
0G DA client server endpoint
localhost:51001
--addr
Server listening address
-
--port
Server listening port
-
2. Deploy DA Client and DA Encoder
‚Äã
For guidance on setting up a 0G DA client and DA Encoder, refer to the
DA integration documentation
.
3. Deploy OP Stack
‚Äã
Prerequisites
‚Äã
Ensure you have installed the following software:
Software
Version
Git
OS default
Go
1.21.6
Node
^20
just
1.34.0
Make
OS default
jq
OS default
direnv
Latest
Required releases:
op-node/v1.9.1
op-proposer/v1.9.1
op-batcher/v1.9.1
op-geth v1.101408.0
Build the Optimism Monorepo
‚Äã
1. Clone and navigate to the Optimism Monorepo:
git clone https://github.com/ethereum-optimism/optimism.git
cd optimism
git fetch --tag --all
git checkout v1.9.1
git submodule update --init --recursive
2. Check your dependencies:
./packages/contracts-bedrock/scripts/getting-started/versions.sh
3. Compile the necessary packages:
make op-node op-batcher op-proposer
make build
Build the Optimism Geth Source
‚Äã
1. Clone and navigate to op-geth:
git clone https://github.com/ethereum-optimism/op-geth.git
cd op-geth
git fetch --tag --all
git checkout v1.101408.0
2. Compile op-geth:
make geth
Get Access to a Sepolia Node
‚Äã
For deploying to Sepolia, access an L1 node using a provider like
Alchemy
(easier) or run your own Sepolia node (harder).
Configure Environment Variables
‚Äã
1. Enter the Optimism Monorepo:
cd ~/optimism
2. Duplicate the sample environment variable file:
cp .envrc.example .envrc
3. Fill out the environment variables:
Variable Name
Description
L1_RPC_URL
URL for your L1 node (a Sepolia node in this case)
L1_RPC_KIND
Kind of L1 RPC you're connecting to (
alchemy
,
quicknode
,
infura
,
parity
,
nethermind
,
debug_geth
,
erigon
,
basic
,
any
)
Generate Addresses
‚Äã
You'll need four addresses and their private keys:
Admin
: Has the ability to upgrade contracts
Batcher
: Publishes Sequencer transaction data to L1
Proposer
: Publishes L2 transaction results (state roots) to L1
Sequencer
: Signs blocks on the p2p network
1. Navigate to the contracts-bedrock package:
cd ~/optimism/packages/contracts-bedrock
2. Generate accounts:
./scripts/getting-started/wallets.sh
you will get the following output:
Copy the following into your .envrc file:
# Admin address
export GS_ADMIN_ADDRESS=0x9625B9aF7C42b4Ab7f2C437dbc4ee749d52E19FC
export GS_ADMIN_PRIVATE_KEY=0xbb93a75f64c57c6f464fd259ea37c2d4694110df57b2e293db8226a502b30a34
# Batcher address
export GS_BATCHER_ADDRESS=0xa1AEF4C07AB21E39c37F05466b872094edcf9cB1
export GS_BATCHER_PRIVATE_KEY=0xe4d9cd91a3e53853b7ea0dad275efdb5173666720b1100866fb2d89757ca9c5a
# Proposer address
export GS_PROPOSER_ADDRESS=0x40E805e252D0Ee3D587b68736544dEfB419F351b
export GS_PROPOSER_PRIVATE_KEY=0x2d1f265683ebe37d960c67df03a378f79a7859038c6d634a61e40776d561f8a2
# Sequencer address
export GS_SEQUENCER_ADDRESS=0xC06566E8Ec6cF81B4B26376880dB620d83d50Dfb
export GS_SEQUENCER_PRIVATE_KEY=0x2a0290473f3838dbd083a5e17783e3cc33c905539c0121f9c76614dda8a38dca
3. Save the addresses:
Copy the output from the above command and paste it into your
.envrc
file. Fund the addresses with Sepolia ETH:
Admin: 0.5 ETH
Proposer: 0.2 ETH
Batcher: 0.1 ETH
Production Note
Use secure hardware for key management in production environments. cast wallet is not designed for production deployments.
Load Environment Variables
‚Äã
1. Enter the Optimism Monorepo:
cd ~/optimism
2. Load the variables with direnv:
direnv allow
Deploy Core Contracts
‚Äã
1. Update the deployment configuration:
cd packages/contracts-bedrock
./scripts/getting-started/config.sh
2. Add 0G DA configuration:
Add the following at the bottom of
getting_started.json
:
{
"useAltDA"
:
true
,
"daCommitmentType"
:
"GenericCommitment"
,
"daChallengeWindow"
:
160
,
"daResolveWindow"
:
160
,
"daBondSize"
:
1000000
,
"daResolverRefundPercentage"
:
0
}
3. Deploy contracts (this can take up to 15 minutes):
DEPLOYMENT_OUTFILE=deployments/artifact.json \
DEPLOY_CONFIG_PATH=deploy-config/getting-started.json \
forge script scripts/deploy/Deploy.s.sol:Deploy \
--broadcast --private-key $GS_ADMIN_PRIVATE_KEY \
--rpc-url $L1_RPC_URL --slow
4. Generate L2 allocations:
CONTRACT_ADDRESSES_PATH=deployments/artifact.json \
DEPLOY_CONFIG_PATH=deploy-config/getting-started.json \
STATE_DUMP_PATH=deploy-config/statedump.json \
forge script scripts/L2Genesis.s.sol:L2Genesis \
--sig 'runWithStateDump()' --chain <YOUR_L2_CHAINID>
Set Up L2 Configuration
‚Äã
1. Navigate to the op-node directory:
cd ~/optimism/op-node
2. Generate genesis and rollup configuration:
go run cmd/main.go genesis l2 \
--deploy-config ../packages/contracts-bedrock/deploy-config/getting-started.json \
--l1-deployments ../packages/contracts-bedrock/deployments/artifact.json \
--outfile.l2 genesis.json \
--outfile.rollup rollup.json \
--l1-rpc $L1_RPC_URL \
--l2-allocs ../packages/contracts-bedrock/deploy-config/statedump.json
3. Add alt_da configuration to rollup.json:
{
"alt_da"
:
{
"da_challenge_contract_address"
:
"0x0000000000000000000000000000000000000000"
,
"da_commitment_type"
:
"GenericCommitment"
,
"da_challenge_window"
:
160
,
"da_resolve_window"
:
160
}
}
4. Generate JWT secret:
openssl rand -hex 32 > jwt.txt
5. Copy files to op-geth directory:
cp genesis.json ~/op-geth
cp jwt.txt ~/op-geth
Initialize and Run Components
‚Äã
Initialize op-geth
‚Äã
cd ~/op-geth
mkdir datadir
build/bin/geth init --datadir=datadir genesis.json
Run op-geth
‚Äã
cd ~/op-geth
./build/bin/geth \
--datadir ./datadir \
--http \
--http.corsdomain="*" \
--http.vhosts="*" \
--http.addr=0.0.0.0 \
--http.port=9545 \
--http.api=web3,debug,eth,txpool,net,engine \
--ws \
--ws.addr=0.0.0.0 \
--ws.port=9546 \
--ws.origins="*" \
--ws.api=debug,eth,txpool,net,engine \
--syncmode=full \
--nodiscover \
--maxpeers=0 \
--networkid=42069 \
--authrpc.vhosts="*" \
--authrpc.addr=0.0.0.0 \
--authrpc.port=9551 \
--authrpc.jwtsecret=./jwt.txt \
--rollup.disabletxpoolgossip=true \
--state.scheme=hash
Run op-node
‚Äã
cd ~/optimism/op-node
./bin/op-node \
--l2=http://localhost:9551 \
--l2.jwt-secret=./jwt.txt \
--sequencer.enabled \
--sequencer.l1-confs=5 \
--verifier.l1-confs=4 \
--rollup.config=./rollup.json \
--rpc.addr=0.0.0.0 \
--rpc.port=8547 \
--p2p.disable \
--rpc.enable-admin \
--p2p.sequencer.key=$GS_SEQUENCER_PRIVATE_KEY \
--l1=$L1_RPC_URL \
--l1.rpckind=$L1_RPC_KIND \
--altda.enabled=true \
--altda.da-server=<DA_SERVER_HTTP_URL> \
--altda.da-service=true \
--l1.beacon.ignore=true
Run op-batcher
‚Äã
cd ~/optimism/op-batcher
./bin/op-batcher \
--l2-eth-rpc=http://localhost:9545 \
--rollup-rpc=http://localhost:8547 \
--poll-interval=1s \
--sub-safety-margin=6 \
--num-confirmations=1 \
--safe-abort-nonce-too-low-count=3 \
--resubmission-timeout=30s \
--rpc.addr=0.0.0.0 \
--rpc.port=8548 \
--rpc.enable-admin \
--max-channel-duration=1 \
--l1-eth-rpc=$L1_RPC_URL \
--private-key=$GS_BATCHER_PRIVATE_KEY \
--altda.enabled=true \
--altda.da-service=true \
--altda.da-server=<DA_SERVER_HTTP_URL>
Controlling Batcher Costs
The
--max-channel-duration=n
setting controls how often data is written to L1. Lower values mean faster synchronization but higher costs. Set to 0 to disable or increase for lower costs.
Run op-proposer
‚Äã
cd ~/optimism/op-proposer
./bin/op-proposer \
--poll-interval=12s \
--rpc.port=9560 \
--rollup-rpc=http://localhost:8547 \
--l2oo-address=$L2OO_ADDR \
--private-key=$GS_PROPOSER_PRIVATE_KEY \
--l1-eth-rpc=$L1_RPC_URL
Acquire Sepolia ETH for Layer 2
‚Äã
1. Navigate to contracts-bedrock:
cd ~/optimism/packages/contracts-bedrock
2. Find the L1 standard bridge contract address:
cat deployments/artifact.json | jq -r .L1StandardBridgeProxy
3. Send Sepolia ETH to the bridge contract address
Test Your Rollup
‚Äã
You now have a fully operational 0G DA-powered Optimism-based EVM Rollup. Experiment with it as you would with any other test blockchain.
Notes
This is a beta integration with active development ongoing
Ensure all necessary ports are open in your firewall configuration
Refer to the
Optimism documentation
for additional configuration options and troubleshooting
Congratulations on setting up your OP Stack rollup with 0G DA!


========== [ https://docs.0g.ai/developer-hub/building-on-0g/storage/sdk ] ==========

Developer Hub
Building on 0G
0G Storage
Storage SDK
On this page
0G Storage SDKs
Build decentralized storage into your applications with our powerful SDKs designed for modern development workflows.
Available SDKs
‚Äã
Go SDK
: Ideal for backend systems and applications built with Go
TypeScript SDK
: Perfect for frontend development and JavaScript-based projects
Core Features
‚Äã
Both SDKs provide a streamlined interface to interact with the 0G Storage network:
Upload and Download Files
: Securely store and retrieve data of various sizes and formats
Manage Data
: List uploaded files, check their status, and control access permissions
Leverage Decentralization
: Benefit from the 0G network's distributed architecture for enhanced data availability, immutability, and censorship resistance
Quick Start Resources
‚Äã
Starter Kits Available
Get up and running quickly with our comprehensive starter kits:
TypeScript Starter Kit
- Complete examples with Express.js server and CLI tool
Go Starter Kit
- Ready-to-use examples with Gin server and CLI tool
Both repositories include working examples, API documentation, and everything you need to start building.
Go SDK
TypeScript SDK
Installation
‚Äã
Install the 0G Storage Client library:
go get github.com/0gfoundation/0g-storage-client
Setup
‚Äã
Import Required Packages
‚Äã
import
(
"context"
"github.com/0gfoundation/0g-storage-client/common/blockchain"
"github.com/0gfoundation/0g-storage-client/common"
"github.com/0gfoundation/0g-storage-client/indexer"
"github.com/0gfoundation/0g-storage-client/transfer"
"github.com/0gfoundation/0g-storage-client/core"
)
Initialize Clients
‚Äã
Create the necessary clients to interact with the network:
// Create Web3 client for blockchain interactions
w3client
:=
blockchain
.
MustNewWeb3
(
evmRpc
,
privateKey
)
defer
w3client
.
Close
(
)
// Create indexer client for node management
indexerClient
,
err
:=
indexer
.
NewClient
(
indRpc
,
indexer
.
IndexerClientOption
{
LogOption
:
common
.
LogOption
{
}
,
}
)
if
err
!=
nil
{
// Handle error
}
Parameters:
evmRpc
is the chain RPC endpoint,
privateKey
is your signer key, and
indRpc
is the indexer RPC endpoint. Use the current values published in the network overview docs for your network.
Core Operations
‚Äã
Node Selection
‚Äã
Select storage nodes before performing file operations:
nodes
,
err
:=
indexerClient
.
SelectNodes
(
ctx
,
expectedReplicas
,
droppedNodes
,
method
,
fullTrusted
)
if
err
!=
nil
{
// Handle error
}
Parameters:
ctx
is the context for the operation.
expectedReplicas
is the number of replicas to maintain.
droppedNodes
is a list of nodes to exclude,
method
can be
min
,
max
,
random
, or a positive number string, and
fullTrusted
limits selection to trusted nodes.
File Upload
‚Äã
Upload files to the network with the indexer client:
file
,
err
:=
core
.
Open
(
filePath
)
if
err
!=
nil
{
// Handle error
}
defer
file
.
Close
(
)
fragmentSize
:=
int64
(
4
*
1024
*
1024
*
1024
)
opt
:=
transfer
.
UploadOption
{
ExpectedReplica
:
1
,
TaskSize
:
10
,
SkipTx
:
true
,
FinalityRequired
:
transfer
.
TransactionPacked
,
FastMode
:
true
,
Method
:
"min"
,
FullTrusted
:
true
,
}
txHashes
,
roots
,
err
:=
indexerClient
.
SplitableUpload
(
ctx
,
w3client
,
file
,
fragmentSize
,
opt
)
if
err
!=
nil
{
// Handle error
}
fragmentSize
controls the split size for large files. The returned
roots
contain the merkle root(s) to download later.
File Hash Calculation
‚Äã
Calculate a file's Merkle root hash for identification:
rootHash
,
err
:=
core
.
MerkleRoot
(
filePath
)
if
err
!=
nil
{
// Handle error
}
fmt
.
Printf
(
"File hash: %s\n"
,
rootHash
.
String
(
)
)
Important
Save the root hash - you'll need it to download the file later!
File Download
‚Äã
Download files from the network:
rootHex
:=
rootHash
.
String
(
)
err
=
indexerClient
.
Download
(
ctx
,
rootHex
,
outputPath
,
withProof
)
if
err
!=
nil
{
// Handle error
}
withProof
enables merkle proof verification during download.
Best Practices
‚Äã
Error Handling
: Implement proper error handling and cleanup
Context Management
: Use contexts for operation timeouts and cancellation
Resource Cleanup
: Always close clients when done using
defer client.Close()
Verification
: Enable proof verification for sensitive files
Monitoring
: Track transaction status for important uploads
Additional Resources
‚Äã
Go SDK Repository
Go Starter Kit
Installation
‚Äã
Install the SDK and its peer dependency:
npm install @0glabs/0g-ts-sdk ethers
note
ethers
is a required peer dependency for blockchain interactions
Setup
‚Äã
Import Required Modules
‚Äã
import
{
ZgFile
,
Indexer
,
Batcher
,
KvClient
}
from
'@0glabs/0g-ts-sdk'
;
import
{
ethers
}
from
'ethers'
;
Initialize Configuration
‚Äã
// Network Constants - Choose your network
// Use the current endpoints from the network overview docs
const
RPC_URL
=
'<blockchain_rpc_endpoint>'
;
const
INDEXER_RPC
=
'<storage_indexer_endpoint>'
;
// Initialize provider and signer
const
privateKey
=
'YOUR_PRIVATE_KEY'
;
// Replace with your private key
const
provider
=
new
ethers
.
JsonRpcProvider
(
RPC_URL
)
;
const
signer
=
new
ethers
.
Wallet
(
privateKey
,
provider
)
;
// Initialize indexer
const
indexer
=
new
Indexer
(
INDEXER_RPC
)
;
Core Operations
‚Äã
File Upload
‚Äã
Complete upload workflow:
async
function
uploadFile
(
filePath
)
{
// Create file object from file path
const
file
=
await
ZgFile
.
fromFilePath
(
filePath
)
;
// Generate Merkle tree for verification
const
[
tree
,
treeErr
]
=
await
file
.
merkleTree
(
)
;
if
(
treeErr
!==
null
)
{
throw
new
Error
(
`
Error generating Merkle tree:
${
treeErr
}
`
)
;
}
// Get root hash for future reference
console
.
log
(
"File Root Hash:"
,
tree
?.
rootHash
(
)
)
;
// Upload to network
const
[
tx
,
uploadErr
]
=
await
indexer
.
upload
(
file
,
RPC_URL
,
signer
)
;
if
(
uploadErr
!==
null
)
{
throw
new
Error
(
`
Upload error:
${
uploadErr
}
`
)
;
}
console
.
log
(
"Upload successful! Transaction:"
,
tx
)
;
// Always close the file when done
await
file
.
close
(
)
;
return
{
rootHash
:
tree
?.
rootHash
(
)
,
txHash
:
tx
}
;
}
File Download
‚Äã
Download with optional verification:
async
function
downloadFile
(
rootHash
,
outputPath
)
{
// withProof = true enables Merkle proof verification
const
err
=
await
indexer
.
download
(
rootHash
,
outputPath
,
true
)
;
if
(
err
!==
null
)
{
throw
new
Error
(
`
Download error:
${
err
}
`
)
;
}
console
.
log
(
"Download successful!"
)
;
}
Key-Value Storage
‚Äã
Store and retrieve key-value data:
// Upload data to 0G-KV
async
function
uploadToKV
(
streamId
,
key
,
value
)
{
const
[
nodes
,
err
]
=
await
indexer
.
selectNodes
(
1
)
;
if
(
err
!==
null
)
{
throw
new
Error
(
`
Error selecting nodes:
${
err
}
`
)
;
}
const
batcher
=
new
Batcher
(
1
,
nodes
,
flowContract
,
RPC_URL
)
;
const
keyBytes
=
Uint8Array
.
from
(
Buffer
.
from
(
key
,
'utf-8'
)
)
;
const
valueBytes
=
Uint8Array
.
from
(
Buffer
.
from
(
value
,
'utf-8'
)
)
;
batcher
.
streamDataBuilder
.
set
(
streamId
,
keyBytes
,
valueBytes
)
;
const
[
tx
,
batchErr
]
=
await
batcher
.
exec
(
)
;
if
(
batchErr
!==
null
)
{
throw
new
Error
(
`
Batch execution error:
${
batchErr
}
`
)
;
}
console
.
log
(
"KV upload successful! TX:"
,
tx
)
;
}
// Download data from 0G-KV
async
function
downloadFromKV
(
streamId
,
key
)
{
const
kvClient
=
new
KvClient
(
"http://3.101.147.150:6789"
)
;
const
keyBytes
=
Uint8Array
.
from
(
Buffer
.
from
(
key
,
'utf-8'
)
)
;
const
value
=
await
kvClient
.
getValue
(
streamId
,
ethers
.
encodeBase64
(
keyBytes
)
)
;
return
value
;
}
Browser Support
‚Äã
For browser environments, use the ESM build:
<
script
type
=
"
module
"
>
import
{
Blob
,
Indexer
}
from
"./dist/zgstorage.esm.js"
;
// Create file object from blob
const
file
=
new
Blob
(
blob
)
;
const
[
tree
,
err
]
=
await
file
.
merkleTree
(
)
;
if
(
err
===
null
)
{
console
.
log
(
"File Root Hash:"
,
tree
.
rootHash
(
)
)
;
}
</
script
>
Stream Support
‚Äã
Work with streams for efficient data handling:
import
{
Readable
}
from
'stream'
;
// Upload from stream
async
function
uploadStream
(
)
{
const
stream
=
new
Readable
(
)
;
stream
.
push
(
'Hello, 0G Storage!'
)
;
stream
.
push
(
null
)
;
const
file
=
await
ZgFile
.
fromStream
(
stream
,
'hello.txt'
)
;
const
[
tx
,
err
]
=
await
indexer
.
upload
(
file
,
RPC_URL
,
signer
)
;
if
(
err
===
null
)
{
console
.
log
(
"Stream uploaded!"
)
;
}
}
// Download as stream
async
function
downloadStream
(
rootHash
)
{
const
stream
=
await
indexer
.
downloadFileAsStream
(
rootHash
)
;
stream
.
pipe
(
fs
.
createWriteStream
(
'output.txt'
)
)
;
}
Best Practices
‚Äã
Initialize Once
: Create the indexer once and reuse it for multiple operations
Handle Errors
: Always implement proper error handling for network issues
Use Appropriate Methods
: Use
ZgFile.fromFilePath
for Node.js and
Blob
for browsers
Secure Keys
: Never expose private keys in client-side code
Close Resources
: Remember to call
file.close()
after operations
Additional Resources
‚Äã
TypeScript SDK Repository
TypeScript Starter Kit
Need more control? Consider running your own
storage node
to participate in the network and earn rewards.


========== [ https://docs.0g.ai/developer-hub/building-on-0g/storage/storage-cli ] ==========

Developer Hub
Building on 0G
0G Storage
Storage CLI
On this page
0G Storage CLI
The 0G Storage CLI is your command-line gateway to interact directly with the 0G Storage network. It simplifies the process of uploading and downloading files while providing full control over your decentralized storage operations.
Why Use the CLI?
‚Äã
Direct Control
: Manage data location and versioning with precision
Automation Ready
: Build scripts and cron jobs for regular operations
Full Feature Access
: Access all storage and KV operations from the terminal
Developer Friendly
: Perfect for integrating into your development workflow
Web-Based Alternative
For a quick and easy web interface, try the
0G Storage Web Tool
- perfect for one-off uploads and downloads.
Installation
‚Äã
Prerequisites
‚Äã
Go 1.18 or higher installed on your system
Git for cloning the repository
Setup Steps
‚Äã
1. Clone the Repository
git clone https://github.com/0gfoundation/0g-storage-client.git
cd 0g-storage-client
2. Build the Binary
go build
3. Add to PATH
(Optional but recommended)
# Move binary to Go bin directory
mv 0g-storage-client ~/go/bin
# Add to PATH if not already configured
export PATH=~/go/bin:$PATH
Command Overview
‚Äã
The CLI provides a comprehensive set of commands for storage operations:
0g-storage-client [command] [flags]
Available Commands:
upload      Upload file to 0G Storage network
download    Download file from 0G Storage network
upload-dir  Upload directory to 0G Storage network
download-dir Download directory from 0G Storage network
diff-dir    Diff directory from 0G Storage network
gen         Generate test files
kv-write    Write to KV streams
kv-read     Read KV streams
gateway     Start gateway service
indexer     Start indexer service
deploy      Deploy storage contracts
completion  Generate shell completion scripts
help        Get help for any command
Global Flags:
--gas-limit uint                Custom gas limit to send transaction
--gas-price uint                Custom gas price to send transaction
--log-level string              Log level (default "info")
--log-color-disabled            Force to disable colorful logs
--rpc-retry-count int           Retry count for rpc request (default 5)
--rpc-retry-interval duration   Retry interval for rpc request (default 5s)
--rpc-timeout duration          Timeout for single rpc request (default 30s)
--web3-log-enabled              Enable Web3 RPC logging
Core Operations
‚Äã
File Upload
‚Äã
Upload files to the 0G Storage network using the indexer service or explicit nodes:
0g-storage-client upload \
--url <blockchain_rpc_endpoint> \
--key <private_key> \
--indexer <storage_indexer_endpoint> \
--file <file_path>
Parameters:
--url
is the chain RPC endpoint,
--key
is your private key, and
--file
is the path to the file you want to upload. Use exactly one of
--indexer
or
--node
.
Common flags include
--tags
,
--submitter
,
--expected-replica
,
--skip-tx
,
--finality-required
,
--task-size
,
--fast-mode
,
--fragment-size
,
--routines
,
--fee
,
--nonce
,
--max-gas-price
,
--n-retries
,
--step
,
--method
,
--full-trusted
,
--timeout
,
--flow-address
, and
--market-address
.
Fee notes (turbo):
unitPrice = 11 / pricePerToken / 1024 * 256
. If
pricePerToken = 1
, then
unitPrice = 2.75
(tokens), or
2.75e18
a0gi.
pricePerSector(256B)/month = lifetimeMonth * unitPrice * 1e18 / 1024 / 1024 / 1024
(no
/12
since $11 is per TB per month).
File Download
‚Äã
Download files from the network using the indexer or explicit nodes:
0g-storage-client download \
--indexer <storage_indexer_endpoint> \
--root <file_root_hash> \
--file <output_file_path>
Parameters:
--file
is the output path. Use exactly one of
--indexer
or
--node
. Use exactly one of
--root
or
--roots
.
Download with Verification
‚Äã
Enable proof verification for enhanced security:
0g-storage-client download \
--indexer <storage_indexer_endpoint> \
--root <file_root_hash> \
--file <output_file_path> \
--proof
The
--proof
flag requests cryptographic proof of data integrity from the storage node.
Directory Upload
‚Äã
Upload an entire directory using explicit storage nodes:
0g-storage-client upload-dir \
--url <blockchain_rpc_endpoint> \
--key <private_key> \
--node <storage_node_endpoint> \
--file <directory_path>
Directory Download
‚Äã
Download a directory by root:
0g-storage-client download-dir \
--indexer <storage_indexer_endpoint> \
--root <directory_root_hash> \
--file <output_directory>
Directory Diff
‚Äã
Compare a local directory with the on-chain version:
0g-storage-client diff-dir \
--indexer <storage_indexer_endpoint> \
--root <directory_root_hash> \
--file <local_directory>
Practical Examples
‚Äã
Upload Example
‚Äã
# Upload a document to 0G Storage
0g-storage-client upload \
--url <blockchain_rpc_endpoint> \
--key YOUR_PRIVATE_KEY \
--indexer <storage_indexer_endpoint> \
--file ./documents/report.pdf
# Output:
# ‚úì File uploaded successfully
# Root hash: 0xc5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470
# Transaction: 0x742d35cc6634c0532925a3b844bc454e8e4a0e3f...
Download Example
‚Äã
# Download file using root hash
0g-storage-client download \
--indexer <storage_indexer_endpoint> \
--root 0xc5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470 \
--file ./downloads/report.pdf
# With verification
0g-storage-client download \
--indexer <storage_indexer_endpoint> \
--root 0xc5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a470 \
--file ./downloads/report.pdf \
--proof
Key-Value Operations
‚Äã
Write to KV Store (Batch Operations)
‚Äã
Write multiple key-value pairs in a single operation:
0g-storage-client kv-write \
--url <blockchain_rpc_endpoint> \
--key <private_key> \
--indexer <storage_indexer_endpoint> \
--stream-id <stream_id> \
--stream-keys <comma_separated_keys> \
--stream-values <comma_separated_values>
Important:
--stream-keys
and
--stream-values
are comma-separated string lists and their length must be equal.
You can use
--indexer
for node selection or pass storage nodes directly with
--node
. If
--indexer
is omitted,
--node
is required.
Example:
0g-storage-client kv-write \
--url <blockchain_rpc_endpoint> \
--key YOUR_PRIVATE_KEY \
--indexer <storage_indexer_endpoint> \
--stream-id 1 \
--stream-keys "key1,key2,key3" \
--stream-values "value1,value2,value3"
Read from KV Store
‚Äã
0g-storage-client kv-read \
--node <kv_node_rpc_endpoint> \
--stream-id <stream_id> \
--stream-keys <comma_separated_keys>
KV Read Endpoint
Note that for KV read operations, you need to specify
--node
as the URL of a KV node, not the indexer endpoint.
RESTful API Gateway
‚Äã
The indexer service provides a RESTful API gateway for easy HTTP-based file access:
File Downloads via HTTP
‚Äã
By Transaction Sequence Number:
GET /file?txSeq=7
By File Merkle Root:
GET /file?root=0x0376e0d95e483b62d5100968ed17fe1b1d84f0bc5d9bda8000cdfd3f39a59927
With Custom Filename:
GET /file?txSeq=7&name=foo.log
Folder Support
‚Äã
Download specific files from within structured folders:
By Transaction Sequence:
GET /file/{txSeq}/path/to/file
By Merkle Root:
GET /file/{merkleRoot}/path/to/file
Advanced Features
‚Äã
Custom Gas Settings
‚Äã
Control transaction costs with custom gas parameters:
0g-storage-client upload \
--gas-limit 3000000 \
--gas-price 10000000000 \
# ... other parameters
RPC Configuration
‚Äã
Configure RPC retry behavior and timeouts:
0g-storage-client upload \
--rpc-retry-count 10 \
--rpc-retry-interval 3s \
--rpc-timeout 60s \
# ... other parameters
Logging Configuration
‚Äã
Adjust logging for debugging:
# Verbose logging with Web3 details
0g-storage-client upload \
--log-level debug \
--web3-log-enabled \
# ... other parameters
# Minimal logging
0g-storage-client download \
--log-level error \
--log-color-disabled \
# ... other parameters
Shell Completion
‚Äã
Enable tab completion for easier command entry:
# Bash
0g-storage-client completion bash > /etc/bash_completion.d/0g-storage-client
# Zsh
0g-storage-client completion zsh > "${fpath[1]}/_0g-storage-client"
# Fish
0g-storage-client completion fish > ~/.config/fish/completions/0g-storage-client.fish
Indexer Service
‚Äã
The indexer service provides two types of storage node discovery:
Trusted Nodes
‚Äã
Well-maintained nodes that provide stable and reliable service.
Discovered Nodes
‚Äã
Nodes discovered automatically through the P2P network.
The indexer intelligently routes data to appropriate storage nodes based on their shard configurations, eliminating the need to manually specify storage nodes or contract addresses.
Important Considerations
‚Äã
Network Configuration
‚Äã
Required Information
RPC endpoints
and
indexer endpoints
are published in the network overview docs. Use the current values for your network. Keep private keys secure and never share them.
File Management
‚Äã
Root Hash Storage
: Save file root hashes after upload - they're required for downloads
Transaction Monitoring
: Track upload transactions on the blockchain explorer
Indexer Benefits
: The indexer automatically selects optimal storage nodes for better reliability
Running Services
‚Äã
Indexer Service
‚Äã
The indexer helps users find suitable storage nodes:
0g-storage-client indexer \
--endpoint :12345 \
--node <storage_node_endpoint>
Or start with a trusted node list:
0g-storage-client indexer \
--endpoint :12345 \
--trusted <node1,node2>
Gateway Service
‚Äã
Run a gateway to provide HTTP access to storage:
0g-storage-client gateway \
--nodes <storage_node_endpoint>
Optionally specify a local file repo:
0g-storage-client gateway \
--nodes <storage_node_endpoint> \
--repo <local_path>
Automation Examples
‚Äã
Backup Script
‚Äã
Create automated backup scripts:
#!/bin/bash
# backup.sh - Daily backup to 0G Storage
DATE=$(date +%Y%m%d)
BACKUP_FILE="/backups/daily-${DATE}.tar.gz"
# Create backup
tar -czf $BACKUP_FILE /important/data
# Upload to 0G
ROOT_HASH=$(0g-storage-client upload \
--url $RPC_URL \
--key $PRIVATE_KEY \
--indexer $INDEXER_URL \
--file $BACKUP_FILE | grep "root =" | awk '{print $NF}')
# Save root hash
echo "${DATE}: ${ROOT_HASH}" >> /backups/manifest.txt
Monitoring Integration
‚Äã
Monitor uploads with logging:
# upload-with-monitoring.sh
0g-storage-client upload \
--file $1 \
--log-level info \
# ... other parameters \
2>&1 | tee -a /var/log/0g-uploads.log
Troubleshooting
‚Äã
Upload fails with "insufficient funds" error
Ensure your wallet has enough tokens for:
Gas fees on 0G Chain
Storage fees for the file size
Check balance: Use a blockchain explorer or wallet to verify funds.
"Indexer not found" error during upload/download
This can happen if:
The indexer service is offline
The indexer endpoint URL is incorrect
Network connectivity issues
Verify the indexer endpoint and try again.
RPC timeout errors
If you experience RPC timeouts, try adjusting the timeout settings:
--rpc-timeout 60s --rpc-retry-count 10 --rpc-retry-interval 3s
Best Practices
‚Äã
Security First
: Store private keys in environment variables, not command line
Backup Root Hashes
: Always save file root hashes after uploads
Use Verification
: Enable
--proof
for important downloads
Monitor Transactions
: Track uploads on the blockchain explorer
Test with Gen
: Use the
gen
command to create test files for development
HTTP Access
: Leverage the RESTful API for web applications and integrations
Batch KV Operations
: Use comma-separated lists for efficient key-value operations
Need more control? Consider running your own
storage node
to participate in the network and earn rewards.


========== [ https://docs.0g.ai/developer-hub/getting-started ] ==========

Developer Hub
On this page
Developer Hub
üöÄ The Problem We Solve
‚Äã
Your AI application needs:
Massive storage
for training data (TBs of datasets)
GPU compute
for model inference ($10K+/month on centralized providers)
Fast data availability
for real-time responses
Decentralization
without sacrificing performance
Modular Infrastructure
0G provides all of this in one integrated ecosystem - or use just the parts you need.
0G Services
‚Äã
‚õìÔ∏è 0G Chain
‚Äã
EVM-compatible blockchain optimized for AI
Deploy Contracts
Precompiles Reference
Chain Architecture
0G Compute
‚Äã
Decentralized GPU marketplace for AI workloads
Overview & Architecture
SDK Reference
Become a Provider
üíæ 0G Storage
‚Äã
High-performance storage for massive datasets
SDK Integration
CLI Commands
Architecture Details
üìä 0G DA
‚Äã
Scalable data availability for any chain
Technical Deep Dive
Integration Guide
Rollup Integrations
Community Projects
‚Äã
Explore our growing ecosystem of DeAI applications in the
awesome-0g
repository, which showcases community projects, tools, and resources built on 0G.
Ready to build? Pick a service above and start in minutes, or
join our Discord
for help.


========== [ https://docs.0g.ai/developer-hub/mainnet/mainnet-overview ] ==========

Developer Hub
Mainnet
On this page
0G Mainnet
Build and run production workloads on the 0G Mainnet.
Mainnet Explorer
üîç
Explore Mainnet Activity
Network Details
‚Äã
Parameters
Network Details
Network Name
0G Mainnet
Chain ID
16661
Token Symbol
0G
RPC URL
https://evmrpc.0g.ai
Storage Indexer
https://indexer-storage-turbo.0g.ai
Block Explorer
https://chainscan.0g.ai
‚úÖ 3rd Party RPCs (Recommended for production)
‚Äã
QuickNode
ThirdWeb
Ankr
Add Network to Wallet
‚Äã
Add 0G Mainnet
Add 0G Mainnet
Alternative RPC Providers
For redundancy in production apps, consider adding multiple RPC providers where available.
Contract Addresses
‚Äã
0G Storage
Flow:
0x62D4144dB0F0a6fBBaeb6296c785C71B3D57C526
Mine:
0xCd01c5Cd953971CE4C2c9bFb95610236a7F414fe
Reward:
0x457aC76B58ffcDc118AABD6DbC63ff9072880870
Developer Tools
‚Äã
Chain Explorer
:
https://chainscan.0g.ai (https://chainscan.0g.ai)


========== [ https://docs.0g.ai/developer-hub/testnet/testnet-overview ] ==========

Developer Hub
Testnet
On this page
0G Testnet (Galileo)
Test your applications on 0G's infrastructure without real costs or risks.
Testnet Explorer
üîç
Explore Testnet Activity
Network Details
‚Äã
Parameters
Network Details
Network Name
0G-Galileo-Testnet
Chain ID
16602
Token Symbol
0G
Block Explorer
https://chainscan-galileo.0g.ai
Faucet
https://faucet.0g.ai
‚úÖ 3rd Party RPCs (Recommended for production)
‚Äã
QuickNode
ThirdWeb
Ankr
dRPC NodeCloud
Getting Started
‚Äã
Step 1: Add Network to Wallet
‚Äã
Remove any old 0G testnet configurations before adding Galileo.
Need help?
Add to MetaMask
Add to OKX Wallet
Step 2: Get Test Tokens
‚Äã
Visit the
0G Faucet
to receive free testnet tokens.
Daily Limit
: 0.1 0G per wallet.
Step 3: Start Building
‚Äã
Choose your integration:
Deploy Smart Contracts
Use Storage SDK
Access Compute Network
Integrate DA Layer
Contract Addresses
‚Äã
caution
Addresses may change during testnet.
0G Storage
Flow:
0x22E03a6A89B950F1c82ec5e74F8eCa321a105296
Mine:
0x00A9E9604b0538e06b268Fb297Df333337f9593b
Reward:
0xA97B57b4BdFEA2D0a25e535bd849ad4e6C440A69
0G DA
DAEntrance:
0xE75A073dA5bb7b0eC622170Fd268f35E675a957B
Developer Tools
‚Äã
Block Explorers
‚Äã
Chain Explorer
: View transactions, blocks, and smart contracts
Storage Explorer
: Track storage operations and metrics
Validator Dashboard
: Monitor network validators
Development RPC
Development Only
This endpoint is for development purposes and should not be used in production applications.
https://evmrpc-testnet.0g.ai
Faucet
‚Äã
Use the
official Faucet
to request tokens. Each user can receive up to 0.1 0G token per day, which is sufficient for most testing needs.
If you require more than 0.1 0G token per day, please reach out in our vibrant
discord
community to request additional tokens.


========== [ https://docs.0g.ai/introduction/how-to-get-0g ] ==========

Introduction
How to Get 0G Token
On this page
How to Get 0G Token
Network Details
Token
: Native gas token (EVM-compatible)
Chain ID
: 16661
Explorer
:
https://chainscan.0g.ai
Mainnet Launch
: September 2025
Centralized Exchanges
‚Äã
The most straightforward way to acquire $0G is through centralized exchanges. After purchasing, withdraw directly to the
0G Mainnet
(select "0G Chain" or "0G Mainnet" as the withdrawal network).
Spot Trading
‚Äã
Exchange
Trading Pairs
HTX
0G/USDT
Binance
0G/USDT, 0G/USDC, 0G/BNB, 0G/FDUSD, 0G/TRY
Bybit
0G/USDT
MEXC
0G/USDT, 0G/USDC
KuCoin
0G/USDT
Gate.io
0G/USDT
Bitget
0G/USDT
HashKey Exchange
0G/USDT
LBank
0G/USDT
Upbit
0G/USDT
Kraken
0G/USDT
Bridge to 0G Chain
‚Äã
Use
XSwap
, the official bridge for the 0G network, powered by
Chainlink CCIP
.
XSwap Bridge
‚Äã
URL
:
https://xswap.link/bridge?toChain=16661
Supported Assets
: USDC and other tokens
Networks
: Ethereum ‚Üî 0G (with more chains coming)
Security
: Powered by
Chainlink CCIP
with enterprise-grade security
How to Bridge:
Visit
xswap.link/bridge?toChain=16661
Connect your wallet (MetaMask, SafePal, etc.)
Select source chain (e.g., Ethereum) and 0G as destination
Choose asset to bridge (e.g., USDC)
Confirm transaction and wait for bridging to complete
Once bridged, swap your assets to $0G on the 0G Hub
Swap on 0G Chain
‚Äã
Once you have assets on the 0G network, swap them for native $0G tokens.
0G Hub (Recommended)
‚Äã
URL
:
https://hub.0g.ai/swap
Features
: Official swap interface for the 0G ecosystem
Powered by
:
Jaine
Available Pairs
: Multiple trading pairs including ETH, USDT, USDC
The 0G Hub provides seamless token swapping, portfolio tracking, and access to the entire 0G ecosystem.
Wallet Setup
‚Äã
To receive and hold $0G, you need a wallet that supports the 0G network.
Supported Wallets
‚Äã
MetaMask
- Add 0G network manually via
Mainnet Overview
SafePal
- Native support for 0G chain
OKX Wallet
- Native support for 0G network
Rabby
- Add 0G network manually
Adding 0G Network
‚Äã
For detailed instructions on adding the 0G network to your wallet, including RPC endpoints and network configuration, visit the
Mainnet Overview
page.
For more information about the 0G network and its features, see
Understanding 0G
.


========== [ https://docs.0g.ai/introduction/new-landing ] ==========

new-landing
Build the Future of
Decentralized AI
Quick Start
Learn Concepts
Join Testnet
Connect to Galileo testnet and start building your first dApp on 0G
Connect to Testnet ‚Üí
New to 0G? Start Here
Learn the basics of 0G's AI infrastructure and get oriented quickly
Get Started ‚Üí
Inference
Integrate AI inference into your applications with SDKs
View Docs ‚Üí
Storage SDK
Store and retrieve massive datasets with Go and TypeScript client libraries
View Docs ‚Üí
Fine-tuning
Customize AI models for your specific use case with our command-line tools
View Docs ‚Üí
Run a Validator
Secure the network and earn rewards by running a validator node
Setup Guide ‚Üí
0
+
Partners
0
M+
Accounts
0
M+
Transactions
Ready to Build?
Join thousands of developers building the future of decentralized AI
View Full Documentation


========== [ https://docs.0g.ai/introduction/understanding-0g ] ==========

Introduction
Understanding 0G
On this page
Understanding 0G
Why 0G Exists
‚Äã
AI (Artificial Intelligence) is rapidly advancing, but its powerful capabilities are largely confined to centralized systems & limited to a few large companies. Bringing AI onto the blockchain unlocks transformative potential: truly verifiable AI, user-owned data powering AI applications, and open, censorship-resistant AI development.
However, a fundamental challenge has held back this vision:
AI's Data Hunger:
AI models and datasets are massive. Existing blockchains make storing and accessing this data impossibly expensive and slow.
Intense Compute Demands:
AI requires significant processing power, far beyond what traditional blockchains can offer efficiently.
Need for Speed:
Real-time AI applications demand high throughput and low latency
Without overcoming these hurdles, the dream of decentralized AI remains out of reach.
0G is built to break these barriers.
We provide the foundational infrastructure like high-performance storage, scalable compute, and a fast, modular blockchain‚Äîdesigned from the ground up to power the future of on-chain AI.
What is 0G?
‚Äã
0G (Zero Gravity) is the first decentralized AI L1 chain that orchestrates hardware resources (storage, compute) and software assets (data, models) to handle AI workloads at scale. It bridges the gap between Web2 AI capabilities and Web3 decentralization.
0G is building the global foundation for a better, fairer, and more open AI ecosystem, where power is distributed and innovation thrives
How it works
: 0G provides four independent services that solve different pieces of the AI + blockchain puzzle:
Storage
‚Üí Where to keep massive AI datasets
Compute
‚Üí How to run AI models economically
Chain
‚Üí Where to execute AI transactions quickly
Data Availability
‚Üí How to ensure data is always accessible
Modular Architecture
‚Äã
Key Benefit of Modular Architecture: You DON'T need to use all of 0G!
Pick only what you need:
Already on Ethereum, Polygon, or any EVM chain?
Use 0G Storage and Compute directly from your existing smart contracts, no need to migrate.
Building on Solana or other non-EVM chains?
Our SDKs support cross-chain integration
Just need one service?
Use only 0G Storage or only 0G Compute
The 4 Components of 0G
‚Äã
Component
Works Independently?
Key Features & Use Cases
Cost Highlight
0G Chain
‚úÖ Yes (Optional for other services)
Fastest modular EVM L1 for AI agents, DeFi with AI logic
Low gas fees in 0G token
0G Storage
‚úÖ Yes (Any app/chain can access)
Store AI models (GBs-TBs), training datasets, user files, game assets
10-100x cheaper than alternatives
0G Compute
‚úÖ Yes (Any app/chain can access)
Run AI inference, model training, verifiable compute, ML pipelines
Pay-only-for-what-you-use
0G DA
‚úÖ Yes (Works with any rollup/L1/L2)
Power gaming chains, AI rollups, high-frequency trading chains
Economical for high-volume DA
*0G Storage can be used completely standalone without any blockchain integration - perfect for traditional apps needing decentralized storage.
Key Concepts Explained Simply
‚Äã
What is decentralized storage?
Instead of storing your files on one company's computer (like Google Drive), they're split and stored across hundreds of computers worldwide.
Why it matters
: If Google's servers crash, you lose access. With decentralized storage, even if 50 computers fail, your data is still safe and accessible.
What is data availability?
It's a guarantee that your data can always be accessed when needed, like having multiple backup generators for your house.
Why it matters
: In blockchain, if data isn't available, the whole system can freeze. 0G ensures this never happens.
What is an AI compute network?
It's like Uber for computing power - connect to available GPUs when you need to run AI models, pay only for what you use.
Why it matters
: Instead of buying expensive GPUs or relying on big tech companies, access computing power on-demand from a global network.
What is a modular blockchain?
Like LEGO blocks, each part of the blockchain (storing data, processing transactions, reaching agreement) is separate and can be upgraded independently.
Why it matters
: Traditional blockchains are like old phones where you can't upgrade just the camera. Modular blockchains let you improve each part without rebuilding everything.
Why "Zero Gravity"?
‚Äã
"0G" represents "Zero Gravity" - the state where everything flows effortlessly. Just as astronauts move freely in zero gravity, data and AI computations flow seamlessly through our network without the heavy "gravity" of:
High costs
Slow speeds
Technical barriers
Platform lock-in
What Can You Build?
‚Äã
With 0G's technology, previously impossible use cases are now within reach:
On-chain AI agents
that learn and evolve
Decentralized ChatGPT
alternatives
AI-powered DeFi
trading systems
Medical AI
with patient-owned data
Large-scale ML training
without AWS bills
And this is just the beginning.
Where to Go Next
‚Äã
Now that you understand what 0G is and why it exists, here's how to dive deeper:
For Learners
‚Üí Read more about
Concepts
to understand how each component works
For Builders
‚Üí Jump into the
Developer Hub
to start building
For Operators
‚Üí Learn how to
Run a Node
and earn rewards
Join the 0G Community
‚Äã
Discord
- Get help and chat with builders
X(Twitter)
- Latest updates and announcements
GitHub
- Contribute to the project
We're excited to have you on board as we build the future of Web3 √ó AI together!


========== [ https://docs.0g.ai/introduction/vision-mission ] ==========

Introduction
Vision & Mission
On this page
Vision & Mission
Our Mission: Make AI a Public Good
‚Äã
At 0G, our mission is clear:
To Make AI a Public Good
.
We believe that AI technology should be accessible, transparent, and beneficial to everyone, not just a select few. By building a decentralized AI operating system, we're creating the infrastructure that will enable this vision.
Our Vision
‚Äã
We envision a world where:
AI is democratized
: Anyone can access and contribute to AI development without gatekeepers
AI is transparent
: The models, data, and processes are open and verifiable
AI is fair
: Resources and benefits are distributed equitably across the network
AI is secure
: Decentralization ensures no single point of failure or control
How We Achieve This
‚Äã
Every component of our ecosystem contributes toward this goal:
Open Infrastructure
: By providing decentralized storage, compute, and data availability, we remove the barriers to AI development
Community Ownership
: Through our node network and governance model, the community owns and operates the infrastructure
Economic Alignment
: Our tokenomics ensure that contributors are fairly rewarded for their participation
Technical Excellence
: We build the fastest, most efficient infrastructure to make decentralized AI competitive with centralized alternatives
Join Our Mission
‚Äã
We invite you to join us in building the foundation for a decentralized AI future. Whether you're a developer, node operator, or community member, there's a place for you in the 0G ecosystem.
Together, we're not just building technology ‚Äì we're shaping the future of AI for the benefit of all humanity.
Join the 0G Community
‚Äã
Discord
X(Twitter)
GitHub


========== [ https://docs.0g.ai/node-sale/ai-alignment-node-user-guide ] ==========

AI Alignment node
AI Alignment Node - Guide
On this page
0G AI Alignment Node - Guide
Who this is for & what you'll learn
Run your own Alignment Node or delegate to a NAAS provider
Understand system requirements, setup steps, and monitoring
Learn NAAS models (commission vs prepaid) and how to delegate/undelegate
Overview
‚Äã
The 0G AI Alignment Node system allows license holders to participate in the network either by running their own nodes or delegating to Node as a Service (NAAS) providers. This guide covers both options to help you choose the best approach for your needs.
Choose Your Path
‚Äã
Quick decision summary
‚Äã
Option
Best for
Setup time
Rewards
Maintenance
Option 1:
Delegate to NAAS
Non-technical users
2-3 Minutes
100% (prepaid) or minus commission
Provider handles
Option 2:
Run your own
Technical users
1-2 Hours
100%
You handle
Option 1: Delegating to NAAS Providers
‚Äã
Understanding NAAS Models
‚Äã
NAAS providers offer two delegation models:
Commission-Based Model
‚Äã
How it works:
NAAS provider takes a percentage of your rewards as commission
Payment:
No upfront payment required
Status:
Nodes start as "Active" immediately
Best for:
Users who prefer sharing rewards over upfront payments
Prepaid Model
‚Äã
How it works:
Pay a fixed fee upfront for node operation
Payment:
One-time or recurring prepaid fee
Status:
Nodes start as "Expired" until payment is confirmed
Best for:
Users who want predictable costs
How to Delegate
‚Äã
Step 1: Choose a NAAS Provider
‚Äã
Access the
0G Claim Portal
Navigate to the NAAS Providers section
Review available providers:
Name & Description
: Provider details
Commission Rate
: Percentage for commission-based model
Prepaid Price
: Cost for prepaid nodes
Reputation
: Community ratings and uptime statistics
Step 2: Complete Provider Onboarding
‚Äã
Visit the selected NAAS provider's platform (URL provided in portal)
Complete their onboarding process:
Create an account
Choose delegation model (commission or prepaid)
If prepaid, complete payment
Receive your
Target NAAS Node Address
from the provider
Important:
Save this address - you'll need it for delegation.
Step 3: Delegate Your Licenses
‚Äã
Return to the 0G Portal
Login with your wallet containing licenses
Navigate to "My Licenses"
Select license(s) to delegate
Choose "Delegate" action
Enter the
Target NAAS Node Address
provided by your NAAS provider
Confirm the transaction
Step 4: Monitor Delegation Status
‚Äã
Your delegation will show different statuses:
Status
Description
Inactive
License not delegated
Pending
Delegation submitted, awaiting NAAS approval
Delegated
Active and earning rewards
Expired
Prepaid period ended or payment issue
Managing Your Delegation
‚Äã
Checking Status
‚Äã
Access the 0G Portal
Navigate to "My Licenses"
View delegation status for each license
Undelegating
‚Äã
To reclaim your licenses:
Select delegated license(s)
Choose "Undelegate"
Confirm the transaction
Licenses immediately return to "Inactive" status
Note:
Undelegation is immediate and doesn't require NAAS approval.
Switching Providers
‚Äã
First undelegate from current provider
Wait for transaction confirmation
Follow delegation steps with new provider
NAAS Payment Management
‚Äã
For Commission-Based:
‚Äã
Rewards automatically distributed after commission deduction
No action required from you
Monitor earnings in the portal
For Prepaid:
‚Äã
Track expiration dates
Renew before expiration to avoid downtime
Provider will update status upon payment
Node shows "Expired" if payment lapses
Option 2: Running Your Own Node
‚Äã
System Requirements
‚Äã
Before setting up your node, ensure your system meets these minimum specifications:
Component
Minimum Requirement
RAM
64 MB
CPU
1 x86 Core @ 2.1GHz
Disk Space
10 GB
Internet
10 Mbps connection
Network
Port must be externally accessible (configure in firewall)
Installation & Setup
‚Äã
Step 1: Download the Node Binary
‚Äã
Download the latest 0G alignment node binary from the official repository:
# Download the binary (replace with actual URL)
wget https://github.com/0gfoundation/alignment-node-release/releases/download/v1.0.0/alignment-node.tar.gz
tar -xzf alignment-node.tar.gz
cd alignment-node
chmod +x 0g-alignment-node
Step 2: Configure Environment
‚Äã
Copy the example environment file:
cp .env.example .env
Edit the
.env
file with your configuration:
nano .env
Configure the following parameters:
export ZG_ALIGNMENT_NODE_LOG_LEVEL=debug
export ZG_ALIGNMENT_NODE_SERVICE_IP="http://127.0.0.1:34567"  # Full URL endpoint
export ZG_ALIGNMENT_NODE_SERVICE_PRIVATEKEY=your_private_key_here
note
The private key is the private key of the wallet you used to purchase the NFT. If the wallet doesn't have any NFTs, the wallet is not eligible to register as operator.
Important Configuration Notes:
LOG_LEVEL
: Set to
debug
for troubleshooting,
info
for normal operation
SERVICE_IP
: The ip of the service you are running. You need to add the external ip of the node to the
.env
file. The external ip is the ip of the node that is accessible from the internet.
PRIVATEKEY
: Your wallet's private key that holds the alignment node license(s)
Step 3: Network Configuration
‚Äã
Open your service port
The port specified in your configuration must be accessible externally for consensus communication.
Make sure this port is open in:
Cloud security groups/firewalls (AWS, Azure, GCP, etc.)
VPS provider firewalls
Local server firewall rules
Steps vary by provider; consult your host's docs.
Step 4: Start Your Node
‚Äã
Load environment variables:
source .env
Register the operator:
./0g-alignment-node registerOperator --key <your_private_key> --token-id <your_token_id> --chain-id <chain_id> --rpc <rpc_url> --contract <contract_address>
note
The token id is the token id of the NFT you purchased. The private key is the private key of the wallet you used to purchase the NFT. If the wallet doesn't have any NFTs, the wallet is not eligible to register as operator.
Configuration Details:
Chain ID
:
42161
(Arbitrum Mainnet)
RPC URL
: Use a reliable Arbitrum RPC endpoint such as:
https://arb1.arbitrum.io/rpc
(Public endpoint)
Or your preferred Arbitrum RPC provider
Alignment manager contract address
:
0xdD158B8A76566bC0c342893568e8fd3F08A9dAac
(Arbitrum Mainnet)
Start the node:
./0g-alignment-node start --mainnet
To run in background (recommended for production):
nohup ./0g-alignment-node start --mainnet > node.log 2>&1 &
Monitoring Your Node
‚Äã
View logs:
tail -f node.log
Node command help
‚Äã
./0g-alignment-node --help
./0g-alignment-node <command> --help
Healthy node checklist
Status reports without errors
Logs show steady activity, no repeated crashes
Troubleshooting
‚Äã
Node not connecting:
Verify port is open and accessible externally
Check your firewall/security group settings
Ensure private key has associated licenses
Node crashes:
Check logs for errors
Verify system requirements are met
Ensure stable internet connection
Best Practices
‚Äã
For Self-Hosted Nodes
‚Äã
Regular Updates
: Keep node binary updated
Monitoring
: Set up alerts for downtime
Backup
: Keep secure backup of private keys
Security
: Use dedicated wallet for node operation
Network
: Ensure stable internet connection
For NAAS Delegation
‚Äã
Research Providers
: Check reputation and uptime history
Understand Terms
: Read commission rates and prepaid terms
Monitor Status
: Regularly check delegation status
Payment Tracking
: Set reminders for prepaid renewals
Diversification
: Consider splitting licenses across providers


========== [ https://docs.0g.ai/node-sale/details/compliance-and-regulatory ] ==========

AI Alignment node
AI Alignment node Details
Compliance and Regulatory Requirements
On this page
Compliance and Regulatory Requirements
Regulation S Compliance
‚Äã
The sale follows Regulation S guidelines, restricting U.S. persons from participating. The sale website, promotional content, and user interface clearly indicate these restrictions, and KYC verification is mandatory for claiming rewards to maintain regulatory adherence. Prospective participants are advised to review these conditions and understand that resale of node licenses is not permitted within the first 12 months.
Information Disclosure
‚Äã
All communications related to the sale are made with transparency but exclude any directed selling to U.S. persons. Information shared in marketing materials, promotional activities, and social media avoids U.S.-targeted content, aligning with compliance requirements to mitigate any regulatory risks.


========== [ https://docs.0g.ai/node-sale/details/incentives-and-rewards ] ==========

AI Alignment node
AI Alignment node Details
Incentives & Rewards
On this page
Incentives, Rewards, and Vesting Mechanisms
Rebate/Commission Portal Tutorial
‚Äã
How can node buyers create a referral code?
‚Äã
Node buyers will be able to share their wallet address as the referral code after they made a purchase, it would give a 10% rebate to their referrals.
What rebate is available when using a referral code?
‚Äã
If you enter a referral code when purchasing a node, you'll receive a 10% rebate on the total price.
How will I receive my commission? (For Referrers)‚Äã
‚Äã
You will be able to claim the commission of any successful node purchased through the node on the reward claim site, which will be available after the public sale. Please refer to the 0G X for access to the claim site closer to the sale date.
Vesting Terms
‚Äã
Rewards from node operation vest over a three-year schedule, promoting consistent and long-term engagement. Vesting reduces the likelihood of short-term sales, fostering network stability and growth.
Additional Community Incentives
‚Äã
Partnerships, 0G ecosystem communities, and referrals provide extra benefits, such as rebates or promotional whitelist access. All promotional activities adhere to
regulatory guidelines
, and any reward or referral payments are conditional upon KYC eligibility.


========== [ https://docs.0g.ai/node-sale/details/kyc-verification ] ==========

AI Alignment node
KYC Verification Guide
On this page
KYC Verification Guide
Introduction
‚Äã
The 0G Alignment Node Portal requires users to complete a Know Your Customer (KYC) verification process to ensure secure and compliant participation. This comprehensive guide will walk you through each step of the verification process using Blockpass, our current KYC provider.
Additional KYC Providers Coming Soon
Additional KYC provider options will be available soon to provide more flexibility for users.
Eligibility Requirements
‚Äã
Before starting the KYC process, please ensure you meet the following requirements:
Age Requirement
‚Äã
You must be at least 18 years old to participate in the 0G Alignment Node Portal.
Geographic Restrictions
‚Äã
Due to regulatory requirements, users from the following countries and regions are
not eligible
to participate:
Restricted Countries:
Belarus, the Central African Republic, The Democratic Republic of Congo, the Democratic People's Republic of Korea, Cuba, Iran, Libya, Russia, Somalia, Sudan, South Sudan, Venezuela, Yemen, Zimbabwe.
Restricted Regions:
Ukraine: Crimea region, Donetsk People's Republic (DNR), Luhansk People's Republic (LNR), Kherson region, and Zaporizhia region
Geographic Restrictions
If you reside in any of these restricted locations, you will not be able to complete the KYC process. This restriction is in place to comply with international regulations and sanctions.
Pre-Verification Checklist
‚Äã
Before starting your KYC verification, ensure you have:
Valid government-issued ID
Recent proof of address document
Good lighting for selfie verification
Stable internet connection (disable VPN)
Desktop/laptop for wallet signing
Compatible browser (Chrome/Firefox recommended)
15-20 minutes of uninterrupted time
Getting Started
‚Äã
Video Guide
Step by step video guide to complete the KYC process.
Step 1: Start the KYC Process
‚Äã
Go to the
KYC page
. Connect your wallet and click on "Start KYC" button.
Step 2: Submit Documents
‚Äã
Upload the required documents, complete the selfie verification. This submission can also be done on mobile for easier document upload.Make sure to close the mobile browser tab after submitting the documents before opening new session on desktop.
Step 3: Complete Wallet Verification
‚Äã
This step is critical and must be completed on desktop. Even if you submitted documents on mobile, you must complete this step on desktop so trigger the magic link again from portal (like the step 1), open the link in desktop browser and continue from there.
Step 1: Connect Your Wallet
‚Äã
Before beginning the KYC process, you'll need to connect your wallet to the 0G platform:
Navigate to the KYC section in your dashboard
Click
"Connect Wallet"
to initiate the verification process
Wait for the wallet connection to be confirmed
Click "Start KYC" to get started
Step 2: Choose Your KYC Provider
‚Äã
Currently,
Blockpass
is integrated as our primary KYC verification service
Select
"Blockpass (Recommended)"
Click
"Continue"
to proceed
Creating Your Blockpass Account
‚Äã
Email Registration Process
‚Äã
Enter your email address in the Blockpass modal
Choose whether to:
Create a new account, or
Use an existing Blockpass account
For new users:
A magic link will be sent to your email
Check your inbox and click the link, This will take you to the document upload interface
Progress Saving
If you've already started the process, Blockpass will remember your progress and allow you to continue where you left off.
Document Upload Process
‚Äã
Required Documents Checklist
‚Äã
You'll need to provide the following three types of verification:
1. Government-Issued ID (Choose One)
‚Äã
Passport
National ID
Driving License
2. Proof of Address
‚Äã
Utility bill
Bank statement
Other supported documents
3. Liveness Verification
‚Äã
Selfie verification with liveness check
Document Upload Guidelines
‚Äã
For ID Documents:
‚Äã
Preparation:
Open your document to the data page
Find good lighting conditions
Clean your camera lens
Capture Requirements:
All four corners must be visible
Avoid glare and shadows
Ensure text is clear and readable
Document should fill most of the frame
Data Extraction Accuracy
Even if the automatic data extraction shows incorrect information, proceed with submission. Our verification team will handle corrections during the manual review process.
For Address Proof:
‚Äã
Upload a document from the supported list
Ensure your current residential address is clearly visible
Document should be recent (typically within 3 months)
For Selfie Verification:
‚Äã
Setup:
Find a well-lit room
Remove glasses if they cause glare
Position yourself at arm's length from camera
Process:
The system will auto-capture your face
Follow on-screen prompts:
Look straight ahead
Turn your head left when prompted
Turn your head right when prompted
Complete all requested movements
Cross-Device Flexibility
One of Blockpass's convenient features is seamless cross-device compatibility:
Use the
"Email me a link"
button on any page
Continue your KYC process on any device
All progress is automatically saved
Switch between mobile and desktop as needed
Recommended Workflow:
Mobile:
Upload documents using your phone's camera
Desktop:
Complete wallet verification for better experience
Either:
Check status and make corrections
Wallet Verification (Critical Step)
‚Äã
Desktop Strongly Recommended
‚Äã
While you can complete most steps on mobile,
we strongly recommend using a desktop or laptop for wallet verification
.
Benefits of Desktop Verification
‚Äã
Better signing experience
More stable connection to wallet extensions
Smoother completion of mandatory signing
How to Switch to Desktop
‚Äã
Complete initial steps on mobile (if preferred):
Upload all documents
Complete selfie verification
Close the mobile browser tab
Continue on desktop:
Open Blockpass on your desktop browser
Log in using the same email address
Use magic link or password to access your account
Resume verification:
Select
"Register with document"
Choose the same ID type you selected earlier
All your information will be pre-filled from the last session
Complete wallet verification:
Connect your wallet (MetaMask, WalletConnect, or Coinbase)
Sign the verification message when prompted
Confirm the signature in your wallet app
Mandatory Wallet Signing
The wallet signing step is mandatory. Submissions without proper signing will NOT be approved due to KYC compliance requirements.
Final Submission
‚Äã
Before Submitting:
‚Äã
Review all uploaded documents
Ensure wallet is properly connected and verified
Check that all sections show "completed" status
To Submit:
‚Äã
Once all steps Completed
The
"Register"
button will become active
Click
"Register"
to submit for KYC verification
You'll see a confirmation screen, You can set password on this screen for future changes
Verification Timeline
‚Äã
Stage
Timeframe
Initial Review
24-36 hours
Additional Checks (if needed)
12-24 hours
Final Approval
Email notification
What to Expect:
‚Äã
Email notifications at each stage
Clear feedback if any issues arise
Direct communication for any clarifications needed
Handling Rejections
‚Äã
If your KYC application is rejected, don't worry! Here's what to do:
Steps to Resubmit:
‚Äã
Check your email
for specific rejection reasons
Log back in
using the same email address
Navigate
to the rejected document section
Update
only the required documents or information
Resubmit
for re-verification
Frequently Asked Questions
‚Äã
How can I ensure my documents are accepted on first submission?
Use good lighting and steady hands when taking photos
Ensure all documents show matching details
Use valid, non-expired identification
Clean your camera lens before capturing documents
Make sure all four corners of documents are visible
Avoid glare and shadows
What's the best way to complete the KYC process?
Complete the process in one session if possible
Have all documents ready before starting
Use Chrome, Safari or Firefox browsers for best compatibility
Disable VPN during verification
Use desktop for wallet signing, mobile for document photos
Don't log in to the same account on multiple devices simultaneously
Is my personal information secure?
Yes, your KYC information is protected with:
End-to-end encryption for all uploads
Secure storage in compliance with GDPR
Information used only for verification purposes
No sharing with third parties without consent
My camera won't activate, what should I do?
Check browser permissions for camera access and ensure your browser allows camera usage for the KYC site.
I can't connect my wallet, how do I fix this?
Try using a different wallet or browser. Make sure pop-ups are enabled and you're using a desktop for the best wallet connection experience.
My document upload keeps failing, what's wrong?
Check that your file size is under 10MB and you have a stable internet connection. Try refreshing the page and uploading again.
The verification message won't sign, what should I do?
Switch to desktop and try a different browser. The wallet signing step works best on desktop computers.
My email link expired, how do I get a new one?
Request a new magic link from the login page. Wait at least 5 minutes before requesting another link.
What are common reasons for KYC rejection?
Common reasons for KYC rejection include:
Blurry or unclear document images
Expired identification documents
Address mismatch between documents
Incomplete wallet signature
Poor selfie quality or failed liveness check
If your KYC application is rejected, check your email for specific rejection reasons and resubmit the corrected documents.
Need Help?
‚Äã
Discord Community:
Join our
Discord
for peer support and quick answers from the team.
Support:
Contact us on Discord with screenshots and clear steps to reproduce any issues.
Disclaimer:
This guide is for informational purposes. KYC requirements may vary by jurisdiction. Always ensure you comply with your local regulations.


========== [ https://docs.0g.ai/node-sale/details/purchasing-nodes ] ==========

AI Alignment node
AI Alignment node Details
How to Purchase Nodes
On this page
Purchasing Nodes: Steps and Payment Options
Supported Blockchains and Payment Methods
: The sale will be conducted in USDC on Arbitrum, but we provide a live bridging gateway at checkout that supports multiple blockchains (ETH, Arbitrum, BNB) and tokens. All purchases require a compatible wallet, and the resulting node licenses are issued as non-transferable ERC-721 NFTs.
Video Tutorial
‚Äã
important
Please note: Initially, the Public Sale was intended to use wETH on Arbitrum, but after the community‚Äôs feedback, it will be USDC on Arbitrum. Please note this correction to USDC on Arbitrum as the video says wETH.
Note that this video is for demonstrative purposes only, may not reflect exact details and have different prices or tokens than the actual sale.
Step-by-Step Purchase Process:
‚Äã
Step 1 - Go to
https://node.0gfoundation.ai
and connect your wallet
‚Äã
Step 2 - Select the chain and token you want to purchase
‚Äã
Step 3 - View the tiers available and purchase
‚Äã
In total there are 32 available for purchase. If you hold a whitelisted address, the Tier that you are entitled to will be shown on the top. Sold out tiers will be moved to the bottom of the page.
Step 4 - Set the quantity
‚Äã
Start by inputting the number of node(s) that you will be purchasing.
Step 5 - Input Promo Code (Optional)
‚Äã
If you have a Promo Code, input before your purchase. The discount rate will be applied directly to your checkout price.
Additionally, if you have already purchased a node, you can provide your wallet address as a referral promo code too! This will provide a 10% rebate to the referral, and 10% commission if they purchase successfully.
Step 6 - Approve transaction
‚Äã
Once you are confirmed on the final price, click the "Approve" button. You are then required to approve the transaction via your wallet.
Step 7 - Complete purchase
‚Äã
Continue by clicking "Purchase". You will need to confirm the transaction via your wallet.
After confirming your purchase, you will receive a notification on the top right of the page to inform you of the successful purchase.


========== [ https://docs.0g.ai/node-sale/disclaimer ] ==========

AI Alignment node
Node Disclaimer
Node Disclaimer
PLEASE READ THE ENTIRETY OF THIS "LEGAL DISCLAIMER" SECTION CAREFULLY. NOTHING HEREIN CONSTITUTES LEGAL, FINANCIAL, BUSINESS, OR TAX ADVICE AND YOU ARE STRONGLY ADVISED TO CONSULT YOUR OWN LEGAL, FINANCIAL, TAX, OR OTHER PROFESSIONAL ADVISOR(S) BEFORE ENGAGING IN ANY ACTIVITY IN CONNECTION HEREWITH. NEITHER  0G FOUNDATION (‚ÄúTHE COMPANY‚Äù), ANY OF THE PROJECT CONTRIBUTORS WHO HAVE WORKED ON THE ZEROGRAVITY NETWORK (AS DEFINED HEREIN) OR ANY PROJECT TO DEVELOP THE ZEROGRAVITY NETWORK IN ANY WAY WHATSOEVER, ANY DISTRIBUTOR AND/OR VENDOR OF NODE LICENSE (OR SUCH OTHER RE-NAMED OR SUCCESSOR TICKER CODE OR NAME OF SUCH TOKENS) (THE ‚ÄúDISTRIBUTOR‚Äù), NOR ANY RELATED SERVICE PROVIDER SHALL BE LIABLE FOR ANY KIND OF DIRECT OR INDIRECT DAMAGE OR LOSS WHATSOEVER WHICH YOU MAY SUFFER IN CONNECTION WITH ACCESSING ANY MATERIAL RELATING TO NODE LICENSE NFT (THE TOKEN DOCUMENTATION) AVAILABLE ON THE WEBSITE AT
https://0g.ai/node-sale
(THE WEBSITE, INCLUDING ANY SUB-DOMAINS THEREON) OR ANY OTHER WEBSITES OR MATERIALS PUBLISHED OR COMMUNICATED BY THE COMPANY OR ITS REPRESENTATIVES FROM TIME TO TIME.
Project purpose
: You agree that you are acquiring Node License NFT to participate in the ZeroGravity Network and to obtain services on the ecosystem thereon. The Company, the Distributor, and their respective affiliates would develop and contribute to the underlying source code for the ZeroGravity Network. The Company is acting solely as an arms‚Äô length third party in relation to the Node License NFT distribution and is not acting in the capacity as a financial advisor or fiduciary of any person with regard to the distribution of Node License NFT.
Eligibility
: To be eligible to use the Website (including services thereon) or participate in the Node License NFT distribution you must be at least eighteen (18) years of age or older. The Website, interface and services thereon is strictly NOT offered to persons or entities who reside in, are citizens of, are incorporated in, or have a registered office in any Restricted Territory, as defined below (any such person or entity from a Restricted Territory shall be a Restricted Person). If you are a Restricted Person, then do not attempt to access or use the Website. Use of a virtual private network (e.g., a VPN) or other means by Restricted Persons to access or use the Website, interface or services is prohibited. For the purpose of these Terms, Restricted Territory shall mean Belarus, Canada, Cuba, North Korea, Iran, Russia, Syria, the United States, the United Kingdom, Venezuela, Yemen, and specific regions in Ukraine and Russia, namely the Crimea region, Donetsk People‚Äôs Republic (DNR), Luhansk People‚Äôs Republic (LNR), as well as the Kherson and Zaporizhia regions.
Validity of Token Documentation and Website
: Nothing in the Token Documentation or the Website constitutes any offer by the Company, the Distributor, or the ZeroGravity Network team to sell any Node License NFT (as defined herein) nor shall it or any part of it nor the fact of its presentation form the basis of, or be relied upon in connection with, any contract or purchase decision. Nothing contained in the Token Documentation or the Website is or may be relied upon as a promise, representation or undertaking as to the future performance of the ZeroGravity Network. The agreement between the Distributor (or any third party) and you, in relation to any distribution or transfer of Node License NFT, is to be governed only by the separate terms and conditions of such agreement.
Deemed Representations and Warranties
: By accessing the Token Documentation or the Website (or any part thereof), you shall be deemed to represent and warrant to the Company, the Distributor, their respective affiliates, and the ZeroGravity Network team as follows:
in any decision to acquire any Node License NFT, you have not relied and shall not rely on any statement set out in the Token Documentation or the Website.
you shall at your own expense ensure compliance with all laws, regulatory requirements, and restrictions applicable to you (as the case may be).
you acknowledge, understand and agree that Node License NFT may have no value, there is no guarantee or representation of value or liquidity for Node License NFT, and Node License NFT is not an investment product nor is it intended for any investment purposes, speculative or otherwise.
none of the Company, the Distributor, their respective affiliates, and/or the ZeroGravity Network team shall be responsible for or liable for the value of Node License NFT, the transferability and/or liquidity of Node License NFT, and/or the availability of any market for Node License NFT through third parties or otherwise.
The Company, the Distributor, and the ZeroGravity Network team do not and do not purport to make, and hereby disclaims, all representations, warranties or undertaking to any entity or person (including without limitation warranties as to the accuracy, completeness, timeliness, or reliability of the contents of the Token Documentation or the Website, or any other materials published by the Company or the Distributor). To the maximum extent permitted by law, the Company, the Distributor, their respective affiliates, and service providers shall not be liable for any indirect, special, incidental, consequential, or other losses of any kind, in tort, contract, or otherwise (including, without limitation, any liability arising from default or negligence on the part of any of them, or any loss of revenue, income or profits, and loss of use or data) arising from the use of the Token Documentation or the Website, or any other materials published, or its contents (including without limitation any errors or omissions) or otherwise arising in connection with the same. Prospective acquirors of Node License NFT should carefully consider and evaluate all risks and uncertainties (including financial and legal risks and uncertainties) associated with the distribution of Node License NFT, the Company, the Distributor, and the ZeroGravity Network team.
Node License NFT
: Node License NFT are designed to be utilized, and that is the goal of the Node License NFT distribution. In particular, it is highlighted that Node License NFT:
does not have any tangible or physical manifestation and does not have any intrinsic value/pricing (nor does any person make any representation or give any commitment as to its value).
is non-refundable, not redeemable for any assets of any entity or organization, and cannot be exchanged for cash (or its equivalent value in any other digital asset) or any payment obligation by the Company, the Distributor, or any of their respective affiliates.
does not represent or confer on the token holder any right of any form with respect to the Company, the Distributor (or any of their respective affiliates), or their revenues or assets, including without limitation any right to receive future dividends, revenue, shares, ownership right or stake, share or security, any voting, distribution, redemption, liquidation, proprietary (including all forms of intellectual property or license rights), right to receive accounts, financial statements or other financial data, the right to requisition or participate in shareholder meetings, the right to nominate a director, or other financial or legal rights, equivalent rights, or intellectual property rights, or any other form of participation in or relating to the ZeroGravity Network, the Company, the Distributor, and/or their service providers.
is not intended to represent any rights under a contract for differences or under any other contract the purpose or intended purpose of which is to secure a profit or avoid a loss.
is not intended to be a representation of money (including electronic money), payment instrument, security, commodity, bond, debt instrument, unit in a collective investment, managed investment scheme, or any other kind of financial instrument or investment.
is not a loan to the Company, the Distributor or any of their respective affiliates, is not intended to represent a debt owed by the Company, the Distributor, or any of their respective affiliates, and there is no expectation of profit nor interest payment.
does not provide the token holder with any ownership or other interest in the Company, the Distributor, or any of their respective affiliates.
Notwithstanding the Node License NFT distribution, users have no economic or legal right over or beneficial interest in the assets of the Company, the Distributor, or any of their affiliates after the token distribution. To the extent any secondary market or exchange for trading Node License NFT does develop, it would be run and operated wholly independently of the Company, the Distributor, and the distribution of Node License NFT. Neither the Company nor the Distributor will create such secondary markets nor will either entity act as an exchange for Node License NFT.
English language
: The Token Documentation and the Website may be translated into a language other than English for reference purpose only and in the event of conflict or ambiguity between the English language version and translated versions of the Token Documentation or the Website, the English language versions shall prevail. You acknowledge that you have read and understood the English language version of the Token Documentation and the Website.
No Distribution
: No part of the Token Documentation or the Website is to be copied, reproduced, distributed, or disseminated in any way without the prior written consent of the Company or the Distributor. By attending any presentation on this Token Documentation or by accepting any hard or soft copy of the Token Documentation, you agree to be bound by the foregoing limitations.


========== [ https://docs.0g.ai/node-sale/faq/ ] ==========

AI Alignment node
FAQ
On this page
Frequently Asked Questions
Where do I purchase AI Alignment Nodes?
‚Äã
https://node.0gfoundation.ai/
How do I purchase AI Alignment Nodes?
‚Äã
See the step by step guide
here
.
Who can participate in the Node Sale?
‚Äã
The 0G AI Alignment Node Sale is open to community members in eligible regions who meet the necessary criteria. Please review all disclaimers and important details regarding the 0G Node Sale by visiting our disclaimer
here
. Only persons who meet the requirements for the sale are allowed to participate. KYC will be required from the purchaser before any receipt of rewards.
Are Whitelist and Public Allocations Separate?
‚Äã
The whitelist allocation is independent and will not be rolled over to the public sale. The
Whitelist Sale
opens November 11, 2024, at 12 PM UTC. The sale will be denominated in USDC on Arbitrum, and only whitelisted participants may purchase nodes during this phase. The
Public Sale
opens November 13, 2024, at 12 PM UTC, is denominated in USDC on Arbitrum, and is available to all users, subject to geographic and regulatory restrictions. Please see our disclaimer for more information.
What price will the sale be pegged to?
‚Äã
Both the Whitelist and Public Sales are denominated in USDC on Arbitrum. Node pricing will be pegged to a snapshot price of wETH of $3,130.
When Can I Operate My Node?
‚Äã
Node operations will commence after the mainnet launch, projected for Q1 2025. License holders will receive further instructions for operation and delegation options if preferred.
How Do Rewards for Alignment Nodes Compare with Other Nodes?
‚Äã
Alignment Nodes are expected to offer higher reward rates due to their unique responsibilities, limited supply, and operational requirements compared to storage or validator nodes.
Are Multiple NFTs Supported per Server?
‚Äã
Yes, users may operate multiple NFTs on a single server, with allowances for connecting and managing several nodes under one setup.
Whitelist & Node Sale
‚Äã
What is the whitelist (WL)?‚Äã
‚Äã
Whitelist is a pre-approved list of participants who are given exclusive access to certain privileges during a sale event. This system is used to reward and incentivize key contributors, partners, or early supporters of a project.
Does entering the whitelist guarantee that I can definitely purchase a node?‚Äã
‚Äã
Your whitelist guarantees an allocation during the Whitelist sale period (starting Nov 11). If a purchase is not made within this period, your allocation will be released.
How to join the whitelist?‚Äã
‚Äã
To get a whitelist spot for the 0G Foundation Node Sale, community members and ecosystem participants are eligible for allocations, please visit 0G X and Discord for ways to receive a whitelist. You can also apply for a whitelist spot by filling out the whitelist form
here
.
Payment & Licenses
‚Äã
What payment methods will be accepted for purchasing a node?‚Äã
‚Äã
The nodes will be priced in USDC for both the Whitelist Sale and Public Sale, both on Arbitrum. However, to facilitate payment for users on different chains, the AI Alignment Node Sale will be accepting multiple tokens across multiple networks through a live bridging aggregator that accepts multichain payment including but not limited to BTC, ETH, ARB, SOL, etc. More info
here
.
How will the node licenses be distributed?‚Äã
‚Äã
After the node sale period is completed, node licenses will be distributed as an NFT to the purchase wallet of the user.
What will I receive from participating in the node sale?‚Äã
‚Äã
You will receive a soulbound NFT (ERC-721) which represents your Node License. The NFTs can be minted and transferred to your wallet via claim.0gfoundation.ai after the conclusion of the node sale. You will be able to operate the node after 0G Mainnet is live.
Will the NFTs be transferable?
‚Äã
The Alignment Node license gives buyers lifetime access. The NFTs will be non-transferable for the first year after the node sale.
Node Operations
‚Äã
What are the hardware requirements?‚Äã
‚Äã
0G Foundation‚Äôs AI Alignment nodes are designed for adoption - they can be run on community member's laptops, desktops, mobiles, or even on cloud instances.
As for device requirements, the configuration needed is very minimal:
64MB RAM
1 x86 CPU Core @2.1GHz
10GB Disk Space
10Mbps Internet Connection
When can I begin operating my node?‚Äã
‚Äã
AI Alignment utility will go live in 2025, after 0G Mainnet Launch.
How many nodes can I purchase?  ‚Äã
‚Äã
The number of purchasable nodes will be capped per tier. Please refer to the
Node Sale Tier documentation
for reference.
How do I run a node? Is it complicated?‚Äã
‚Äã
Running a node can be quite straightforward and easy, typically involving just a few steps. Here's a video tutorial to guide you through the process:
If you prefer not to manage the node yourself, you can delegate to other node operators with just a single click through our explorer, which will be available shortly.
Will the sale be accessible from other platforms?‚Äã
‚Äã
Both Public and Whitelist sale will be available
here
except for Partnered Launchpads, in which case the front end will be on 0G Partners' website.
What will happen to any unsold node rewards?
‚Äã
Rewards from unsold nodes will be reallocated to the sold node runners.
What is your tokenomics?
‚Äã
What is the % unlocked at TGE?
‚Äã
33% of node rewards will be initially claimable on TGE. In order to encourage long term participation in the 0G ecosystem, there will be a penalty based on duration in which a participant chooses to receive this initial reward. The remaining 67% of alignment rewards are linearly unlocked (daily) over 36 months. This penalty mechanism is subject to community vote, further showcasing the "community first approach" of 0G.
When is KYC required? How is it conducted?
‚Äã
KYC is required before claiming rewards. It will be provided by a third-party provider. No refund will be offered if the node purchaser does not meet KYC requirements.
How many nodes can I purchase?
‚Äã
Each individual can purchase up to a certain amount of nodes per tier. See
here
for the caps per tier.
What happened to unsold nodes?
‚Äã
Unsold alignment node NFT licenses will be burned. The rewards from the unsold nodes will be re-allocated to the sold verified node buyers.
What are the launchpad partners working on the 0G Foundation Node Sale?
‚Äã


========== [ https://docs.0g.ai/node-sale/intro/ ] ==========

AI Alignment node
Introduction
Introduction
On this page
Introduction
Purpose and Benefits of the 0G AI Alignment Node Sale
‚Äã
The objective of 0G Foundation: The 0G Foundation‚Äôs node sale aims to create a decentralized AI operating system (deAIOS) that operates transparently, safely, and under community influence. Centralized AI structures may lack these attributes, creating potential risks for data integrity and security. The 0G Foundation‚Äôs goal is to develop AI as a public good, fostering an ecosystem that prioritizes transparency and reduces reliance on central authorities, making it suitable for broad public utility.
What is an AI Alignment Node?
‚Äã
Alignment nodes provide the key utility of monitoring whether the other kinds of decentralized nodes in the 0G network - validator nodes, storage nodes, security nodes - faithfully follow network protocols. To start, the nodes will have certain utility, and in the near future, AI Alignment Nodes will have additional utility to monitor on-chain AI model drift and to ensure that 0G‚Äôs on-chain AI is behaving as intended.
To sustain the node utility operation and allow the network to be further secured, Alignment Node owners may receive a portion of the fees collected by the network by operating the nodes and contributing work to the 0G network. Node sale participants and stakers (i.e., long term believers who secure the ecosystem) may get additional rewards from the 0G ecosystem over time. The 0G node sale will be a launch that enables the community to participate on an equal playing field with entry points within multiple tiers.
Why Run a Node?
‚Äã
Running a node on the 0G network offers participants a chance to directly contribute to the growth and security of our decentralized AI ecosystem. Nodes are the backbone of our network, enabling the following:
Decentralization
: Ensure the network remains decentralized and resilient.
Network Security
: Enhance the security and reliability of the network by verifying transactions.
AI Processing Power
: Support AI computations, thereby contributing to the network's ability to deliver on-chain AI services.
Reward Opportunities
: Node operators can earn rewards in the form of native tokens or other incentives for their contribution to the network.
What are the specific use cases for AI Alignment Nodes?
‚Äã
Protocol Monitoring
: Ensuring that validator, storage, and security nodes comply with network protocols.
AI Model Monitoring
: Tracking AI model alignment and checking for any unintended behavior.
Network Security
: Safeguarding the ecosystem by flagging deviations and ensuring consistent ethical standards.
Economic and Governance Roles
: Node holders may earn rewards and participate in governance decisions, influencing the network‚Äôs direction.


========== [ https://docs.0g.ai/node-sale/intro/eligibility ] ==========

AI Alignment node
Introduction
Eligibility & Disclaimer
On this page
Eligibility and Whitelist Participation
Whitelist Access
: Community engagement, role-based eligibility, and participation in 0G‚Äôs Discord or X channels provide pathways to secure a whitelist spot. Please be aware that joining the whitelist does not guarantee a purchase; it only ensures allocation access during the whitelist phase.
Special Roles and Partnerships
: Selected community members, project partners, and Key Opinion Leaders (KOLs) may receive additional access or promotional privileges, subject to guidelines for eligible users and without guarantees for resale or transferability of whitelist spots.
Disclaimer‚Äã
‚Äã
Please review the full details and terms of the 0G Node Sale by visiting our
disclaimer
.


========== [ https://docs.0g.ai/node-sale/intro/node-holder-benefits ] ==========

AI Alignment node
Introduction
Node Holder Benefits
Node Holder Benefits
As an alignment node operator in 0G's ecosystem, users have the opportunity to earn up to 15% of the total 0G ecosystem supply allocated over the next 3 years by helping to secure the network. Nodes are rewarded for assisting in checking and verifying the correct behavior of storage, DA, and serving nodes within the network, playing a crucial role in ensuring data integrity for the AI and other supported workloads. Additional rewards will be provided from 0G‚Äôs vibrant and growing ecosystem as a coordinated effort to maintain the AI integrity and security of the platform.
The Alignment Node sale offers holders the chance to join the network and contribute to the development and security of a decentralized ecosystem for the largest collection of decentralized AI computing and data processing power, without requiring every regular user to be prepared for intensive computational tasks from day one.


========== [ https://docs.0g.ai/node-sale/intro/sale-structure ] ==========

AI Alignment node
Introduction
Sale Structure & Timeline
On this page
Sale Structure, Dates, and Tiers
Sale Phases
‚Äã
The sale is structured into two phases:
Whitelist Sale
: Whitelist Sale opens November 11, 2024, at 12 PM UTC and remains open for two days. The sale will be denominated in USDC on Arbitrum, and only whitelisted participants may purchase nodes during this phase.
Public Sale
opens November 13, 2024, at 12 PM UTC, is denominated in USDC on Arbitrum, and is available to all users, subject to geographic and regulatory restrictions. Please see our disclaimer for more information.
Tier Pricing
‚Äã
The node sale is segmented into 32 pricing tiers, beginning at 0.05 ETH per node, allowing for varied entry points that cater to diverse participant levels. While pegged to an ETH snapshot price of $3,130, both sales are conducted in USDC on Arbitrum. See
here
for more details.


========== [ https://docs.0g.ai/node-sale/node-sale-index ] ==========

AI Alignment node
Welcome to the 0G Foundation AI Alignment Node Sale
In this section, learn more about the 0G Foundation AI Alignment Node Sale.
What is the 0G Foundation AI Alignment Node Sale?
Why should I participate?
Who can participate?
How do I purchase nodes?
Frequently Asked Questions


========== [ https://docs.0g.ai/resources/blog ] ==========

blog


========== [ https://docs.0g.ai/resources/glossary ] ==========

Resources
Glossary
On this page
Glossary
A comprehensive list of technical terms used throughout the 0G documentation.
A
‚Äã
AI Agent
: An autonomous software program that can perceive its environment, make decisions, and take actions to achieve specific goals.
AVS (Actively Validated Services)
: Services that require active validation from operators, commonly used in restaking protocols like EigenLayer and Babylon.
C
‚Äã
Chain
: In the context of 0G, refers to the 0G blockchain that serves as the foundational layer for transactions and smart contracts.
Compute Network
: The distributed network of nodes that provide computational resources for AI workloads including inference and training.
D
‚Äã
DA (Data Availability)
: A layer that ensures data required by blockchain applications is available when needed, crucial for scalability and security.
deAIOS
: Decentralized AI Operating System - 0G's comprehensive infrastructure for decentralized AI applications.
Decentralized Storage
: A storage system that distributes data across multiple nodes rather than relying on centralized servers.
E
‚Äã
ERC-721
: The standard interface for non-fungible tokens (NFTs) on Ethereum and EVM-compatible chains.
ERC-7857
: An extension of ERC-721 that adds support for encrypted metadata, enabling secure transfer of AI agents as NFTs.
Erasure Coding
: A data protection method that breaks data into fragments and encodes it with redundant pieces to ensure recovery even if some parts are lost.
I
‚Äã
Inference
: The process of using a trained AI model to make predictions or decisions based on new input data.
INFT (Intelligent Non-Fungible Token)
: NFTs that can encapsulate AI agents with their intelligence and capabilities intact.
M
‚Äã
Modular Blockchain
: A blockchain architecture where different functions (consensus, execution, data availability) are separated into specialized layers.
O
‚Äã
Oracle
: In the context of INFTs, a service that verifies the integrity of metadata transfers using either TEE or ZKP technology.
P
‚Äã
Precompile
: Built-in functions in a blockchain that are implemented at the protocol level for optimal performance.
Proof of Random Access (PoRA)
: 0G's consensus mechanism that ensures data availability by requiring nodes to prove they can access random data samples.
Q
‚Äã
Quorum
: A minimum number of nodes required to reach consensus on a decision in a distributed system.
R
‚Äã
RaaS (Rollup as a Service)
: Platforms that provide infrastructure and tools to easily deploy and manage blockchain rollups.
Rollup
: A scaling solution that processes transactions off the main chain while posting transaction data back to it.
S
‚Äã
Sharding
: A scaling technique that divides a network into smaller parts (shards) to process transactions in parallel.
Storage Node
: A node in the 0G network that stores and serves data to the network.
T
‚Äã
TEE (Trusted Execution Environment)
: A secure area of a processor that ensures code and data loaded inside are protected with respect to confidentiality and integrity.
Testnet
: A test network that mimics the main network but uses test tokens, allowing developers to experiment without real value at risk.
V
‚Äã
Validator Node
: A node that participates in consensus by validating transactions and proposing new blocks.
W
‚Äã
Web3
: The vision of a decentralized internet built on blockchain technology, emphasizing user ownership and control.
Z
‚Äã
Zero Gravity (0G)
: The name representing the weightless state where transactions and data exchanges occur effortlessly in the 0G ecosystem.
Zero-Knowledge Proof (ZKP)
: A cryptographic method where one party can prove to another that a statement is true without revealing any information beyond the validity of the statement.
This glossary is continuously updated as the 0G ecosystem evolves. If you encounter a term not listed here, please contribute by submitting a pull request to our
documentation repository
.


========== [ https://docs.0g.ai/resources/how-to-contribute ] ==========

Resources
How to Contribute?
Contribute to 0G Blockchain
We welcome and encourage you to contribute to our open-source project!
Whether you're a seasoned developer or new to the community, your participation helps us push decentralized AI forward. Every contribution, no matter the size, makes a difference. Please check out our
Contribution Guidelines
to learn more about our criteria and how you can get involved. All submissions will be reviewed by a team member to ensure they meet our standards.
Let's build the future of decentralized AI together.


========== [ https://docs.0g.ai/resources/security ] ==========

Resources
Security
On this page
Security at 0G
At 0G, we prioritize the security and integrity of our platform. Our commitment to security is reflected in our rigorous audit processes and our active bug bounty program.
Audits
‚Äã
We regularly conduct thorough security audits of our smart contracts, protocols, and infrastructure to ensure the highest level of security for our users.
Recent Audits
‚Äã
Date
Auditor
Scope
Report
Aug 2024 - Sept 2024
Halborn
0G Storage
Report
Aug 2024
Zellic
0G Storage and 0G DA
Report
Aug 2025
Octane
0G Chain
Report
For a complete list of our audits and their detailed reports, please visit our
GitHub repository
.
0G Labs Bug Bounty Program with Hackenproof
‚Äã
At 0G, we believe in the power of
community-driven security
. Our bug bounty program invites security researchers and developers to help us identify and resolve potential vulnerabilities, ensuring the robustness of our systems.
Scope of the Bug Bounty Program
‚Äã
Our bug bounty program covers:
Smart Contracts
Infrastructure
Protocol
Focus Area
‚Äã
In-Scope Vulnerabilities:
‚Äã
We are interested in vulnerabilities that result in incorrect behavior of the smart contract and could lead to unintended functionality, including:
Stealing or loss of funds
Unauthorized transactions
Transaction manipulation
Attacks on logic (behavior that deviates from the intended business logic)
Reentrancy attacks
Reordering transactions
Overflows and underflows
Out-of-Scope Vulnerabilities:
‚Äã
The following are out of scope for the bug bounty program:
Theoretical vulnerabilities without proof or demonstration
Old compiler versions
Unlocked compiler version
Vulnerabilities in imported contracts
Code style guide violations
Redundant code
Gas optimizations
Best practice issues
Vulnerabilities exploitable through front-run attacks only
Additionally, the following contracts are out of scope for 0g-storage-contract:
cashier
token
reward/OnePoolReward
reward/ChunkDecayReward
uploadMarket
utils/Exponent.sol
Rewards
‚Äã
Rewards are based on the severity of the discovered vulnerability:
Severity
Reward Range
Critical
$35,000
High
$8000
Medium
$2000
Low
$500
Program Rules
‚Äã
Avoid using web application scanners for automatic vulnerability searching which generates massive traffic
Make every effort not to damage or restrict the availability of products, services, or infrastructure
Avoid compromising any personal data, interruption, or degradation of any service
Don‚Äôt access or modify other user data, localize all tests to your accounts
Perform testing only within the scope
Don‚Äôt exploit any DoS/DDoS vulnerabilities, social engineering attacks, or spam
Don‚Äôt spam forms or account creation flows using automated scanners
In case you find chain vulnerabilities we‚Äôll pay only for vulnerability with the highest severity.
Don‚Äôt break any law and stay in the defined scope
Any details of found vulnerabilities must not be communicated to anyone who is not a HackenProof Team or an authorized employee of this Company without appropriate permission
Disclosure Guidelines
‚Äã
important
Do not discuss this program or any vulnerabilities (even resolved ones) outside of the program without express consent from the organization
No vulnerability disclosure, including partial is allowed for the moment.
Please do NOT publish/discuss bugs
Eligibility and Coordinated Disclosure
‚Äã
We are happy to thank everyone who submits valid reports which help us improve the security. However, only those that meet the following eligibility requirements may receive a monetary reward:
You must be the first reporter of a vulnerability.
The vulnerability must be a qualifying vulnerability
Any vulnerability found must be reported no later than 24 hours after discovery and exclusively through hackenproof.com
You must send a clear textual description of the report along with steps to reproduce the issue, include attachments such as screenshots or proof of concept code as necessary.
You must not be a former or current employee of us or one of its contractors.
ONLY USE the EMAIL under which you registered your HackenProof account (in case of violation, no bounty can be awarded)
Provide detailed but to-the point reproduction steps
We look forward to working with the community to enhance 0G's security!


========== [ https://docs.0g.ai/resources/whitepaper ] ==========

Resources
0G Whitepaper
If you're unable to view the PDF, please
click here to download it
.
Download PDF


========== [ https://docs.0g.ai/run-a-node/archival-node ] ==========

Run a Node
Archival Node
On this page
Archival Node
Running an Archival node for the
0G-Galileo-Testnet
means providing complete historical data storage and access for the network, maintaining the full blockchain history and state.
What You'll Need
Linux system with sufficient disk space for archive data
lz4
compression tool installed
Public IP address for node connectivity
Stable internet connection
Hardware Requirements
‚Äã
Component
Requirement
Memory
64 GB
CPU
8 cores
Disk
Large NVME SSD (for full archive data)
Bandwidth
100 MBps for Download / Upload
Prerequisites
‚Äã
Required Files
‚Äã
Node Package
:
galileo-archive.tar.gz
Archive Snapshot
: Download from
https://chain-snapshot.oss-cn-hongkong.aliyuncs.com/snapshot/galileo/archive/20250717.tar.lz4
System Requirements
‚Äã
Linux system with sufficient disk space for archive data
lz4
compression tool installed
Public IP address for node connectivity
Setup Guide
‚Äã
1. Download Node Package
‚Äã
Download the node package:
galileo-archive.tar.gz
2. Extract Node Package
‚Äã
Unzip the file to your home directory
3. Download Archive Snapshot
‚Äã
Download the archive node snapshot from:
https://chain-snapshot.oss-cn-hongkong.aliyuncs.com/snapshot/galileo/archive/20250717.tar.lz4
4. Extract Snapshot
‚Äã
lz4 -d 20250717.tar.lz4 | tar -xvf - -C /your/snapshot/directory
Deployment Steps
‚Äã
1. Copy Files and Set Permissions
‚Äã
cd galileo-v1.2.0
cp -r 0g-home {your data path}
sudo chmod 777 ./bin/geth
sudo chmod 777 ./bin/0gchaind
2. Initialize Geth
‚Äã
./bin/geth init --state.scheme=hash --db.engine=pebble --datadir /{your data path}/0g-home/geth-home ./genesis.json
3. Initialize 0gchaind with Temporary Directory
‚Äã
./bin/0gchaind init {node name} --home /{your data path}/tmp
4. Copy Node Files to 0gchaind Home
‚Äã
cp /{your data path}/tmp/data/priv_validator_state.json /{your data path}/0g-home/0gchaind-home/data/
cp /{your data path}/tmp/config/node_key.json /{your data path}/0g-home/0gchaind-home/config/
cp /{your data path}/tmp/config/priv_validator_key.json /{your data path}/0g-home/0gchaind-home/config/
5. Copy Data from Snapshot
‚Äã
cp -r /your/snapshot/directory/0g-home/geth-home/geth/chaindata /{your data path}/0g-home/geth-home/geth/
cp -r /your/snapshot/directory/0g-home/0gchaind-home/data /{your data path}/0g-home/0gchaind-home/
6. Start 0gchaind
‚Äã
cd galileo-v1.2.0
nohup ./bin/0gchaind start \
--rpc.laddr tcp://0.0.0.0:26657 \
--chaincfg.chain-spec devnet \
--chaincfg.kzg.trusted-setup-path=kzg-trusted-setup.json \
--chaincfg.engine.jwt-secret-path=jwt-secret.hex \
--chaincfg.kzg.implementation=crate-crypto/go-kzg-4844 \
--chaincfg.block-store-service.enabled \
--chaincfg.node-api.enabled \
--chaincfg.node-api.logging \
--chaincfg.node-api.address 0.0.0.0:3500 \
--pruning=nothing \
--home /{your data path}/0g-home/0gchaind-home \
--p2p.seeds 85a9b9a1b7fa0969704db2bc37f7c100855a75d9@8.218.88.60:26656 \
--p2p.external_address {your node ip}:26656 > /{your data path}/0g-home/log/0gchaind.log 2>&1 &
7. Start Geth
‚Äã
cd galileo-v1.2.0
nohup ./bin/geth \
--config geth-archive-config.toml \
--nat extip:{your node ip} \
--bootnodes enode://de7b86d8ac452b1413983049c20eafa2ea0851a3219c2cc12649b971c1677bd83fe24c5331e078471e52a94d95e8cde84cb9d866574fec957124e57ac6056699@8.218.88.60:30303 \
--datadir /{your data path}/0g-home/geth-home \
--state.scheme=hash \
--gcmode archive \
--networkid 16601 > /{your data path}/0g-home/log/geth.log 2>&1 &
8. Verify Setup
‚Äã
Check the logs to ensure the node is running properly:
# Check Geth logs
tail -f /{your data path}/0g-home/log/geth.log
# Check 0gchaind logs
tail -f /{your data path}/0g-home/log/0gchaind.log
Success Indicators
0gchaind should show "Committed state" messages
Geth should show archive mode synchronization
No error messages in either log
Important Configuration Notes
‚Äã
Variables to Replace
‚Äã
{your data path}
: Your chosen data directory path
{node name}
: Your chosen node name
{your node ip}
: Your server's public IP address
/your/snapshot/directory
: Path where you extracted the snapshot
Directory Structure
‚Äã
After setup, your directory structure should look like:
{your data path}/
‚îî‚îÄ‚îÄ 0g-home/
‚îú‚îÄ‚îÄ geth-home/
‚îú‚îÄ‚îÄ 0gchaind-home/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ node_key.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ priv_validator_key.json
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îî‚îÄ‚îÄ priv_validator_state.json
‚îî‚îÄ‚îÄ log/
‚îú‚îÄ‚îÄ 0gchaind.log
‚îî‚îÄ‚îÄ geth.log
Network Ports
‚Äã
Ensure the following ports are open:
26657
: 0gchaind RPC
26656
: 0gchaind P2P
3500
: Node API
30303
: Geth network
Archive Node Benefits
‚Äã
Archive nodes provide several key benefits to the 0G network:
Complete Historical Data
: Full access to all historical blockchain data and state
Enhanced Query Capabilities
: Support for complex historical queries and analytics
Network Resilience
: Backup and redundancy for the network's historical data
Developer Support
: Essential for applications requiring historical blockchain data
Storage Requirements
Archive nodes require significantly more storage space than regular nodes as they maintain the complete blockchain history. Ensure adequate disk space before setup.


========== [ https://docs.0g.ai/run-a-node/community-docker-repo ] ==========

Run a Node
Community Docker Repository
On this page
Community Docker Repository
This section provides a list of Docker images üê≥ for 0G DA from the community. For instructions on running 0G nodes via binary installation, please visit the node pages directly.
For most users, Docker offers the simplest method to get 0G nodes up and running. Docker is a platform for containerization, allowing 0G nodes to operate in an isolated environment. This approach enables you to run 0G nodes on your system without needing to install and configure all the necessary dependencies manually.
Most of the officially endorsed 0G Docker implementations can be found under the documentation page for each 0G node type.
Below is a list of community-maintained Docker images for 0G DA. Please note that these images are not officially endorsed by 0G, and users should proceed with caution.
All Node Types
‚Äã
Ember Stake
Validator Node
‚Äã
CryptoWarden


========== [ https://docs.0g.ai/run-a-node/da-node ] ==========

Run a Node
Data Availability Node
On this page
Data Availability Node
While there are various approaches to running a DA (Data Availability) node, this guide outlines our recommended method and the necessary hardware specifications. DA Nodes perform the core functions of verifying, signing, and storing encoded blob data.
To operate effectively, your DA signer needs to run a DA node to verify encoded blob data, sign it, and store it for future farming and rewards. Currently, to run a DA Node on Testnet, users must stake 10 OG tokens. These can be obtained through our
faucet
or via rewards from running Storage Nodes or Validator Nodes. You can also reach out to our technical moderators on
Discord
.
Hardware Requirements
‚Äã
Node Type
Memory
CPU
Disk
Bandwidth
Additional Notes
DA Node
16 GB
8 cores
1 TB NVMe SSD
100 MBps
For Download/Upload
Standing up a DA Node and DA Signer
‚Äã
Run with Docker
Build from Source
Become a Signer
1. Clone the DA Node Repo:
git clone https://github.com/0gfoundation/0g-da-node.git
cd 0g-da-node
2. Generate BLS Private Key (if needed):
If you don't have a BLS private key, generate one:
cargo run --bin key-gen
Keep the generated BLS private key secure.
3. Set up config.toml:
Create a configuration file named
config.toml
in the project root directory.
Add the following content to the file, adjusting values as needed:
log_level = "info"
data_path = "/data"
# path to downloaded params folder
encoder_params_dir = "/params"
# grpc server listen address
grpc_listen_address = "0.0.0.0:34000"
# chain eth rpc endpoint
eth_rpc_endpoint = "https://evmrpc-testnet.0g.ai"
# public grpc service socket address to register in DA contract
# ip:34000 (keep same port as the grpc listen address)
# or if you have dns, fill your dns
socket_address = "<public_ip/dns>:34000"
# data availability contract to interact with
da_entrance_address = "0x857C0A28A8634614BB2C96039Cf4a20AFF709Aa9" # testnet config
# deployed block number of da entrance contract
start_block_number = 940000 # testnet config
# signer BLS private key
signer_bls_private_key = ""
# signer eth account private key
signer_eth_private_key = ""
# miner eth account private key, (could be the same as `signer_eth_private_key`, but not recommended)
miner_eth_private_key = ""
# whether to enable data availability sampling
enable_das = "true"
Make sure to fill in the
signer_bls_private_key
,
signer_eth_private_key
, and
miner_eth_private_key
fields with your actual private keys.
4. Build and Start the Docker Container:
docker build -t 0g-da-node .
docker run -d --name 0g-da-node 0g-da-node
5. Verify the Node is Running
On the first run, the DA node will register the signer information in the DA contract. You can monitor the console output to ensure the node is running correctly and has successfully registered.
Node Operations
‚Äã
As a DA node operator, your node will perform the following tasks:
Encoded blob data verification
Signing of verified data
Storing blob data for further farming
Receiving rewards for these operations
Troubleshooting
‚Äã
If you encounter any issues, check the console output for error messages.
Ensure that the ports specified in your
config.toml
file are not being used by other applications.
Verify that you have the latest stable version of Rust installed.
Make sure your system meets the minimum hardware requirements.
Conclusion
‚Äã
You have now successfully set up and run a 0g DA node as a DA Signer. For more advanced configuration options and usage instructions, please refer to the
Official GitHub repository
.
Remember to keep your private keys secure and regularly update your node software to ensure optimal performance and security.
Step 1: Clone and Build the Repository
‚Äã
Install dependencies:
sudo apt-get update && sudo apt-get install clang cmake build-essential pkg-config libssl-dev protobuf-compiler llvm llvm-dev
Clone the repository and checkout the specific version:
git clone https://github.com/0gfoundation/0g-da-node.git
cd 0g-da-node
Build the project:
cargo build --release
Download necessary parameters:
./dev_support/download_params.sh
Step 2: Generate BLS Private Key (if needed)
‚Äã
If you don't have a BLS private key, generate one:
cargo run --bin key-gen
Keep the generated BLS private key secure.
Step 3: Configure the Node
‚Äã
Create a configuration file named
config.toml
in the project root directory.
Add the following content to the file, adjusting values as needed:
log_level = "info"
data_path = "./db/"
# path to downloaded params folder
encoder_params_dir = "params/"
# grpc server listen address
grpc_listen_address = "0.0.0.0:34000"
# chain eth rpc endpoint
eth_rpc_endpoint = "https://evmrpc-testnet.0g.ai"
# public grpc service socket address to register in DA contract
# ip:34000 (keep same port as the grpc listen address)
# or if you have dns, fill your dns
socket_address = "<public_ip/dns>:34000"
# data availability contract to interact with
da_entrance_address = "0x857C0A28A8634614BB2C96039Cf4a20AFF709Aa9" # testnet config and see testnet page for the latest info
# deployed block number of da entrance contract
start_block_number = 940000 # testnet config
# signer BLS private key
signer_bls_private_key = ""
# signer eth account private key
signer_eth_private_key = ""
# miner eth account private key, (could be the same as `signer_eth_private_key`, but not recommended)
miner_eth_private_key = ""
# whether to enable data availability sampling
enable_das = "true"
Make sure to fill in the
signer_bls_private_key
,
signer_eth_private_key
, and
miner_eth_private_key
fields with your actual private keys.
Step 4: Run the Node
‚Äã
Start the 0g DA node using the following command:
./target/release/server --config config.toml
This will start the node using the configuration file you created.
Step 5: Verify the Node is Running
‚Äã
On the first run, the DA node will register the signer information in the DA contract. You can monitor the console output to ensure the node is running correctly and has successfully registered.
Node Operations
‚Äã
As a DA node operator, your node will perform the following tasks:
Encoded blob data verification
Signing of verified data
Storing blob data for further farming
Receiving rewards for these operations
Troubleshooting
‚Äã
If you encounter any issues, check the console output for error messages.
Ensure that the ports specified in your
config.toml
file are not being used by other applications.
Verify that you have the latest stable version of Rust installed.
Make sure your system meets the minimum hardware requirements.
Conclusion
‚Äã
You have now successfully set up and run a 0g DA node as a DA Signer. For more advanced configuration options and usage instructions, please refer to the
Official GitHub repository
.
Remember to keep your private keys secure and regularly update your node software to ensure optimal performance and security.
Overview
‚Äã
The DASigners contract is an interface through which Solidity contracts can interact with the 0G chain module DASigners. It is registered as a precompiled contract, similar to other precompiled EVM extensions.
Becoming a DA Signer
‚Äã
To become a DA signer, you must meet the following requirements:
Delegation Requirement: To become a signer, an address must receive enough delegations, equivalent to at least the TokensPerVote amount of OG tokens (30 tokens per vote in the testnet), registered in the DASigners module.
Node Operation: Each signer needs to run a DA (Data Availability) node that verifies blob encoding and generates BLS signatures for signed blobs.
Registration: Signers must register their information using the registerSigner function. This includes providing their address, node socket address, BLS public key, and a signature signed by their BLS private key.
Epoch Participation: Signers must submit a registration message (using the registerNextEpoch function) with a signature for each epoch they wish to participate in. This is necessary for joining quorums in the next epoch.
Voting Power: Each signer‚Äôs voting power is determined by the number of tokens delegated to them. Signers can have up to 1024 votes, and the votes are distributed randomly into quorums.
Quorum Responsibilities: Each signer in a quorum is responsible for validating, signing, and storing a specific row of encoded blob data during an epoch.
Prerequisites
‚Äã
Ensure you have the following installed on your system:
Git
Rust (latest stable version)
Cargo (comes with Rust)
Contract Details
‚Äã
Address
:
0x0000000000000000000000000000000000001000
Contract Params (Testnet)
‚Äã
TokensPerVote = 30
MaxVotesPerSigner = 1024
MaxQuorums = 10
EpochBlocks = 5760
EncodedSlices = 3072
Terminology
‚Äã
Signer
‚Äã
A Signer is an address with sufficient delegations (at least
TokensPerVote
OG) registered in the DASigners module. Each signer should run a DA node to verify DA blob encoding and generate BLS signatures for signed blobs. The BLS curve used is BN254, and the public keys of signers are registered in the contract.
Note
: For accounts with delegations to more than 10 validators, only 10 of these delegations are counted and accumulated.
Epoch
‚Äã
The consecutive blocks in the 0g chain are divided into groups of
EpochBlocks
, and each group is an epoch.
Quorum
‚Äã
In an epoch, there can be up to
MaxQuorums
quorums. Each quorum is a list of signer addresses with size
EncodedSlices
. The i-th signer in the quorum is responsible for validating, signing, and storing the i-th row of the encoded blob data assigned to this quorum.
Vote
‚Äã
Signers can submit their signatures on a registration message to request joining the quorums in the next epoch. At the start of each epoch, the DASigners module calculates the voting power for registered signers based on their delegated token amounts. Each delegated
TokensPerVote
OG counts as one vote, and each signer can have up to
MaxVotesPerSigner
votes. All votes are then randomly ordered and distributed into quorums.
Interface
‚Äã
Find the Solidity interface in the
0g-da-contract repo
.
ABI
‚Äã
Find the ABI in the
0g-chain repo
.
Transactions
‚Äã
registerSigner
‚Äã
Register signer's information, including signer address, DA node service socket address, signer BLS public key on G1 and G2 group, and a signature signed by the BLS private key of the following message:
Keccak256(signerAddress, chainID, "0G_BN254_Pubkey_Registration")
Here
chainID
is left-padded to 32 bytes by zeros.
function registerSigner(
SignerDetail memory _signer,
BN254.G1Point memory _signature
) external;
updateSocket
‚Äã
Update signer's socket address.
function updateSocket(string memory _socket) external;
registerNextEpoch
‚Äã
Register to join the quorums in the next epoch. The signer needs to submit a signature signed by their BLS private key:
Keccak256(signerAddress, epoch, chainID)
Here
chainID
is left-padded to 32 bytes by zeros and
epoch
is an unsigned 64-bit number in big-endian format.
function registerNextEpoch(BN254.G1Point memory _signature) external;
Queries
‚Äã
epochNumber
‚Äã
Get the current epoch number.
function epochNumber() external view returns (uint);
quorumCount
‚Äã
Get the number of quorums for a given epoch.
function quorumCount(uint _epoch) external view returns (uint);
isSigner
‚Äã
Check if a given address is a registered signer.
function isSigner(address _account) external view returns (bool);
getSigner
‚Äã
Get the information of given signers.
function getSigner(address[] memory _account) external view returns (SignerDetail[] memory);
getQuorum
‚Äã
Get the signer list of a given epoch and quorum id.
function getQuorum(uint _epoch, uint _quorumId) external view returns (address[] memory);
getQuorumRow
‚Äã
Get the signer of a specific row in a given epoch and quorum id.
function getQuorumRow(uint _epoch, uint _quorumId, uint32 _rowIndex) external view returns (address);
registeredEpoch
‚Äã
Check if a given address is registered to join the given epoch.
function registeredEpoch(address _account, uint _epoch) external view returns (bool);
getAggPkG1
‚Äã
Get the aggregated G1 public key for a given signers set. The signers set is specified by the epoch, quorum id, and a bitmap. The bitmap has
EncodedSlices
bits, and each bit denotes whether the row is chosen or not.
function getAggPkG1(
uint _epoch,
uint _quorumId,
bytes memory _quorumBitmap
) external view returns (BN254.G1Point memory aggPkG1, uint total, uint hit);


========== [ https://docs.0g.ai/run-a-node/overview ] ==========

Run a Node
On this page
Overview
Want to become an active participant in the 0G network and earn rewards while you're at it? üëá
Each node type plays a crucial role in maintaining the 0G network's functionality, from transaction validation and data storage to ensuring data availability and retrieval. Here, we'll introduce you to the various types of nodes you can run, each contributing to the network's health and security.
What Nodes Can I Run?
‚Äã
Validator Nodes
‚Äã
The guardians of the network, validator nodes are responsible for verifying transactions, ensuring consensus, and maintaining the blockchain. They're essential for keeping the 0G blockchain secure and running smoothly.
Storage Nodes
‚Äã
Unlike Validator Nodes that focus on securing the blockchain itself, Storage Nodes focus on managing and serving data. They are the backbone of the network's data storage capabilities, ensuring persistence and availability for long-term data storage (e.g., training datasets, large AI models). By running a storage node, you'll contribute to the decentralized storage of 0G data, making it accessible and resilient.
Data Availability Services
‚Äã
DA Nodes are similar to Storage Nodes but focus on immediacy and short-term accessibility to support real-time operations. This data is typically used by Layer 2 and rollup solutions for data availability and is not typically stored long-term. Think of these nodes as the network's librarians, ensuring that data can be quickly retrieved when needed.
Archival Nodes
‚Äã
Archival Nodes maintain complete historical blockchain data and state, providing comprehensive access to the network's entire history. These nodes are essential for applications requiring historical data analysis, compliance, and serving as reliable backups for the network's complete transaction history.
Why Run a Node?
‚Äã
Running a node isn't just about supporting the network; it's also a way to earn rewards for your contribution. By actively participating in the 0G ecosystem, you'll be eligible to receive rewards that incentivize your efforts.
Ready to Dive In?
‚Äã
We've made it easy to get started. The table below outlines the hardware requirements for each type of node, so you can choose the one that best suits your setup. Once you're ready, head over to the 0G documentation for detailed instructions on how to set up and run your chosen node.
Node Type
Description
Memory
CPU
Disk
Bandwidth
Validator Node
Validates transactions and maintains network consensus
64 GB
8 cores
1 TB NVME SSD (4 TB on Testnet)
100 MBps
Storage Node
Stores data within the 0g network
16 GB
4 cores
500GB / 1T NVME SSD
500 MBps
Storage KV
Handles key-value storage operations
4 GB
2 cores
Matches KV streams size
-
DA Node
Performs blob data verification, signing, and storage
16 GB
8 cores
1 TB NVME SSD
100 MBps
DA Retriever
Retrieves data availability information
8 GB
2 cores
-
100 MBps
DA Encoder*
Encodes data for availability purposes
-
-
-
-
DA Client
Interacts with the Data Availability layer
8 GB
2 cores
-
100 MBps
Note: For DA Encoder, GPU support is currently tested with NVIDIA 12.04 drivers on the RTX 4090. Other NVIDIA GPUs may require parameter adjustments and have not been tuned yet.
Next Steps
‚Äã
Ready to set up your node? Check out our detailed guides:
Validator Node Setup Guide
Storage Node Setup Guide
Data Availability Service Setup Guide
Archival Node Setup Guide


========== [ https://docs.0g.ai/run-a-node/storage-node ] ==========

Run a Node
Storage Node
On this page
Storage Node
In the 0G network, storage nodes play a vital role in maintaining the system's decentralized storage layer. They are responsible for storing and serving data, ensuring data availability and reliability across the network. By running a storage node, you actively contribute to the network and earn rewards for your participation.
This guide details the process of running a storage node, including hardware specifications and interaction with on-chain contracts.
Hardware Requirements
‚Äã
Component
Storage Node
Storage KV
Memory
32 GB RAM
32 GB RAM
CPU
8 cores
8 cores
Disk
500GB / 1TB SSD
Size matches the KV streams it maintains
Bandwidth
100 Mbps (Download / Upload)
-
note
For Storage Node: The SSD ensures fast read/write operations, critical for efficient blob storage and retrieval.
For Storage KV: The disk size requirement is flexible and should be adjusted based on the volume of KV streams you intend to maintain.
Next Steps
‚Äã
For detailed instructions on setting up and operating your Storage Node or Storage KV, please refer to our comprehensive setup guides below:
Storage Node
Storage KV Node
Prerequisites
‚Äã
Before setting up your storage node:
Understand that 0G Storage interacts with on-chain contracts for blob root confirmation and PoRA mining.
Choose your network:
Testnet
or
Mainnet
Check the respective network overview pages for deployed contract addresses and RPC endpoints.
For mainnet deployment
: Ensure you have real 0G tokens for transaction fees and mining rewards.
Install Dependencies
‚Äã
Start by installing all the essential tools and libraries required to build the 0G storage node software.
Linux
Mac
sudo apt-get update
sudo apt-get install clang cmake build-essential pkg-config libssl-dev protobuf-compiler
brew install llvm cmake
Install
rustup
: rustup is the Rust toolchain installer, necessary as the 0G node software is written in Rust.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Download the Source Code
:
git clone -b <latest_tag> https://github.com/0gfoundation/0g-storage-node.git
Build the Source Code
cd 0g-storage-node
# Build in release mode
cargo build --release
This compiles the Rust code into an executable binary. The
--release
flag optimizes the build for performance.
Setup and Configuration
‚Äã
Navigate to the run directory and configure your storage node for either testnet or mainnet.
Config File References
The official configuration files are in the
run/
directory. Currently only
turbo
is available:
run/config-testnet-turbo.toml
run/config-mainnet-turbo.toml
Always use the latest versions from the repository as they contain the most up-to-date network parameters.
Turbo vs Standard
Both
turbo
and
standard
configs are identical in structure and fields. The only difference is pricing; choose the file that matches the pricing tier you want to run.
Where The Full Config Is Explained
The most detailed, up-to-date comments for every field live in
run/config-testnet-turbo.toml
(and the corresponding
run/config-testnet-standard.toml
). Use those files as the authoritative field-by-field explanation.
Key Fields (Same Wording As
run/config-testnet-turbo.toml
)
network_boot_nodes
Note: List of nodes to bootstrap UDP discovery. Note,
network_enr_address
should be configured as well to enable UDP discovery.
log_contract_address
Note: Flow contract address to sync event logs.
mine_contract_address
Note: Mine contract address for incentive.
reward_contract_address
Note: Reward contract address.
db_max_num_sectors
Note: The max number of chunk entries to store in db. Each entry is 256B, so the db size is roughly limited to
256 * db_max_num_sectors
Bytes. If this limit is reached, the node will update its
shard_position
and store only half data.
chunk_pool_write_window_size
Note: Maximum number of threads to upload segments of a single file simultaneously.
chunk_pool_max_writings
Note: Maximum number of threads to upload segments for all files simultaneously.
auto_sync_enabled
Note: Enable file sync among peers automatically. When enabled, each node will store all files, and sufficient disk space is required.
neighbors_only
Note: Indicates whether to sync file from neighbor nodes only. This is to avoid flooding file announcements in the whole network, which leads to high latency or even timeout to sync files.
CLI Options
CLI flags override values in
config.toml
.
-c
,
--config <FILE>
: Sets a custom config file.
--log-config-file [FILE]
: Sets log configuration file (Default:
log_config
).
--miner-key [KEY]
: Sets miner private key (Default: None).
--blockchain-rpc-endpoint [URL]
: Sets blockchain RPC endpoint (Default:
http://127.0.0.1:8545
).
--db-max-num-chunks [NUM]
: Sets the max number of chunks to store in DB (Default: None).
--network-enr-address [URL]
: Sets the network ENR address (Default: None).
Configuration Reference
Full configuration keys are defined in
node/src/config/mod.rs
and log sync behavior is implemented in
node/log_entry_sync/src/sync_manager/config.rs
.
Network
network_dir
: Directory for node keyfile and network data (Default:
network
).
network_listen_address
: IP address to listen on (Default:
0.0.0.0
).
network_enr_address
: Public address advertised in ENR (Default: None).
network_enr_tcp_port
: TCP port advertised in ENR (Default:
1234
).
network_enr_udp_port
: UDP port advertised in ENR (Default:
1234
).
network_libp2p_port
: Libp2p TCP port (Default:
1234
).
network_discovery_port
: Discovery UDP port (Default:
1234
).
network_target_peers
: Target number of connected peers (Default:
50
).
network_boot_nodes
: ENR boot nodes list for discovery (Default: empty list).
network_libp2p_nodes
: Initial libp2p peers to connect to (Default: empty list).
network_private
: Enable private mode (Default:
false
).
network_disable_discovery
: Disable discovery protocol (Default:
false
).
network_find_chunks_enabled
: Enable find-chunks behavior (Default:
false
).
Discv5
discv5_request_timeout_secs
: Timeout per UDP request (Default:
5
).
discv5_query_peer_timeout_secs
: Timeout to mark a query peer unresponsive (Default:
2
).
discv5_request_retries
: Retry count for UDP requests (Default:
1
).
discv5_query_parallelism
: Parallelism per query (Default:
5
).
discv5_report_discovered_peers
: Emit discovered ENRs during traversal (Default:
false
).
discv5_disable_packet_filter
: Disable incoming packet filter (Default:
false
).
discv5_disable_ip_limit
: Disable /24 subnet limit in kbuckets (Default:
false
).
discv5_disable_enr_network_id
: Disable ENR network ID checks (Default:
false
).
Log Sync
blockchain_rpc_endpoint
: RPC endpoint to sync EVM logs (Default:
http://127.0.0.1:8545
).
log_contract_address
: Flow contract address to sync logs from (Default: empty).
log_sync_start_block_number
: Block number to start syncing logs (Default:
0
).
force_log_sync_from_start_block_number
: Force sync from start block even if progress exists (Default:
false
).
confirmation_block_count
: Blocks required for confirmation to handle reorgs (Default:
3
).
log_page_size
: Max number of logs per poll (Default:
999
).
max_cache_data_size
: Max cached data size in bytes (Default:
104857600
).
cache_tx_seq_ttl
: Cache TTL for tx sequence entries (Default:
500
).
rate_limit_retries
: Retries after RPC timeouts (Default:
100
).
timeout_retries
: Retries for rate-limited responses (Default:
100
).
initial_backoff
: Initial backoff in ms before retry (Default:
500
).
recover_query_delay
: Delay in ms between paginated getLogs calls (Default:
50
).
default_finalized_block_count
: Finality lag assumed behind latest block (Default:
100
).
remove_finalized_block_interval_minutes
: Interval in minutes to prune finalized blocks (Default:
30
).
watch_loop_wait_time_ms
: Watch loop delay in ms (Default:
500
).
blockchain_rpc_timeout_secs
: RPC connect/read timeout in seconds (Default:
120
).
Chunk Pool
chunk_pool_write_window_size
: Max threads per file upload (Default:
4
).
chunk_pool_max_cached_chunks_all
: Max cached chunk bytes across all files (Default:
4194304
).
chunk_pool_max_writings
: Max concurrent file uploads (Default:
16
).
chunk_pool_expiration_time_secs
: Cached chunk expiration in seconds (Default:
300
).
Database
db_dir
: Directory to store data (Default:
db
).
db_max_num_sectors
: Max number of chunk entries to store (Default: None).
prune_check_time_s
: Interval to check prune conditions in seconds (Default:
60
).
prune_batch_size
: Number of entries per prune batch (Default:
16384
).
prune_batch_wait_time_ms
: Wait between prune batches in ms (Default:
1000
).
merkle_node_cache_capacity
: Merkle node cache capacity in bytes (Default:
33554432
).
Misc
log_config_file
: Log configuration file name (Default:
log_config
).
log_directory
: Directory for log output (Default:
log
).
Mining
mine_contract_address
: Mine contract address (Default: empty).
miner_id
: Optional miner ID (Default: None).
miner_key
: Miner private key (Default: None).
miner_cpu_percentage
: CPU usage percentage for mining (Default:
100
).
mine_iter_batch_size
: Mining iteration batch size (Default:
100
).
reward_contract_address
: Reward contract address (Default: empty).
shard_position
: Shard selector in
<shard_id>/<shard_number>
format (Default: None).
mine_context_query_seconds
: Interval to query mine context in seconds (Default:
5
).
Testnet
Mainnet
Configuration
‚Äã
Copy the testnet configuration:
cd run
cp config-testnet-turbo.toml config.toml
Update the following fields in
config.toml
:
# Contract addresses for testnet
log_contract_address = "FLOW_CONTRACT_ADDRESS"
mine_contract_address = "MINE_CONTRACT_ADDRESS"
# Testnet RPC endpoint
blockchain_rpc_endpoint = "https://evmrpc-testnet.0g.ai"
# Start sync block number for testnet
log_sync_start_block_number = 1
# Reward contract for testnet
reward_contract_address = "REWARD_CONTRACT_ADDRESS"
# Your private key for mining (64 chars, no '0x' prefix)
miner_key = "YOUR_PRIVATE_KEY"
Optional: Configure network settings if needed:
# Target number of connected peers (can be increased for better connectivity)
network_target_peers = 50
Sharding Configuration
‚Äã
Sharding allows you to control how much data your storage node stores. This is particularly useful when disk space is limited.
Understanding Shard Position
‚Äã
The
shard_position
parameter determines which portion of the total network data your node stores:
# Format: shard_id/shard_number where shard_number is 2^n
# This only applies if there is no stored shard config in db
shard_position = "0/2"
Format
:
<shard_id>/<shard_number>
shard_id
: Which shard this node stores (0, 1, 2, 3, etc.)
shard_number
: Total number of shards (must be a power of 2: 2, 4, 8, 16, etc.)
Examples
:
shard_position = "0/2"
‚Üí Store 50% of data (shard 0 of 2 total shards)
shard_position = "1/2"
‚Üí Store 50% of data (shard 1 of 2 total shards)
shard_position = "0/4"
‚Üí Store 25% of data (shard 0 of 4 total shards)
shard_position = "2/4"
‚Üí Store 25% of data (shard 2 of 4 total shards)
Each shard stores a
specific range
of the total data. For example, with
0/2
and
1/2
, both store 50% of data but on different, non-overlapping ranges.
How Sharding Works
‚Äã
Consider a scenario with 128 GB total network data and a 100 GB disk:
Initial Setup
(
shard_position = "0/2"
):
Your node stores 64 GB (50% of 128 GB)
Stores a specific, deterministic data range
Fits comfortably on your 100 GB disk
Network Growth
(total data becomes 256 GB):
Your node would be responsible for 128 GB (50% of 256 GB)
This exceeds your 100 GB disk capacity
Automatic adjustment
: Node automatically splits to
x/4
Now stores 64 GB (25% of 256 GB)
shard_id
changes:
0
‚Üí
0 or 2
,
1
‚Üí
1 or 3
(randomly assigned by the node, you cannot control which)
Automatic Shard Division
‚Äã
When network data exceeds your disk capacity, the node automatically:
Doubles the shard number (2 ‚Üí 4 ‚Üí 8 ‚Üí 16, etc.)
Randomly reassigns to a new shard within the split (you cannot control which)
Maintains storage within disk limits
Important Notes
Initial setup is deterministic
: When you first configure
shard_position
, the shard_id deterministically maps to a specific data range
This setting only applies on
first startup
if no shard config exists in the database
Auto-splitting is NOT deterministic
: When the node automatically splits shards due to capacity limits, you cannot control which new shard_id you get - it's randomly assigned
Once shard config is stored in the database, the node manages all future shard adjustments automatically
Choosing Your Shard Configuration
‚Äã
To determine the right shard configuration:
Estimate network data size
: Check current total data on the network
Consider disk capacity
: Leave headroom for growth (e.g., use 70-80% of disk)
Calculate shard number
:
shard_number = total_data / (disk_capacity * 0.75)
Round up to nearest power of 2
: 2, 4, 8, 16, 32, etc.
Example
:
Network data: 500 GB
Your disk: 200 GB
Safe storage: 150 GB (75% of disk)
Calculation: 500 / 150 ‚âà 3.33 ‚Üí Round up to 4
Configuration:
shard_position = "0/4"
(stores ~125 GB)
Running the Node
‚Äã
Check configuration options:
../target/release/zgs_node -h
Run the testnet storage service:
cd run
../target/release/zgs_node --config config.toml --miner-key <your_private_key>
Configuration
‚Äã
Copy the mainnet configuration:
cd run
cp config-mainnet-turbo.toml config.toml
Update the following fields in
config.toml
:
# Contract addresses for mainnet
log_contract_address = "FLOW_CONTRACT_ADDRESS"
mine_contract_address = "MINE_CONTRACT_ADDRESS"
reward_contract_address = "REWARD_CONTRACT_ADDRESS"
# Mainnet RPC endpoint
blockchain_rpc_endpoint = "https://evmrpc.0g.ai"
# Start sync block number for mainnet
log_sync_start_block_number = 2387557
# Your private key for mining (64 chars, no '0x' prefix)
miner_key = "YOUR_PRIVATE_KEY"
The mainnet configuration includes predefined boot nodes for network connectivity:
network_boot_nodes = ["/ip4/34.66.131.173/udp/1234/p2p/16Uiu2HAmG81UgZ1JJLx9T2HqELgJNP36ChHzYkCdA9HdxvAbb5jQ","/ip4/34.60.163.4/udp/1234/p2p/16Uiu2HAmL3DoA7e7mbxs7CkeCPtNrAcfJFFtLpJDr2HWuR6QwJ8k","/ip4/34.169.236.186/udp/1234/p2p/16Uiu2HAm489RdhEgZUFmNTR4jdLEE4HjrvwaPCkEpSYSgvqi1CbR","/ip4/34.71.110.60/udp/1234/p2p/16Uiu2HAmBfGfbLNRegcqihiuXhgSXWNpgiGm6EwW2SYexfPUNUHQ"]
Sharding Configuration
‚Äã
Sharding allows you to control how much data your storage node stores. This is particularly useful when disk space is limited.
Understanding Shard Position
‚Äã
The
shard_position
parameter determines which portion of the total network data your node stores:
# Format: shard_id/shard_number where shard_number is 2^n
# This only applies if there is no stored shard config in db
shard_position = "0/2"
Format
:
<shard_id>/<shard_number>
shard_id
: Which shard this node stores (0, 1, 2, 3, etc.)
shard_number
: Total number of shards (must be a power of 2: 2, 4, 8, 16, etc.)
Examples
:
shard_position = "0/2"
‚Üí Store 50% of data (shard 0 of 2 total shards)
shard_position = "1/2"
‚Üí Store 50% of data (shard 1 of 2 total shards)
shard_position = "0/4"
‚Üí Store 25% of data (shard 0 of 4 total shards)
shard_position = "2/4"
‚Üí Store 25% of data (shard 2 of 4 total shards)
Each shard stores a
specific range
of the total data. For example, with
0/2
and
1/2
, both store 50% of data but on different, non-overlapping ranges.
How Sharding Works
‚Äã
Consider a scenario with 128 GB total network data and a 100 GB disk:
Initial Setup
(
shard_position = "0/2"
):
Your node stores 64 GB (50% of 128 GB)
Stores a specific, deterministic data range
Fits comfortably on your 100 GB disk
Network Growth
(total data becomes 256 GB):
Your node would be responsible for 128 GB (50% of 256 GB)
This exceeds your 100 GB disk capacity
Automatic adjustment
: Node automatically splits to
x/4
Now stores 64 GB (25% of 256 GB)
shard_id
changes:
0
‚Üí
0 or 2
,
1
‚Üí
1 or 3
(randomly assigned by the node, you cannot control which)
Automatic Shard Division
‚Äã
When network data exceeds your disk capacity, the node automatically:
Doubles the shard number (2 ‚Üí 4 ‚Üí 8 ‚Üí 16, etc.)
Randomly reassigns to a new shard within the split (you cannot control which)
Maintains storage within disk limits
Important Notes
Initial setup is deterministic
: When you first configure
shard_position
, the shard_id deterministically maps to a specific data range
This setting only applies on
first startup
if no shard config exists in the database
Auto-splitting is NOT deterministic
: When the node automatically splits shards due to capacity limits, you cannot control which new shard_id you get - it's randomly assigned
Once shard config is stored in the database, the node manages all future shard adjustments automatically
Choosing Your Shard Configuration
‚Äã
To determine the right shard configuration:
Estimate network data size
: Check current total data on the network
Consider disk capacity
: Leave headroom for growth (e.g., use 70-80% of disk)
Calculate shard number
:
shard_number = total_data / (disk_capacity * 0.75)
Round up to nearest power of 2
: 2, 4, 8, 16, 32, etc.
Example
:
Network data: 500 GB
Your disk: 200 GB
Safe storage: 150 GB (75% of disk)
Calculation: 500 / 150 ‚âà 3.33 ‚Üí Round up to 4
Configuration:
shard_position = "0/4"
(stores ~125 GB)
Running the Node
‚Äã
Check configuration options:
../target/release/zgs_node -h
Run the mainnet storage service:
cd run
../target/release/zgs_node --config config.toml --miner-key <your_private_key>
Important Mainnet Notes
:
Ensure your miner key has sufficient 0G tokens for transaction fees
Mainnet nodes should have stable internet connectivity and sufficient bandwidth
Monitor your node's performance and logs regularly
Snapshot
‚Äã
Make sure to only include
flow_db
and delete
data_db
under
db
folder when you use a snapshot from a 3rd party !
Using others'
data_db
will make the node mine for others!
Additional Notes
Security:
Keep your private key (
miner_key
) safe and secure. Anyone with access to it can control your node and potentially claim your mining rewards.
Network Connectivity:
Ensure your node has a stable internet connection and that the necessary ports are open for communication with other nodes.
Monitoring:
Monitor your node's logs and resource usage to ensure it's running smoothly.
Updates:
Stay informed about updates to the 0G storage node software and follow the project's documentation for any changes in the setup process.
Remember:
Running a storage node is a valuable contribution to the 0G network. You'll be helping to maintain its decentralization and robustness while earning rewards for your efforts.
Overview
‚Äã
0G Storage KV is a key-value store built on top of the 0G Storage system. This guide provides detailed steps to deploy and run a 0G Storage KV node.
Prerequisites
‚Äã
Before setting up your 0G Storage KV node:
Understand that 0G KV interacts with on-chain contracts and storage nodes to simulate KV data streams.
For official deployed contract addresses, visit the
testnet information page
.
Install Dependencies
‚Äã
Follow the same steps to install dependencies and Rust as in the storage node setup:
linux
mac
sudo apt-get update
sudo apt-get install clang cmake build-essential pkg-config libssl-dev protobuf-compiler
brew install llvm cmake
Install
rustup
: rustup is the Rust toolchain installer, necessary as the 0G node software is written in Rust.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
1. Download the Source Code
‚Äã
git clone -b <latest_tag> https://github.com/0gfoundation/0g-storage-kv.git
2. Build the Source Code
‚Äã
cd 0g-storage-kv
# Build in release mode
cargo build --release
Configuration
‚Äã
Copy the example configuration file and update it:
cp config_example.toml config.toml
nano config.toml
Update the following fields in
config.toml
:
#######################################################################
###                   Key-Value Stream Options                      ###
#######################################################################
# Streams to monitor.
stream_ids = ["000000000000000000000000000000000000000000000000000000000000f2bd", "000000000000000000000000000000000000000000000000000000000000f009", "0000000000000000000000000000000000000000000000000000000000016879", "0000000000000000000000000000000000000000000000000000000000002e3d"]
#######################################################################
###                     DB Config Options                           ###
#######################################################################
# Directory to store data.
db_dir = "db"
# Directory to store KV Metadata.
kv_db_dir = "kv.DB"
#######################################################################
###                     Log Sync Config Options                     ###
#######################################################################
blockchain_rpc_endpoint = "BLOCKCHAIN_RPC_ENDPOINT" #rpc endpoint, see testnet information
log_contract_address = "LOG_CONTRACT_ADDRESS" #flow contract address, see testnet information
# log_sync_start_block_number should be earlier than the block number of the first transaction that writes to the stream being monitored.
log_sync_start_block_number = 0
#######################################################################
###                     RPC Config Options                          ###
#######################################################################
# Whether to provide RPC service.
rpc_enabled = true
# HTTP server address to bind for public RPC.
rpc_listen_address = "0.0.0.0:6789"
# Zerog storage nodes to download data from.
zgs_node_urls = "http://127.0.0.1:5678,http://127.0.0.1:5679"
#######################################################################
###                     Misc Config Options                         ###
#######################################################################
log_config_file = "log_config"
Running the Storage KV Node
‚Äã
Navigate to the
run
directory:
cd run
Run the KV service:
../target/release/zgs_kv --config config.toml
For long-running sessions, consider using
tmux
or
screen
to run the node in the background.
Monitoring and Maintenance
‚Äã
Check logs:
The node outputs logs based on the
log_config
file specified in your configuration.
Updating the node:
To update to the latest version, pull the latest changes from the repository and rebuild:
git pull
cargo build --release
Troubleshooting
‚Äã
If you encounter issues:
Check the logs for any error messages.
Ensure your node meets the hardware requirements.
Verify that your
config.toml
file is correctly formatted and contains valid settings.
Check your internet connection and firewall settings.
Ensure the specified blockchain RPC endpoint and contract addresses are correct and accessible.
Getting Help
‚Äã
If you need assistance:
Check the
GitHub Issues
for known problems and solutions.
Join the 0G community channels (Discord, Telegram, etc.) for community support.
For critical issues, consider reaching out to the 0G team directly.
Conclusion
‚Äã
Running a 0G Storage KV node is an important part of the 0G ecosystem, providing key-value storage capabilities. By following this guide, you should be able to set up and operate your node successfully. Remember to keep your node updated and monitor its performance regularly to ensure optimal operation.


========== [ https://docs.0g.ai/run-a-node/validator-node ] ==========

Run a Node
Validator Node
On this page
Validator Node
Running a Validator node means providing validator services for the network, processing transactions and maintaining consensus.
What You'll Need
Linux/macOS system with adequate hardware
Stable internet connection
Ethereum RPC endpoint (mainnet for Aristotle, HoleSky testnet for Galileo)
Hardware Requirements
‚Äã
Component
Mainnet (Aristotle)
Testnet (Galileo)
Memory
64 GB
64 GB
CPU
8 cores
8 cores
Disk
1 TB NVME SSD
4 TB NVME SSD
Bandwidth
100 MBps for Download / Upload
100 MBps for Download / Upload
Restaking RPC Configuration
‚Äã
Validator Nodes
: When running your consensus client, add the following flags to enable restaking and configure the Symbiotic RPC:
--chaincfg.restaking.enabled \
--chaincfg.restaking.symbiotic-rpc-dial-url ${ETH_RPC_URL} \
--chaincfg.restaking.symbiotic-get-logs-block-range ${BLOCK_NUM}
ETH_RPC_URL
: The RPC endpoint for the Symbiotic network.
Mainnet (Aristotle)
: Use an Ethereum Mainnet RPC endpoint (e.g.,
https://eth-mainnet.g.alchemy.com/v2/YOUR_API_KEY
)
Testnet (Galileo)
: Use an Ethereum HoleSky RPC endpoint
BLOCK_NUM
: The maximum block number range per call when syncing restaking events. Default is 1. Adjust based on your RPC provider limits.
Non-Validator Nodes
: No restaking-related configuration is required; you can keep your current startup parameters unchanged.
This enables staking in Symbiotic contracts on Ethereum (mainnet: Ethereum, testnet: HoleSky) to participate in 0G Chain consensus. Validators must be able to read the Ethereum contract state to generate and verify new blocks. You can run your own node or use a third-party RPC provider such as QuickNode or Infura for
${ETH_RPC_URL}
.
Non-Validator Nodes
Restaking configuration is NOT required for non-validator nodes. Do not add the
--chaincfg.restaking.*
flags when running non-validator nodes.
Mainnet (Aristotle)
Testnet (Galileo)
Mainnet (Aristotle) Setup Guide
‚Äã
1. Download Package
‚Äã
Download the latest Aristotle mainnet package:
wget -O aristotle.tar.gz https://github.com/0gfoundation/0gchain-Aristotle/releases/download/v1.0.4/Aristotle-v1.0.4.tar.gz
Version Information
Latest Aristotle mainnet release: v1.0.4. Check
releases page
for newer versions.
2. Extract Package
‚Äã
Extract the Aristotle node package to your home directory:
tar -xzvf Aristotle-v1.0.4.tar.gz -C ~
3. Create Data Directory and Copy Configuration
‚Äã
Create your data directory and copy the default configuration:
cd Aristotle-v1.0.4
cp -r 0g-home {your data path}
sudo chmod 777 ./bin/geth
sudo chmod 777 ./bin/0gchaind
4. Initialize Geth
‚Äã
Initialize the Geth execution client with the genesis configuration:
./bin/geth init --datadir {your data path}/0g-home/geth-home ./geth-genesis.json
Expected output:
"Successfully wrote genesis state"
5. Initialize 0gchaind for Mainnet
‚Äã
Create a temporary initialization with the mainnet chain specification:
./bin/0gchaind init {node name} --home {your data path}/tmp --chaincfg.chain-spec mainnet
‚ö†Ô∏è
Important:
The
--chaincfg.chain-spec mainnet
flag is REQUIRED for validators
6. Copy Node Keys and Validator Keys
‚Äã
Copy all necessary keys to the permanent directory:
cp {your data path}/tmp/data/priv_validator_state.json {your data path}/0g-home/0gchaind-home/data/
cp {your data path}/tmp/config/node_key.json {your data path}/0g-home/0gchaind-home/config/
cp {your data path}/tmp/config/priv_validator_key.json {your data path}/0g-home/0gchaind-home/config/
7. Generate JWT Authentication Token
‚Äã
Generate a JWT token with mainnet specification for secure communication:
./bin/0gchaind jwt generate --home {your data path}/0g-home/0gchaind-home --chaincfg.chain-spec mainnet
cp -f {your data path}/0g-home/0gchaind-home/config/jwt.hex ./
8. Configure Node Name
‚Äã
Update the node moniker in the configuration file:
sed -i 's/moniker = "0G-mainnet-aristotle-node"/moniker = "{your node name}"/' {your data path}/0g-home/0gchaind-home/config/config.toml
9. Verify Configuration Files
‚Äã
Ensure all required configuration files are present:
ls -la {your data path}/0g-home/0gchaind-home/config/
Should display:
app.toml
client.toml
config.toml
genesis.json
jwt.hex
node_key.json
priv_validator_key.json
10. Set Environment Variables
‚Äã
Configure required environment variables for Symbiotic restaking:
# Set your Ethereum RPC endpoint (mainnet)
export ETH_RPC_URL="https://eth-mainnet.g.alchemy.com/v2/YOUR_API_KEY"
# Set block range for syncing (adjust based on your RPC limits)
export BLOCK_NUM=1
11. Start 0gchaind
‚Äã
Launch the 0gchaind consensus client with validator-specific parameters:
cd Aristotle-v1.0.4
nohup ./bin/0gchaind start \
--rpc.laddr tcp://0.0.0.0:26657 \
--chaincfg.kzg.trusted-setup-path=kzg-trusted-setup.json \
--chaincfg.engine.jwt-secret-path=jwt.hex \
--chaincfg.block-store-service.enabled \
--chaincfg.restaking.enabled \
--chaincfg.restaking.symbiotic-rpc-dial-url ${ETH_RPC_URL} \
--chaincfg.restaking.symbiotic-get-logs-block-range ${BLOCK_NUM} \
--home {your data path}/0g-home/0gchaind-home \
--p2p.external_address {your node ip}:26656 > {your data path}/0g-home/log/0gchaind.log 2>&1 &
Validator-Specific Parameters:
--chaincfg.restaking.enabled
: Enables restaking functionality
--chaincfg.restaking.symbiotic-rpc-dial-url
: Ethereum RPC for Symbiotic protocol
--chaincfg.restaking.symbiotic-get-logs-block-range
: Block range per sync call
12. Start Geth
‚Äã
Launch the Geth execution client:
cd Aristotle-v1.0.4
nohup ./bin/geth \
--config geth-config.toml \
--nat extip:{your node ip} \
--datadir {your data path}/0g-home/geth-home \
--networkid 16661 > {your data path}/0g-home/log/geth.log 2>&1 &
13. Verify Node Status
‚Äã
Check that both services are running correctly:
# Check 0gchaind logs
tail -f {your data path}/0g-home/log/0gchaind.log
# Check geth logs
tail -f {your data path}/0g-home/log/geth.log
Testnet (Galileo) Setup Guide
‚Äã
1. Download Package
‚Äã
Download the latest Galileo testnet package:
wget -O galileo.tar.gz https://github.com/0gfoundation/0gchain-NG/releases/download/v3.0.4/Galileo-v3.0.4.tar.gz
Version Information
Latest Galileo testnet release: v3.0.4. Check
releases page
for newer versions.
2. Extract Package
‚Äã
Extract the package to your home directory:
tar -xzvf Galileo-v3.0.4.tar.gz -C ~
3. Create Data Directory and Copy Configuration
‚Äã
Copy the configuration files and set proper permissions:
cd Galileo-v3.0.4
cp -r 0g-home {your data path}
sudo chmod 777 ./bin/geth
sudo chmod 777 ./bin/0gchaind
4. Initialize Geth
‚Äã
Initialize the Geth execution client with the genesis file:
./bin/geth init --datadir {your data path}/0g-home/geth-home ./geth-genesis.json
Expected output:
"Successfully wrote genesis state"
5. Initialize 0gchaind for Testnet
‚Äã
Create a temporary directory for initial configuration with testnet chain specification:
./bin/0gchaind init {node name} --home {your data path}/tmp --chaincfg.chain-spec testnet
‚ö†Ô∏è
Important:
The
--chaincfg.chain-spec testnet
flag is REQUIRED for testnet validators
6. Generate JWT Authentication Token
‚Äã
Generate a JWT token with testnet specification for secure communication:
./bin/0gchaind jwt generate --home {your data path}/0g-home/0gchaind-home --chaincfg.chain-spec testnet
cp -f {your data path}/0g-home/0gchaind-home/config/jwt.hex ./
7. Copy Node Files
‚Äã
Move the generated keys to the proper location:
cp {your data path}/tmp/data/priv_validator_state.json {your data path}/0g-home/0gchaind-home/data/
cp {your data path}/tmp/config/node_key.json {your data path}/0g-home/0gchaind-home/config/
cp {your data path}/tmp/config/priv_validator_key.json {your data path}/0g-home/0gchaind-home/config/
Note: The temporary directory can be deleted after this step.
8. Set Environment Variables
‚Äã
Configure required environment variables for Symbiotic restaking:
# Set your Ethereum HoleSky RPC endpoint (testnet)
export ETH_RPC_URL="https://holesky-rpc.g.alchemy.com/v2/YOUR_API_KEY"
# Set block range for syncing (adjust based on your RPC limits)
export BLOCK_NUM=1
9. Start 0gchaind
‚Äã
Launch the 0gchaind consensus client with testnet parameters:
cd ~/Galileo-v3.0.4
nohup ./bin/0gchaind start \
--rpc.laddr tcp://0.0.0.0:26657 \
--chaincfg.chain-spec testnet \
--chaincfg.kzg.trusted-setup-path=kzg-trusted-setup.json \
--chaincfg.engine.jwt-secret-path=jwt.hex \
--chaincfg.block-store-service.enabled \
--chaincfg.restaking.enabled \
--chaincfg.restaking.symbiotic-rpc-dial-url ${ETH_RPC_URL} \
--chaincfg.restaking.symbiotic-get-logs-block-range ${BLOCK_NUM} \
--home {your data path}/0g-home/0gchaind-home \
--p2p.external_address {your node ip}:26656 > {your data path}/0g-home/log/0gchaind.log 2>&1 &
10. Start Geth
‚Äã
Launch the Geth execution client:
cd ~/Galileo-v3.0.4
nohup ./bin/geth \
--config geth-config.toml \
--nat extip:{your node ip} \
--datadir {your data path}/0g-home/geth-home \
--networkid 16602 > {your data path}/0g-home/log/geth.log 2>&1 &
11. Verify Setup
‚Äã
Check the logs to confirm your node is running properly:
# Check 0gchaind logs
tail -f {your data path}/0g-home/log/0gchaind.log
# Check geth logs
tail -f {your data path}/0g-home/log/geth.log
Success Indicators
0gchaind should show "Committed state" messages
No error messages in either log
Validator is participating in consensus
Validator Operations
‚Äã
Initialize Your Validator
‚Äã
Once your validator node is running and fully synced (
catching_up: false
), you need to initialize your validator on the blockchain to start validating transactions.
Next Step:
Follow the
Validator Initialization Guide
to:
Generate validator signature
Prepare validator description and settings
Execute the initialization transaction
Verify your validator activation (typically 30-60 minutes)
Monitor Consensus Participation
‚Äã
# Check if your validator is in the active set
curl http://localhost:26657/validators | jq
Check Sync Status
‚Äã
# Should show "catching_up": false when fully synced
curl http://localhost:26657/status | jq '.result.sync_info'
Key Management
‚Äã
‚ö†Ô∏è
Critical Security Notice:
Backup your validator keys immediately
:
priv_validator_key.json
and
node_key.json
Never share your private validator key
with anyone
Store backups in multiple secure locations
Test recovery process in a non-production environment first
Backup & Recovery
These files are essential for validator recovery and must be backed up securely:
# Essential validator keys
/{your data path}/0g-home/0gchaind-home/config/
Recovery Process
‚Äã
To restore your validator from backup:
Stop running services
:
pkill 0gchaind
pkill geth
Restore key files
:
cp ~/validator-backup/node_key.json /{your data path}/0g-home/0gchaind-home/config/
cp ~/validator-backup/priv_validator_key.json /{your data path}/0g-home/0gchaind-home/config/
Restart services
following the appropriate setup guide steps.
Upgrade Validator
Step 1: Extract New Release
‚Äã
# For Testnet (Galileo)
wget -O galileo.tar.gz https://github.com/0gfoundation/0gchain-NG/releases/download/v3.0.4/Galileo-v3.0.4.tar.gz
tar -xzvf Galileo-v3.0.4.tar.gz -C ~
# For Mainnet (Aristotle)
wget -O aristotle.tar.gz https://github.com/0gfoundation/0gchain-Aristotle/releases/download/v1.0.4/Aristotle-v1.0.4.tar.gz
tar -xzvf Aristotle-v1.0.4.tar.gz -C ~
# Verify extraction
ls -la {network}-v{version}/
Step 2: Stop Services
‚Äã
# Stop consensus layer (0gchaind)
pkill 0gchaind
# Stop execution layer (geth)
pkill geth
Step 3: Backup Your Data
‚Äã
# Create backup directory with timestamp
BACKUP_DIR="backup-$(date +%Y%m%d-%H%M%S)"
mkdir -p $BACKUP_DIR
# Backup execution layer data (geth-home)
cp -r {your_geth_datadir} $BACKUP_DIR/geth-backup
# Backup consensus layer data (0gchaind-home)
cp -r {your_0gchaind_home} $BACKUP_DIR/0gchaind-backup
Step 4: Start Node
‚Äã
If you get error while starting node due to missing
priv_validator_state.json
, create an empty
priv_validator_state.json
file in that directory with
{}
.
For testnet (Galileo), use
--chaincfg.chain-spec testnet
:
nohup ./bin/0gchaind start \
--rpc.laddr tcp://0.0.0.0:26657 \
--chaincfg.chain-spec testnet \
--chaincfg.restaking.enabled \
--chaincfg.restaking.symbiotic-rpc-dial-url ${ETH_RPC_URL} \
--chaincfg.restaking.symbiotic-get-logs-block-range ${BLOCK_NUM} \
--chaincfg.kzg.trusted-setup-path=kzg-trusted-setup.json \
--chaincfg.engine.jwt-secret-path=jwt.hex \
--chaincfg.block-store-service.enabled \
--home {your_cl_home} \
--p2p.external_address {your_node_ip}:26656 > {your_log_path}/0gchaind.log 2>&1 &
For mainnet (Aristotle), use
--chaincfg.chain-spec mainnet
:
nohup ./bin/0gchaind start \
--rpc.laddr tcp://0.0.0.0:26657 \
--chaincfg.chain-spec mainnet \
--chaincfg.restaking.enabled \
--chaincfg.restaking.symbiotic-rpc-dial-url ${ETH_RPC_URL} \
--chaincfg.restaking.symbiotic-get-logs-block-range ${BLOCK_NUM} \
--chaincfg.kzg.trusted-setup-path=kzg-trusted-setup.json \
--chaincfg.engine.jwt-secret-path=jwt.hex \
--chaincfg.block-store-service.enabled \
--home {your_cl_home} \
--p2p.external_address {your_node_ip}:26656 > {your_log_path}/0gchaind.log 2>&1 &
Then start Geth:
nohup ./bin/geth --config geth-config.toml \
--nat extip:{your_node_ip} \
--datadir {your_geth_datadir} \
--networkid {network_id} > {your_log_path}/geth.log 2>&1 &
Step 5: Verify Upgrade Success
‚Äã
# Monitor consensus layer logs
tail -f {your_log_path}/0gchaind.log
# Monitor execution layer logs
tail -f {your_log_path}/geth.log
Next Steps
‚Äã
Staking Integration
‚Äã
Once your validator node is running, you can interact with the staking system programmatically using smart contracts:
Staking Interfaces Guide
- Complete documentation for integrating with 0G Chain staking smart contracts


========== [ https://docs.0g.ai/ ] ==========

Build the Future of
Decentralized AI
Quick Start
Learn Concepts
Join Testnet
Connect to Galileo testnet and start building your first dApp on 0G
Connect to Testnet ‚Üí
AI Alignment Node
Learn more about the AI Alignment Node operation
Learn More ‚Üí
Inference
Integrate AI inference into your applications with SDKs
View Docs ‚Üí
Storage SDK
Store and retrieve massive datasets with Go and TypeScript client libraries
View Docs ‚Üí
Fine-tuning
Customize AI models for your specific use case with our command-line tools
View Docs ‚Üí
Run a Validator
Secure the network and earn rewards by running a validator node
Setup Guide ‚Üí
0
+
Partners
0
M+
Accounts
0
M+
Transactions
Ready to Build?
Join thousands of developers building the future of decentralized AI
View Full Documentation


